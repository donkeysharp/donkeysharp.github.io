[{"content":"\n¡Presentando AWS Nuke! En este post daré una introducción rápida a AWS Nuke, una herramienta desarrollada en Golang que tiene como objetivo eliminar todos los recursos en una cuenta de AWS. Esta herramienta me ayudó muchísimo.\nCasos de Uso Limpiar una Cuenta AWS Free Tier Este fue más un caso de uso personal. Creé una cuenta de AWS Free Tier hace algunos meses y la he estado utilizando para diferentes propósitos. Algunos de los recursos que creé fueron mediante Terraform, lo que hizo más sencillo eliminarlos luego de usarlos. Por otro lado, creé otros recursos manualmente, y algunos de ellos me estaban generando costos! Así que preferí eliminar todo en esta cuenta ya que la uso solo con fines de aprendizaje. AWS Nuke es una excelente herramienta para esta tarea.\nNota sobre las nuevas cuentas free plan: AWS anunció sus nuevos planes del free tier. En lugar de ofrecerte el uso gratuito de algunos servicios durante el período de free-tier, te dan cuentas con 100 USD de créditos durante seis meses, lo cual personalmente creo que es mejor para las personas que son nuevas en AWS, ya que el nivel gratuito anterior no incluía algunos recursos y utilizarlos costaba dinero, por ejemplo, los NAT Gateway. Así que, si no quieres matar rápidamente tus 100 USD, ¡AWS Nuke puede ayudarte!\nLimpiar Cuentas de Investigación Es muy común que algunas empresas tengan cuentas utilizadas para investigación, donde los ingenieros pueden probar nuevos servicios, conceptos, etc. Dependiendo de cómo fueron creados los recursos, puede ser simple o no hacer seguimiento y eliminarlos. Lo bueno es que estas cuentas de investigación no deberían tener infraestructura de producción en ejecución, por lo tanto, eliminar los recursos para ahorrar dinero es un caso perfecto para usar AWS Nuke.\nAdvertencia: Los dos casos de uso para AWS Nuke que mencioné consideran únicamente cuentas temporales o efímeras. Es importante destacar que usar herramientas de Infraestructura como Código o al menos tener una buena convención de etiquetado (para poder identificar fácilmente qué recursos existen) puede evitar la necesidad de usar esta herramienta, que en mi opinión es similar a usar kill -9 en sistemas Unix, es decir, úsala como último recurso.\nUsando AWS Nuke Requisitos Antes de continuar, asegúrate de que tu cuenta de AWS tenga un alias asociado, esto es OBLIGATORIO. Para configurarlo, inicia sesión en tu cuenta, ve al servicio IAM y en la sección derecha verás una sección llamada \u0026ldquo;Cuenta\u0026rdquo; donde puedes editar el alias.\nDado que esta herramienta puede ser muy destructiva, es importante que sepas lo que estás haciendo y, sobre todo, qué recursos estás a punto de eliminar. De todas modos, la herramienta se ejecuta en modo de prueba (dry-run) por defecto, es decir, no aplicará ningún cambio hasta que añadas un flag específico y hagas confirmaciones adicionales. Por suerte, puedes ser tan específico como desees sobre qué quieres eliminar: desde recursos concretos de un tipo específico hasta todos los recursos de uno o varios tipos.\nAlgo que me encantó de cómo está programada es que fallará si el alias de tu cuenta AWS contiene la palabra prod, lo cual me parece una validación muy importante para evitar ejecuciones por error.\nPara descargarla e instalarla, sigue su página de documentación. Una vez instalada, puedes continuar con los siguientes pasos.\nAdemás de la herramienta, se espera que ya tengas acceso a AWS mediante la CLI.\nConfigurando tu archivo YAML AWS Nuke necesita un archivo de configuración donde puedes especificar qué cuentas se verán afectadas, qué recursos se incluirán o excluirán, etc. Este archivo está en formato YAML.\nEn mi caso, lo que quería hacer era eliminar todos los recursos de mi cuenta AWS, excepto la VPC predeterminada, el usuario IAM que uso para administración, sus claves de acceso y la configuración MFA.\nEste es el archivo de configuración que usé.\n# aws-nuke-config.yml regions: - us-east-1 # only delete in us-east-1 - global resource-types: excludes: # Some optimizations, for instance do not delete each S3 Object # or DynamoDBTable record, internally aws nuke will empty the bucket anyway - OSPackage - S3Object - DynamoDBTableItem # Keep for default VPC - EC2DefaultSecurityGroupRule # Do not remove IAM User and its dependencies - IAMUser - IAMLoginProfile - IAMUserAccessKey - IAMVirtualMFADevice - IAMUserPolicyAttachment blocklist: - \u0026#34;999999999999\u0026#34; # aws nuke always requires to have an account blocklist accounts: \u0026#34;123456789777\u0026#34;: # my account filters: # Exclude all resources that have the DefaultVPC or the IsDefault properties EC2DHCPOption: - property: DefaultVPC value: \u0026#34;true\u0026#34; EC2InternetGateway: - property: DefaultVPC value: \u0026#34;true\u0026#34; EC2InternetGatewayAttachment: - property: DefaultVPC value: \u0026#34;true\u0026#34; EC2RouteTable: - property: DefaultVPC value: \u0026#34;true\u0026#34; EC2Subnet: - property: DefaultVPC value: \u0026#34;true\u0026#34; EC2VPC: - property: IsDefault value: \u0026#34;true\u0026#34; # END: Filter all default VPC resources ¡Ejecutando AWS Nuke! Ejecutarlo es muy sencillo, una vez que AWS Nuke está instalado solo necesitas correr el siguiente comando para tener un plan en modo de prueba de lo que está por eliminarse:\n$ aws-nuke nuke --config ./aws-nuke-config.yml Esto generará un plan, y como se ve en el screenshot, los registros que serán eliminados contienen el texto would be removed.\nAnímate a probar Intenta eliminar o modificar tu archivo de configuración YAML y observa qué cambia en el plan.\nEl Último Paso, ¡Núkelos a Todos! Una vez que estés conforme con el plan de eliminación, puedes ejecutar:\n$ aws-nuke nuke --config ./aws-nuke-config.yml --no-dry-run-mode Lo cual te pedirá el alias y confirmará que realmente deseas eliminar los recursos de esa cuenta.\nComentarios Finales Espero que este post te sea útil. AWS Nuke me ayudó mucho con cuesntas personales que uso para aprendizaje, así puedo ahorrar algunos dólares. Pero nunca olvides que esta es una herramienta destructiva, debes tener MUCHO cuidado al usarla.\n","permalink":"https://blog.donkeysharp.xyz/es/intro-to-aws-nuke/","summary":"\u003cp\u003e\u003cimg alt=\"alt text\" loading=\"lazy\" src=\"/img/aws-nuke.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"presentando-aws-nuke\"\u003e¡Presentando AWS Nuke!\u003c/h2\u003e\n\u003cp\u003eEn este post daré una introducción rápida a \u003ca href=\"https://aws-nuke.ekristen.dev/\"\u003eAWS Nuke\u003c/a\u003e, una herramienta desarrollada en Golang que tiene como objetivo eliminar todos los recursos en una cuenta de AWS. Esta herramienta me ayudó muchísimo.\u003c/p\u003e\n\u003ch2 id=\"casos-de-uso\"\u003eCasos de Uso\u003c/h2\u003e\n\u003ch3 id=\"limpiar-una-cuenta-aws-free-tier\"\u003eLimpiar una Cuenta AWS Free Tier\u003c/h3\u003e\n\u003cp\u003eEste fue más un caso de uso personal. Creé una cuenta de AWS Free Tier hace algunos meses y la he estado utilizando para diferentes propósitos. Algunos de los recursos que creé fueron mediante Terraform, lo que hizo más sencillo eliminarlos luego de usarlos. Por otro lado, creé otros recursos manualmente, y algunos de ellos me estaban generando costos! Así que preferí eliminar todo en esta cuenta ya que la uso solo con fines de aprendizaje. AWS Nuke es una excelente herramienta para esta tarea.\u003c/p\u003e","title":"Utilizando AWS Nuke para limpiar tus cuentas de AWS"},{"content":"En Bolivia hace unos 15 años aproximadamente pensar en comprar por internet no era algo simple, salvo tengas una tarjeta de crédito cosa que no es algo simple como abrir una cuenta de banco (esto hasta hoy por la burocracia de los bancos), por esos motivos, varios bancos comenzaron a permitir hacer compras por internet utilizando las tarjetas de débito que los bancos ofrecen. Eso facilitó muchas cosas y era mucho más simple acceder a diferentes servicios que antes no eran del todo simples. Ejemplo, pagar compras de Ebay, pagar tu dominio o hosting web, pagar servicios de streaming, suscribirte a servicios de educación, etc. Por otro lado para las personas que viajan mucho, esto ayudó un montón ya que era posible usar tu tarjeta de débito cuando salías del país sin mucha dificultad, buenos años.\nBolivia actualmente está viviendo una situación económica que es nueva para much@s, sea por la inflación que es algo que nuestros padres vivieron, pero ahora es un poco diferente, antes el internet no era un servicio básico. Si vives en Bolivia, sabes que existen dos tipos de cambio para el USD actualmente, el oficial que es 6.97Bs por USD y el paralelo que está aproximadamente 14Bs (al momento de escribir este post). A raíz de esto los bancos pusieron límites a las compras por internet que antes eran de mucha utilidad, de la misma forma al uso de tarjetas de débito y crédito en el exterior, algunas con un límite de 50 USD al mes (seamos honestos, no hay mucho que se pueda hacer con 50 USD en otro país).\nEn este post pondré algunas alternativas para poder sobrellevar esta situación. Obviamente la situación económica de cada persona puede variar demasiado, cada opción que mostraré puede o no ajustarse a la situación de cada uno, pero al menos saber qué opciones hay ayuda a tener un mejor panorama.\nAlternativa #1: Comprar USDs físicos Hoy en día tener la esperanza de comprar dólares físicos al cambio oficial (6.97) es casi nula, entonces tienes dos opciones, tener la esperanza y mantener tus Bolivianos en el banco o bajo el colchón viendo cada día como pierde su valor o ir a un librecambista y comprarlos al precio real.\nSe que muy pocas empresas han aumentado sueldos, lo cual hace que la situación sea más complicada. Entonces si por ejemplo antes ganabas 2500Bs, comprar 100$ era aproximadamente 700Bs, hoy en día es aprox 1400Bs, casi la mitad de un sueldo básico.\nEntonces por qué comprar USDs físicos? La respuesta rápida es para mantener el valor de tu dinero. Si al final del mes después de haber pagado alquiler, comida, etc, etc. te sobra por ejemplo 1400Bs, no los dejes en el banco, sacalos y anda a un librecambista y compra 100 USD.\n¿Por qué?\nEn unas semanas o unos meses esos 100 USD posiblemente valgan 1500Bs o 1600Bs y las cosas estarán mas caras, no se sabe. Pero de necesitar ese dinero lo venderás al precio que realmente es, que es mejor que mantener tus 1400Bs que ya no valen lo que valían cuando compraste o pudiste comprar esos 100 USD.\nPros Mantienes el valor de tu dinero si el dólar sigue subiendo, cuando necesites dinero de esos ahorros, venderás USDs al precio real.\nContras Si bien mantienes el valor de tu dinero, no puedes hacer compras por internet, en cuestión de viajes tienes que andar siempre con efectivo y no poder usar tus tarjetas. A medida que la tecnología avanza, es cada vez más común en otros países que hay lugares que no aceptan cash lastimosamente, entonces la necesidad de una tarjeta es importante.\nAntes de continuar!!! Para las alternativas virtuales que mostraré ahora, es importante que para cada plataforma sea Binance, Meru, etc. En todas deberás identificar tu identidad, te pedirán que saques una foto ya sea a tu Carnet de Identidad o a tu Pasaporte, esto por las políticas de cada plataforma. Caso contrario no podrás utilizar sus plataformas.\nPor otro lado, cualquier plataforma te cobra un monto de comisión por transacción que suele ser el 1% o más de cada transferencia que hagas, entonces es importante leer cuanto es la comisión de cada plataforma para que no termines sorprendid@.\nAlternativa #2: Cryptos No daré una clase de que son las cryptos, pero existen dos criptomonedas USDC y USDT que están a la par del USD (el dólar de US). Por suerte hoy en día comprar USDC o USDT es super simple gracias a Binance.\nNo caigas en estafas: Hoy en día existen muchas estafas relacionadas a criptomonedas, son estafas piramidales donde te prometen retornos de inversión altos, no existe el dinero fácil, no regales tu dinero a ladrones.\nAbajo dejo links de videos de como comprar o vender USDTs en Binance para gente de Bolivia.\nEl flujo explicado rápido es:\nComprar USDT:\nEntras a Binance. Eliges un \u0026ldquo;cryptocambista\u0026rdquo; a quien le comprarás USDTs. Le depositas en Bs por QR al \u0026ldquo;cryptocambista\u0026rdquo;, digamos 1400Bs. El \u0026ldquo;cryptocambista\u0026rdquo; verifica que le llegaron los 1400Bs y libera la transacción. Tendrás en Binance 100 USDTs. Es en la compra donde una persona puede sentirse un poco más desconfiada, porque primero debes pasar el dinero a la cuenta de la otra persona, vendrán pensamientos de \u0026ldquo;y si no me deposita\u0026rdquo;, etc. pero a esas personas no les conviene que reportes sus cuentas porque te tomaron el pelo, para ellos implica que Binance les bloqueará lo cual no les conviene. De todos modos en la lista de cryptocambistas hay personas verificadas por la plataform que podria decirse son personas de confianza.\nVender USDT:\nEntras a Binance. Eliges un \u0026ldquo;cryptocambista\u0026rdquo; para venderle tus USDTs. Le pasa QR de tu banco. El \u0026ldquo;cryptocambista\u0026rdquo; te depositará lo que estes vendiendo a tu cuenta del banco en Bolivianos. El \u0026ldquo;cryptocambista\u0026rdquo; recibirá en su cuenta de Binance la cantidad de USDTs que vendiste. Pros De la misma manera mantienes el valor de tu dinero, no necesitas ir a un banco y como escribiré en la siguiente alternativa, podrás utilizar ese dinero para compras por Internet y uso en el extranjero.\nContras Los USDTs suele estar unos centavos más caros que el dólar físico.\nAlternativa #3: De Binance a Meru y utilizar su tarjeta de débito virtual para compras por internet. Una plataforma que ayuda un montón es Meru, puedes descargarla desde el Play Store o el App Store. Existen varias plataformas similares ej. Payoneer, AirTM, etc.\nMeru me gusta porque es la que de momento tiene menos comisiones, además que se integra bastante bien con Binance. Por otro lado es posible habilitar una tarjeta de débito virtual para poder hacer compras por internet como también enlazarla con Google Pay o Apple Pay si deseas usar tu smartphone para realizar pagos por si sales del país. Leí por ahí que planean también ofrecer una tarjeta física pero de momento, con la virtual bastará.\nPros Ya podrás utilizar tu dinero como solías hacerlo antes, comprar por internet, usarlo en viajes mediante Google Pay o Apple Pay.\nContras Cuando transfieres dinero entre plataformas, no hay forma de evitar las comisiones, entonces es algo que debes calcular antes de cualquier movimiento. Por otro lado, si bien tienes una tarjeta de débito que puedes utilizar para casi todo, hay algunas cosas que aún requieren tarjetas de crédito como servicios de rent-a-car o algunos hoteles en el exterior. Pero para eso existe Tower Bank, que te permite obtener todas esas facilidades, pero eso lo escribiré en otro post.\nBonus: Flujo de trabajo En las alternativas no físicas, es decir, Alternativa #1 y Alternativa #2 yo utilizo el siguiente flujo:\nSi deseo solo mantener el valor de mi dinero, solo compro USDTs en Binance y lo mantengo ahí hasta que yo requiera cambiar USDTs a Bolivianos. Si deseo utilizar mis USDTs para hacer compras por internet o para viajes, transfiero el monto requerido de Binance a Meru (existe una comisión). Recursos Comprar USDT desde Binance para Bolivia Vender USDT desde Binance para Bolivia Mover dinero de Meru a Binance Mover dinero de Binance a Meru Solicitar tarjeta virtual de Meru ","permalink":"https://blog.donkeysharp.xyz/es/guia-dolar-bolivia/","summary":"\u003cp\u003eEn Bolivia hace unos 15 años aproximadamente pensar en comprar por internet no era algo simple, salvo tengas una tarjeta de crédito cosa que no es algo simple como abrir una cuenta de banco (esto hasta hoy por la burocracia de los bancos), por esos motivos, varios bancos comenzaron a permitir hacer compras por internet utilizando las tarjetas de débito que los bancos ofrecen. Eso facilitó muchas cosas y era mucho más simple acceder a diferentes servicios que antes no eran del todo simples. Ejemplo, pagar compras de Ebay, pagar tu dominio o hosting web, pagar servicios de streaming, suscribirte a servicios de educación, etc. Por otro lado para las personas que viajan mucho, esto ayudó un montón ya que era posible usar tu tarjeta de débito cuando salías del país sin mucha dificultad, buenos años.\u003c/p\u003e","title":"Guía del fin del mundo para el USD en Bolivia"},{"content":"En este post, mostraré una solución (que personalmente encuentro bastante limpia) a un problema que encontré con AWS SSO y Serverless Framework.\nServerless Framework es una de mis herramientas preferidas al trabajar con AWS Lambda y otros servicios serverless. Es una alternativa a AWS SAM, pero personalmente prefiero Serverless Framework debido a su compatibilidad con diversos cloud providers y plugins. Ya he usado Serverless Framework antes para acceder a las API de AWS configurando el archivo ~/.aws/credentials. Sin embargo, esta vez me tocó utilizar AWS Single Sign-On para acceder a las API de AWS desde mi computadora local. Lamentablemente, al intentar hacer deploy de funciones Lambda con la CLI de sls tuve algunos errores. Mostró un mensaje de error que indicaba que el AWS profile que estaba utilizando no estaba configurado, a pesar de haber iniciado sesión con éxito hace unos minutos.\n$ export AWS_PROFILE=\u0026#39;some-aws-sso-profile\u0026#39; $ aws sso login # Logs in successfully $ sls deploy --stage dev DOTENV: Loading environment variables from .env: Deploying some-random-lambda to stage dev (us-east-1) ✖ Stack some-random-lambda failed to deploy (64s) Environment: linux, node 18.16.0, framework 3.33.0 (local) 3.33.0v (global), plugin 6.2.3, SDK 4.3.2 Docs: docs.serverless.com Support: forum.serverless.com Bugs: github.com/serverless/serverless/issues Error: AWS profile \u0026#34;some-aws-sso-profile\u0026#34; doesn\u0026#39;t seem to be configured Después de investigar un poco, descubrí que Serverless no es compatible con AWS SSO. Parece que sls espera que el archivo ~/.aws/credentials esté configurado, pero AWS SSO no requiere almacenar credenciales localmente, ya que genera credenciales temporales cada vez que inicias sesión.\nTambién aprendí que podía instalar un plugin de Serverless llamado Better Credentials. Sin embargo, personalmente prefiero evitar instalar y versionar un plugin que solo es útil para el desarrollo local.\nCómo AWS SSO almacena las credenciales Tras iniciar sesión con éxito, un access token se guarda en un archivo JSON en ~/.aws/sso/cache. Afortunadamente, este token de acceso se puede utilizar para obtener la ACCESS KEY y SECRET ACCESS KEY reales, que luego se pueden agregar al archivo ~/.aws/credentials, que la CLI sls espera que este configurado. El contenido de este archivo JSON se ve así:\n{ \u0026#34;startUrl\u0026#34;: \u0026#34;https://\u0026lt;some-id\u0026gt;.awsapps.com/start#/\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;accessToken\u0026#34;: \u0026#34;\u0026lt;access-token\u0026gt;\u0026#34;, \u0026#34;expiresAt\u0026#34;: \u0026#34;2023-10-20T05:58:17Z\u0026#34; } Además, mi archivo ~/.aws/config incluye la siguiente configuración de SSO:\n[profile some-aws-sso-profile] sso_start_url = https://\u0026lt;some-id\u0026gt;.awsapps.com/start#/ sso_region = us-east-1 sso_account_id = 123456789012 sso_role_name = MyRoleName region = us-east-1 Para obtener ACCESS KEY y SECRET ACCESS KEY, se debe ejecutar el siguiente comando:\n$ export SSO_TOKEN=\u0026#39;token-del-archivo-json\u0026#39; $ aws sso get-role-credentials --access-token $SSO_TOKEN --role-name MyRoleName --account-id 123456789012 { \u0026#34;roleCredentials\u0026#34;: { \u0026#34;accessKeyId\u0026#34;: \u0026#34;una-clave-de-acceso\u0026#34;, \u0026#34;secretAccessKey\u0026#34;: \u0026#34;una-clave-de-acceso-secreta\u0026#34;, \u0026#34;sessionToken\u0026#34;: \u0026#34;un-token-de-sesión\u0026#34;, \u0026#34;expiration\u0026#34;: 1698038986000 } } Esta información se puede agregar al archivo ~/.aws/credentials utilizando el mismo nombre de perfil de AWS, lo que permite que sls funcione.\nPresentando aws-sso-creds-helper Aunque la solución mencionada anteriormente funciona, implica varios pasos manuales. Afortunadamente, existe una utilidad que automatiza este proceso. Se trata de una utilidad JavaScript conocida como aws-sso-creds-helper.\n$ npm install -g aws-sso-creds-helper Poniéndolo todo junto El paso final es integrar todos estos elementos. Para que la solución funcione en su totalidad, todo lo que debes hacer es ejecutar el comando ssocreds inmediatamente después de iniciar sesión con AWS SSO.\n$ export AWS_PROFILE=\u0026#39;some-aws-sso-profile\u0026#39; $ aws sso login # Inicio de sesión exitoso $ ssocreds -p $AWS_PROFILE # Agrega las credenciales temporales al archivo ~/.aws/credentials $ sls deploy --stage dev # deployment exitoso Pensamientos finales A pesar de la introducción de una herramienta adicional para trabajar con AWS SSO, considero que esta solución es elegante, ya que elimina la necesidad de modificar o agregar dependencias adicionales al proyecto solo con el fin de utilizarlo en desarrollo local.\n","permalink":"https://blog.donkeysharp.xyz/es/post/serverless-aws-sso/","summary":"\u003cp\u003eEn este post, mostraré una solución (que personalmente encuentro bastante limpia) a un problema que encontré con AWS SSO y Serverless Framework.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.serverless.com/framework/docs\"\u003eServerless Framework\u003c/a\u003e es una de mis herramientas preferidas al trabajar con AWS Lambda y otros servicios serverless. Es una alternativa a \u003ca href=\"https://aws.amazon.com/serverless/sam/\"\u003eAWS SAM\u003c/a\u003e, pero personalmente prefiero Serverless Framework debido a su compatibilidad con diversos cloud providers y plugins. Ya he usado Serverless Framework antes para acceder a las API de AWS configurando el archivo \u003ccode\u003e~/.aws/credentials\u003c/code\u003e. Sin embargo, esta vez me tocó utilizar \u003ca href=\"https://aws.amazon.com/what-is/sso/\"\u003eAWS Single Sign-On\u003c/a\u003e para acceder a las API de AWS desde mi computadora local. Lamentablemente, al intentar hacer deploy de funciones Lambda con la CLI de \u003ccode\u003esls\u003c/code\u003e tuve algunos errores. Mostró un mensaje de error que indicaba que el AWS profile que estaba utilizando no estaba configurado, a pesar de haber iniciado sesión con éxito hace unos minutos.\u003c/p\u003e","title":"Usando Serverless Framework con AWS SSO"},{"content":"Hace algún tiempo estaba trabajando en la creación de un entorno de desarrollo local basado en docker para algunos microservicios, para que los desarrolladores puedan tener los componentes de infraestructura necesarios en sus máquinas y eso les ayudará con sus tareas diarias. Inicialmente, la lógica de negocio de algunos microservicios era una caja negra para mí. Después de colocar las aplicaciones en contenedores y crear la configuración de docker-compose, algunas de ellas comenzaron a fallar y, después de verificar los logs, resultó que las aplicaciones usaban el AWS SDK para obtener la metadata de la instancia ec2.\nPara aquellos que no están familiarizados con la metadata de EC2, se trata de un conjunto de endpoints HTTP que están disponibles en la dirección IP 169.254.169.254. Esto se usa para recuperar metadata como la IP de la instancia, la región de AWS, la zona de disponibilidad, las credenciales de IAM, etc. E internamente, el SDK de AWS usa estos enpodints para el mismo propósito.\nPor defecto, cualquier usuario de sus máquinas locales no podrá llegar a 169.254.169.254 porque es parte del espacio de direcciones local-link. Así que tenemos dos problemas:\nRutear todo el tráfico a esa dirección IP especial a algún lugar conocido. Simular todos los endpoints HTTP para la metadata. Hacer que 169.254.169.254 esté disponible localmente Afortunadamente, es posible hacer que el tráfico a 169.254.169.254 funcione localmente o en un entorno local basado en Docker. Linux y MacOS proporcionan herramientas que simplifican este tipo de tareas.\nSegún el sistema operativo que esté utilizando, existen diferentes formas de enrutar el tráfico 169.254.169.254 a la interfaz local.\nEn MacOS puedes hacerlo ejecutando el comando:\n$ sudo ifconfig lo0 alias 169.254.169.254 En Linux, hay diferentes opciones:\nUsando ifconfig:\n$ sudo ifconfig lo:0 169.254.169.254 máscara de red 255.255.255.255 Usando iptabes:\n$ sudo iptables -t nat -A SALIDA -d 169.254.169.254 -j DNAT --a-destino 127.0.0.1 De esta manera, cualquier conexión de red que vaya a 169.254.169.254 irá a nuestra máquina local.\nSimular los endpoints HTTP para la metadata Debido a que muchos ingenieros pueden tener el mismo problema de acceder al servidor de metadata en un entorno local, AWS ha creado una utilidad que sirve todos los endpoints HTTP para la metadata. El proyecto amazon-ec2-metadata-mock nos ayuda con eso.\nSolo hay que descargar el binario para su sistema operativo desde su página de releases y podrá comenzar a usarlo.\nAlgunas opciones que tiene son: Para que el AWS SDK funcione al intentar hacer request de metadata, un request a http://169.254.169.254/latest/meta-data debe funcionar. Afortunadamente, solucionamos el problema de apuntar 169.254.169.254 a localhost en la sección anterior. ec2-metadata-mock se expone de forma predeterminada en el puerto 1338, por lo que para engañar al AWS SDK necesitamos exponer los endpoints falsos en el puerto 80.\nPara eso, solo necesitamos ejecutarlo como:\n$ sudo ec2-metadata-mock -p 80 Juntándolo todo Ahora que sabemos cómo enrutar el tráfico a 169.254.169.254 donde queramos y tenemos un servidor de metadata EC2 falso, podemos unir todo y tener un entorno de desarrollo completamente basado en Docker.\nPara esto, habrá con container para la herramienta ec2-metadata-mock y otro que se llamará debug que podría representar cualquier aplicación que necesite acceso al servidor de metadata.\nEl código fuente de este experimento se puede encontrar en este repositorio.\nEntonces, el archivo de Docker compose se verá así:\nversion: \u0026#39;3\u0026#39; services: mock_metadata: image: ec2-metadata-mock build: context: . dockerfile: Dockerfile.metadata-mock debug: image: ec2-metadata-debug build: context: . dockerfile: Dockerfile.debug environment: MOCK_HOSTNAME: mock_metadata command: - sleep - \u0026#39;3600\u0026#39; cap_add: - NET_ADMIN Y contiene un contenedor que ejecutará el servidor ec2-metadata-mock en el puerto 80 y un contenedor de debugging que simula una aplicación. Recordemos que el objetivo es realizar cualquier request HTTP desde el contenedor de la aplicación (en este caso, el contenedor debug) a http://169.254.169.254/ y que la conexión vaya al contenedor del servidor de metadata.\nPara que las aplicaciones enruten el tráfico al servidor de metadata, agregué un script entrypoint que se ejecuta antes de que se inicie la aplicación. Recupera la dirección IP interna utilizada en la red docker para el contenedor del servidor de metadata, luego crea una regla iptable que enruta cualquier tráfico a 169.254.169.254 a la dirección IP del servidor de metadata. Es importante tener en cuenta que debemos agregar el Linux capability NET_ADMIN para usar iptables dentro de un contenedor.\n#!/bin/bash if [[ -z $MOCK_HOSTNAME ]]; then echo \u0026#34;MOCK_HOSTNAME must be set\u0026#34; exit 1 fi mock_ip_address=$(dig +short $MOCK_HOSTNAME) echo \u0026#39;INFO - Make traffic to 169.254.169.254 go through local mock server\u0026#39; iptables -t nat -A OUTPUT -d 169.254.169.254 -j DNAT --to-destination ${mock_ip_address} exec $@ Entonces, una vez que ejecutamos la solución completa, podemos probar que, de hecho, podemos hacer curl 169.254.169.254 desde el contenedor debug.\n$ docker exec -it local-ec2-metadata_debug_1 curl http://169.254.169.254/latest/meta-data/instance-id i-1234567890abcdef0 Recomendaciones Aunque esta solución usa iptables y funciona, investigaré y actualizaré este post si es posible definir una red personalizada en docker-compose usando el rango link-local y asignar una dirección IP específica al contenedor ec2-metadata.\n","permalink":"https://blog.donkeysharp.xyz/es/mock-ec2-metadata/","summary":"\u003cp\u003eHace algún tiempo estaba trabajando en la creación de un entorno de desarrollo local basado en docker para algunos microservicios, para que los desarrolladores puedan tener los componentes de infraestructura necesarios en sus máquinas y eso les ayudará con sus tareas diarias. Inicialmente, la lógica de negocio de algunos microservicios era una caja negra para mí. Después de colocar las aplicaciones en contenedores y crear la configuración de docker-compose, algunas de ellas comenzaron a fallar y, después de verificar los logs, resultó que las aplicaciones usaban el AWS SDK para obtener la metadata de la instancia ec2.\u003c/p\u003e","title":"Simulando servidor de metadata de EC2 localmente"},{"content":"Es posible que cuando estas escribiendo un programa, en algún momento necesites pausar la ejecución de un proceso llamando a la función sleep(NUMBER_OF_SECONDS) dependiendo del problema que estés resolviendo. En este post, compartiré lo que aprendí hasta ahora mientras investigaba los mecanismos internos del kernel que hacen que la función sleep funcione de la forma en que lo hace.\nAgradezco su feedback. No soy un experto en este tema ya que las partes internas del Kernel de Linux son nuevas para mí, fue solo mi curiosidad lo que me llevó a revisar el código fuente del Kernel y quería compartir lo que aprendí. Si encuentran algo incorrecto en este post, pueden abrir un issue en el repositorio de Github de este blog. Gracias!.\nProcess State Uno de los primeros conceptos que necesitamos revisar es el estado de un proceso. Un proceso en Linux tiene un estado asociado que representa su estado de ejecución en el sistema operativo. Un proceso puede estar en uno de los siguientes estados:\nRunning Sleeping (interruptible and uninterruptible) Stopped Zombie Cuando un proceso está ejecutando instrucciones en la CPU, se encuentra en estado \u0026ldquo;Running\u0026rdquo; y cuando el proceso está esperando que suceda algo, es decir, esperando I/O de red o disco, o se llama a la funcion sleep, cambiará a un estado Sleeping.\nPodemos comprobarlo con un sencillo programa de ejemplo en C:\n// states.c // gcc states.c -o states #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #define SOME_MAGIC_NUMBER 365000000l void start_processing() { long i; printf(\u0026#34;Starting Loop\\n\u0026#34;); for (i = 0; i \u0026lt; (long)(10 * SOME_MAGIC_NUMBER); i++); printf(\u0026#34;Loop Finished\\n\u0026#34;); } int main() { pid_t pid = getpid(); printf(\u0026#34;PID: %d\\n\u0026#34;, pid); start_processing(); printf(\u0026#34;Sleep process\\n\u0026#34;); sleep(5); printf(\u0026#34;Sleep finished\\n\u0026#34;); start_processing(); return 0; } El código anterior ejecutará un loop durante algunos segundos, luego se suspenderá durante 5 segundos y finalmente ejecutará otro loop durante otro número de segundos. Por lo tanto, esperamos que el estado del proceso sea Running -\u0026gt; Sleeping -\u0026gt; Running.\nMientras se ejecuta el programa, podemos comprobar el estado del proceso con la herramienta Htop, que normalmente mostrará una letra en la octava columna que representa el estado del proceso, e.g. R (Running), S (Sleeping), T (Stopped), etc.\nComo era de esperar, los estados que tenía el proceso durante la ejecución fueron: Running (R) -\u0026gt; Sleeping (S) -\u0026gt; Running (R).\nTL;DR (super resumido) Cuando un programa llama a la función sleep(NUMBER_OF_SECONDS) (en C), este usará la llamada al sistema (syscall) nanosleep. Otros lenguajes de programación usan diferentes syscalls que también pueden enviar un proceso a dormir durante algunos segundos, e.g. select.\nLa implementación del kernel de Linux de la syscall nanosleep hará lo siguiente:\nInicializar un High Resolution sleep timer. Cambiar el estado del proceso a TASK_INTERRUPTIBLE (Sleeping). Inicia el High Resolution sleep timer. Indicar al scheduler de procesos para poner a otro proceso en ejecución y pausar la ejecución del proceso actual. El kernel de Linux procesa los High Resolution Timers de la siguiente manera:\nEl hardware de la computadora tiene un CPU timer que causa interrupciones periódicamente, haciendo que el kernel las maneje llamando a la función hrtimer_interrupt. La función hrtimer_interrupt procesará los High Resolution Timers (a nivel de software) existentes y verá si un timer expiró. Una vez que un High Resolution Timer expire, el kernel llamará a la función hrtimer_wakeup que activará el proceso asociado con el timer, y eso cambiará el estado de TASK_INTERRUPTIBLE (Sleeping) a TASK_RUNNING (Running). Finalmente, algunos ciclos de CPU más tarde, el scheduler de procesos continuará la ejecución del proceso exactamente donde se detuvo. Sigue leyendo si está interesado en más detalles.\nYendo Más A Fondo Como ingenieros de software, es probable que la mayor parte del tiempo estemos escribiendo aplicaciones que se ejecutan en user space o user mode, como servidores (de cualquier tipo) o aplicaciones del lado del servidor, web , aplicaciones móviles o de escritorio, scripts de automatización, etc.\nNo importa el lenguaje de programación, el framework o la tecnología, internamente un programa que se ejecuta en user mode siempre interactuará de una forma u otra con el sistema operativo (en este post Linux) a través de System Calls o syscalls. Por ejemplo, cuando leemos un archivo, nuestro código (sin importar el lenguaje de programación) se comunicará indirectamente con el Kernel de Linux a través de la syscall read (no es la única), luego el kernel le pedirá al disco duro físico el contenido del archivo que queremos basado en el sistema de archivos, y finalmente devolverá el contenido solicitado a nuestro programa.\nHay una herramienta llamada Strace que monitorea todas las syscall que ejecuta un proceso.\nSi ejecutamos el ejemplo anterior en C usando strace, podemos ver la siguiente salida:\n$ strace ./states ... syscalls for process loading (they won\u0026#39;t be useful right now) ... write(1, \u0026#34;PID: 26846\\n\u0026#34;, 11PID: 26846 ) = 11 write(1, \u0026#34;Starting Loop\\n\u0026#34;, 14Starting Loop ) = 14 write(1, \u0026#34;Loop Finished\\n\u0026#34;, 14Loop Finished ) = 14 write(1, \u0026#34;Sleep process\\n\u0026#34;, 14Sleep process ) = 14 nanosleep({tv_sec=5, tv_nsec=0}, 0x7ffefc933be0) = 0 write(1, \u0026#34;Sleep finished\\n\u0026#34;, 15Sleep finished ) = 15 write(1, \u0026#34;Starting Loop\\n\u0026#34;, 14Starting Loop ) = 14 write(1, \u0026#34;Loop Finished\\n\u0026#34;, 14Loop Finished ) = 14 exit_group(0) = ? +++ exited with 0 +++ La salida verdadera es más larga que la que se muestra arriba, pero la mayoría de las primeras syscalls siempre se ejecutan cuando se inicia un proceso y carga la biblioteca estándar de C entre otras cosas, pero las que nos interesa revisar son las últimas.\nLa syscall write le dice al Kernel que el programa quiere mostrar una cadena de texto en la salida estándar (en este caso la terminal). Con esa información, podemos tener una idea de que la función printf se comunica con el sistema operativo llamando a la syscall write.\nLuego se llama a la syscall nanosleep, que indicará al kernel de Linux que mueva el proceso de un estado Running a un estado Sleeping.\nAntes de revisar la implementación en Linux de la syscall nanosleep, primero tenemos que revisar un par de conceptos para tener una mejor comprensión de lo que viene.\nHigh Resolution Timers Dentro del kernel de Linux, diferentes componentes deben esperar un tiempo antes de ejecutar algo, aquí es donde entra el concepto de Timers. Un timer es una estructura (struct) en la que definimos su tiempo de expiración (el tiempo de espera) y qué función se llamará una vez que el timer expira.\nEl kernel de Linux tiene dos tipos de timers: Low Resolution Timers y High Resolution Timers. Revisaremos los Hight Resolution Timers.\nEl framework detrás de los High Resolution Timers dentro del kernel de Linux es un conjunto de structs y funciones que procesan los timers de manera óptima. Su implementación se basa en una cola de timers que se ordenan por el timer que expirará más pronto. Para que esta cola sea eficiente, utiliza la estructura de datos Red Black Tree para que la inserción y la eliminación se puedan realizar en tiempo logarítmico.\nEste framework es muy interesante como tal, en este artículo solo revisaremos cómo se usa y algunas partes específicas de su implementación. Para obtener más información sobre los detalles de implementación, lea [0], [1] y [2].\nLa gestión del tiempo o time management dentro del kernel de Linux es un tema muy interesante y la charla dada por Stephen Boyd titulada \u0026ldquo;Timekeeping In The Linux Kernel\u0026rdquo; [4] me dio una mejor comprensión de cómo el Kernel de Linux maneja las tareas relacionadas al tiempo, así como su complejidad. Esta charla me ayudó mucho. ¡Gracias!\nHardware Timer La CPU (físicamente) tiene un reloj e internamente tiene un timer programable. En palabras simples, el objetivo principal de este timer es causar interrupciones periódicamente (muchas veces dentro de un segundo) para que el kernel pueda manejarlas. La frecuencia de estas interrupciones dependerá de la arquitectura que se especifique en el Kernel de Linux durante su compilación. El kernel de Linux abstrae esto como Clock Event Devices y hay un Clock Event Device por CPU. Se utiliza un Clock Event Device para programar la siguiente interrupción que se generará [3].\nCuando ocurre una interrupción del timer, el kernel de Linux lo manejará llamando a una función. [4] menciona que para los High Resolution Timers, la función hrtimer_interrupt es el manejador de las interrupciones del timer (revisaremos su código más adelante).\nOtra forma de verificar qué manejador se utilizará, es revisar el archivo de solo lectura /proc/timer_list que contiene la lista de timers pendientes y los Clock Event devices.\nEn mi caso, validé que el manejador de estos dispositivos en mi máquina es efectivamente la función hrtimer_interrup.\n# /proc/timer_list .... Tick Device: mode: 1 Per CPU device: 0 Clock Event Device: lapic-deadline max_delta_ns: 1916620707137 min_delta_ns: 1000 mult: 9624619 shift: 25 mode: 3 next_event: 14403083615478 nsecs set_next_event: lapic_next_deadline shutdown: lapic_timer_shutdown periodic: lapic_timer_set_periodic oneshot: lapic_timer_set_oneshot oneshot stopped: lapic_timer_shutdown event_handler: hrtimer_interrupt \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; the interrupt handler retries: 1316 Tick Device: mode: 1 Per CPU device: 1 Clock Event Device: lapic-deadline max_delta_ns: 1916620707137 min_delta_ns: 1000 mult: 9624619 shift: 25 mode: 3 next_event: 14403083615478 nsecs set_next_event: lapic_next_deadline shutdown: lapic_timer_shutdown periodic: lapic_timer_set_periodic oneshot: lapic_timer_set_oneshot oneshot stopped: lapic_timer_shutdown event_handler: hrtimer_interrupt \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; the interrupt handler retries: 484 .... The rest of devices per CPU of my machine Ahora que tenemos una idea de los High Resolution Timers y que la CPU tiene un timer de hardware que periódicamente causa interrupciones al kernel, podemos continuar con la syscall nanosleep.\nImplementación de la syscall nanosleep en Linux El Kernel de Linux es un proyecto enorme, miles de archivos y millones de líneas de código, navegar a través de ellos puede ser todo un desafío. Hay una herramienta online llamada LXR que ayuda a navegar el código fuente del Kernel de Linux de manera amigable. La URL del sitio es https://elixir.bootlin.com/linux/5.14/source.\nHasta ahora sabemos que la syscall nanosleep hace toda la magia para mover el estado del proceso de Running a Sleeping durante un determinado número de segundos, luego pasar al estado Running nuevamente. Ahora exploraremos el código fuente del kernel de Linux y revisaremos cuáles son los mecanismos internos detrás de ese comportamiento \u0026ldquo;simple\u0026rdquo;.\nPrimero, tenemos que verificar dónde está definida la syscall nanosleep. Después de buscar un poco en Google, encontré un documento que especifica cómo se definen las syscall en el kernel de Linux. Por lo tanto, tenemos que buscar SYSCALL_DEFINE2(nanosleep, ....), el 2 en SYSCALL_DEFINE2 indica el número de argumentos de la syscall. Sé que nanosleep tiene dos argumentos después de comprobar en su manual (todas las syscall tienen una página man(2)).\nDespués de buscar el término nanosleep en LXR, encontré que la syscall está definida en archivo kernel/time/hrtimer.c .\nSYSCALL_DEFINE2(nanosleep, struct __kernel_timespec __user *, rqtp, struct __kernel_timespec __user *, rmtp) { struct timespec64 tu; if (get_timespec64(\u0026amp;tu, rqtp)) return -EFAULT; if (!timespec64_valid(\u0026amp;tu)) return -EINVAL; current-\u0026gt;restart_block.nanosleep.type = rmtp ? TT_NATIVE : TT_NONE; current-\u0026gt;restart_block.nanosleep.rmtp = rmtp; return hrtimer_nanosleep(timespec64_to_ktime(tu), HRTIMER_MODE_REL, CLOCK_MONOTONIC); } Por supuesto, cada línea de código tiene su razón de ser, pero resaltaré la llamada a timespec64_to_ktime que convierte los argumentos de entrada de la syscall en la estructura ktime que es utilizada por el framework de High Resolution Timers. Finalmente, llama a la función hrtimer_nanosleep donde comienza toda la diversión.\nIré función por función en el orden en que son llamadas y explicaré las partes que considero relevantes:\nLa función hrtimer_nanosleep:\nlong hrtimer_nanosleep(ktime_t rqtp, const enum hrtimer_mode mode, const clockid_t clockid) { ... hrtimer_init_sleeper_on_stack(\u0026amp;t, clockid, mode); hrtimer_set_expires_range_ns(\u0026amp;t.timer, rqtp, slack); ret = do_nanosleep(\u0026amp;t, mode); ... } Hay tres partes relevantes aquí:\nLa inicialización del High Resolution Timer (lo revisaremos más adelante) El tiempo de expiración del timer inicializado. Aunque parece una tarea simple, hay mucha lógica por debajo. Debido a que el Kernel de Linux funciona a nivel de hardware, para convertir el tiempo humano en tiempo de computadora tiene que usar algunas fórmulas que se basan en la constante HZ que varía según la arquitectura. Además, aparece el concepto de Jiffies. No entraré en más detalles, sin embargo [4] explica muy bien esta parte. Finalmente, llama a la función do_nanosleep que tiene la lógica que envía un proceso a dormir. La función hrtimer_init_sleeper_on_stack (que al final llama a __hrtimer_init_sleeper) asigna e inicializa un High Resolution Timer asociado con el proceso actual que se está ejecutando.\nEl atributo function del High Resolution Sleep Timer es la función callback, lo que significa que este atributo function se llamará después de que expire el High Resolution Timer. En este caso el valor del atributo function es la función hrtimer_wakeup que veremos más adelante (no se olviden de esto \u0026#x1f609;).\nstatic void __hrtimer_init_sleeper(struct hrtimer_sleeper *sl, clockid_t clock_id, enum hrtimer_mode mode) { ... __hrtimer_init(\u0026amp;scicil-\u0026gt;timer, clock_id, mode); sl-\u0026gt;timer.function = hrtimer_wakeup; // \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; This function will be called after the timer expires sl-\u0026gt;task = current; // \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; Associates the timer with the current process } En el kernel de Linux, la variable current es un puntero al proceso actual que se está ejecutando (en nuestro caso, el programa que llama a la función sleep).\nAntes de continuar con la función do_nanosleep, haré un paréntesis sobre la función __hrtimer_init.\nstatic void __hrtimer_init(struct hrtimer *timer, clockid_t clock_id, enum hrtimer_mode mode) { ... timerqueue_init(\u0026amp;timer-\u0026gt;node); } Mencioné que los High Resolution Timers usan una cola que por debajo es implementada utilizando un Red Black Tree. La llamada a las funciones timerqueue_init solo asigna e inicializa un nodo del Red Black Tree, sin embargo este nodo no es agregado al árbol aún.\nDespués de ese breve paréntesis, veamos qué sucede dentro de la función do_nanosleep.\nInicialmente pensé que el ciclo do/while itera hasta que High Resolution Timer expire (algo como un bucle infinito), sin embargo las cosas suceden de manera diferente.\nstatic int __sched do_nanosleep(struct hrtimer_sleeper *t, enum hrtimer_mode mode) { struct restart_block *restart; do { set_current_state(TASK_INTERRUPTIBLE); // \u0026lt;\u0026lt;\u0026lt;\u0026lt; This causes the process to go to a Sleeping state hrtimer_sleeper_start_expires(t, mode); if (likely(t-\u0026gt;task)) freezable_schedule(); hrtimer_cancel(\u0026amp;t-\u0026gt;timer); mode = HRTIMER_MODE_ABS; } while (t-\u0026gt;task \u0026amp;\u0026amp; !signal_pending(current)); __set_current_state(TASK_RUNNING); if (!t-\u0026gt;task) return 0; restart = \u0026amp;current-\u0026gt;restart_block; if (restart-\u0026gt;nanosleep.type != TT_NONE) { ktime_t rem = hrtimer_expires_remaining(\u0026amp;t-\u0026gt;timer); struct timespec64 rmt; if (rem \u0026lt;= 0) return 0; rmt = ktime_to_timespec64(rem); return nanosleep_copyout(restart, \u0026amp;rmt); } return -ERESTART_RESTARTBLOCK; } Como se mencionó anteriormente, cuando se llama a la función sleep, el proceso actual pasará al estado Sleeping. Podemos ver que eso sucede en la línea que llama a la función set_current_state que cambia el estado del proceso actual a TASK_INTERRUPTIBLE (Sleeping).\nLa llamada a la función hrtimer_sleeper_start_expires llamará a otras funciones hasta que llame a __hrtimer_start_range_ns que a su vez llamará a enqueue_hrtimer, es en este punto donde el timer (nodo) inicializado antes se agrega a la estructura del Red Black Tree para que el timer pueda ser procesado más tarde.\nFinalmente, la función freezable_schedule invoca al process scheduler para que haga schedule de otro proceso, porque el proceso actual current entró en suspensión y la ejecución de nuestro proceso se detiene aquí.\nCómo Despierta El Proceso? Hasta ahora hemos comprobado que la implementación de nanosleep cambia el estado del proceso a TASK_INTERRUPTIBLE y pausa la ejecución del proceso.\nAhora que el estado del proceso está en el estado TASK_INTERRUPTIBLE, el process scheduler no considerará la ejecución del proceso hasta que el estado del proceso vuelva a TASK_RUNNING.\nMencionamos que el Hardware Timer causa interrupciones periódicas para que el Kernel de Linux pueda manejarlas llamando a la función hrtimer_interrupt en cada interrupción (varias veces en un segundo). Es en esta función donde se procesan los High Resolution Timers llamando a la función __hrtimer_run_queues.\nstatic void __hrtimer_run_queues(struct hrtimer_cpu_base *cpu_base, ktime_t now, unsigned long flags, unsigned int active_mask) { struct hrtimer_clock_base *base; unsigned int active = cpu_base-\u0026gt;active_bases \u0026amp; active_mask; for_each_active_base(base, cpu_base, active) { struct timerqueue_node *node; ktime_t basenow; basenow = ktime_add(now, base-\u0026gt;offset); while ((node = timerqueue_getnext(\u0026amp;base-\u0026gt;active))) { struct hrtimer *timer; timer = container_of(node, struct hrtimer, node); if (basenow \u0026lt; hrtimer_get_softexpires_tv64(timer)) break; __run_hrtimer(cpu_base, base, timer, \u0026amp;basenow, flags); if (active_mask == HRTIMER_ACTIVE_SOFT) hrtimer_sync_wait_running(cpu_base, flags); } } } La función __hrtimer_run_queues iterará los timers en el Red Black Tree, recuerden que iterará comenzando por los timers que están más próximos a expirar. Algo a tener en cuenta aquí es que romperá el ciclo while si el timer aún no ha expirado (¿por qué tener iteraciones innecesarias si el header de la cola es un timer que no ha caducado aún?). Pero cuando el timer expire, llamará a la función __run_hrtimer. Como podemos ver, su implementación llamará al callback que configuramos durante la inicialización del High Resolution Timer.\nstatic void __run_hrtimer(struct hrtimer_cpu_base *cpu_base, struct hrtimer_clock_base *base, struct hrtimer *timer, ktime_t *now, unsigned long flags) __must_hold(\u0026amp;cpu_base-\u0026gt;lock) { ... fn = timer-\u0026gt;function; // \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; This fn function is pointing to the hrtimer_wakeup function ... restart = fn(timer); ... } La función que se configuró como el callback durante la inicialización del High Resolution Timer fue la función hrtimer_wakeup.\nstatic enum hrtimer_restart hrtimer_wakeup(struct hrtimer *timer) { struct hrtimer_sleeper *t = container_of(timer, struct hrtimer_sleeper, timer); struct task_struct *task = t-\u0026gt;task; t-\u0026gt;task = NULL; if (task) wake_up_process(task); // \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; Wake up the process!! return HRTIMER_NORESTART; } Como podemos ver, esta función llamará a la función wake_up_process enviando el proceso (tarea) asociado con el High Resolution Timer como parámetro. La función wake_up_process, entre otras cosas, establecerá el estado del proceso en TASK_RUNNING.\nAlgunos ciclos de CPU más tarde, el process schduler reanudará la ejecución de nuestro proceso donde se detuvo (después de la llamada a la función freezable_schedule). Luego, el resto de la función do_nanosleep liberará memoria, eliminará el timer del Red Black Tree y continuará con la ejecución. ¡Y eso es todo!\nHay otras alternativas a nanosleep La syscall nanosleep no es la única syscall que se puede usar para dormir un proceso. Por ejemplo, la función time.sleep de Python usa la syscall select por detrás, sin embargo, si revisamos la implementación de do_select que a su vez llama a la función schedule_hrtimeout_range, se nota que llama a la función schedule_hrtimeout_range que inicializa un High Resolution Timer y le dice al process scheduler que haga schedule de otro proceso (la misma lógica que con nanosleep).\nPython sleep es un ejemplo, pero otros lenguajes posiblemente usan otras syscalls.\nComentarios Finales Aunque llamar a la función sleep en nuestros programas puede ser algo trivial, todos los mecanismos que viven detrás de esa simple función son asombrosos. Cuando comencé a investigar para entender qué sucede cuando llamas a una función sleep, no me hubiera imaginado cuánto iba a aprender.\nEn caso de que haya partes de este post que sean incorrectas, puedenm abrir un issue en el repositorio de Github de este blog. ¡Muchas gracias!.\nReferencias [0] https://lwn.net/Articles/167897/ [1] https://lwn.net/Articles/152436/ [2] https://www.kernel.org/doc/html/latest/timers/hrtimers.html [3] https://www.kernel.org/doc/html/latest/timers/highres.html [4] https://www.youtube.com/watch?v=Puv4mW55bF8 ","permalink":"https://blog.donkeysharp.xyz/es/post/what-happens-when-a-process-goes-to-sleep/","summary":"\u003cp\u003eEs posible que cuando estas escribiendo un programa, en algún momento necesites pausar la ejecución de un proceso llamando a la función \u003ccode\u003esleep(NUMBER_OF_SECONDS)\u003c/code\u003e dependiendo del problema que estés resolviendo. En este post, compartiré lo que aprendí hasta ahora mientras investigaba los mecanismos internos del kernel que hacen que la función \u003ccode\u003esleep\u003c/code\u003e funcione de la forma en que lo hace.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eAgradezco su feedback.\u003c/strong\u003e No soy un experto en este tema ya que las partes internas del Kernel de Linux son nuevas para mí, fue solo mi curiosidad lo que me llevó a revisar el código fuente del Kernel y quería compartir lo que aprendí. Si encuentran algo incorrecto en este post, pueden abrir un issue en el \u003ca href=\"https://github.com/donkeysharp/donkeysharp.github.io\"\u003erepositorio de Github\u003c/a\u003e de este blog. Gracias!.\u003c/p\u003e","title":"Qué sucede cuando un proceso de Linux se va a dormir?"},{"content":"Cuando aún estaba en secundaria y tenía 15 años, solo por esa gestión nos tocó un nuevo profesor de Física. Su nombre era Elias Quispe. Una manera de enseñar diferente presentando varios retos y sobre todo rompiendonos la cabeza con problemas complejos de Física (resulta que también era docente de Física en la Universidad).\nEn una de las clases mencionó que la tarea estaba en su página web y yo era \u0026ldquo;woooooooow!, cómo habrá logrado tener su página web?\u0026rdquo; y bueno entre a su sitio, me gustó, también tenía foro y recursos interesantes.\nDespúes de haber visto eso me dije a mi mismo \u0026ldquo;también quiero mi página web\u0026rdquo; y como muchas cosas en esta vida entre a Google y busqué: \u0026ldquo;como crear páginas web\u0026rdquo; y llegué a un sitio llamado Web Estilo. Luego pude participar en foros de diferentes temas: programación, hacking, etc. Había encontrado algo que me gustaba hacer sin que nadie me lo ordenase.\nHoy me enteré con tristeza que el profesor Elias falleció y me siento triste. Sin mover un dedo o hablar directamente conmigo hizo mucho por mí. Gracias a él tuve ese chispaso de curiosidad que practicamente me guió a hacer algo que me apasiona y desde ese día hasta hoy no dejé de disfrutarlo y esa chispa de curiosidad sigue y es más grande.\nSe que hay muchos estudiantes al igual que yo que lamentan su partida y estoy seguro que no fuí el único estudiante en el que tuvo impacto.\nGracias profe Elías.\n","permalink":"https://blog.donkeysharp.xyz/es/post/gracias-profe-elias/","summary":"\u003cp\u003eCuando aún estaba en secundaria y tenía 15 años, solo por esa gestión nos tocó un nuevo profesor de Física. Su nombre era Elias Quispe. Una manera de enseñar diferente presentando varios retos y sobre todo rompiendonos la cabeza con problemas complejos de Física (resulta que también era docente de Física en la Universidad).\u003c/p\u003e\n\u003cp\u003eEn una de las clases mencionó que la tarea estaba en su página web y yo era \u0026ldquo;woooooooow!, cómo habrá logrado tener su página web?\u0026rdquo; y bueno entre a su sitio, me gustó, también tenía foro y recursos interesantes.\u003c/p\u003e","title":"Gracias Profe Elias"},{"content":"Hola, durante los meses de Octubre y Noviembre sucedieron diferentes conflictos sociales y políticos en Bolivia, esta entrada no es tanto para discutir el tema político, es más será una entrada 100% técnica pero se encuentra relacionada con estos hechos.\nLa anterior semana una gran cantidad de clientes de un ISP local recibieron un SMS con un link de bit.ly a un video MP4 en Dropbox que después fue dado de baja.\nEste video que fue viral no solo por SMS sino en redes sociales y medios de comunicación locales, mostraba la llamada que en la que un dirigente local habla con Evo Morales.\nCorrieron rumores de que el video era un malware o era utilizado para hacer tracking de las personas que lo abriesen, la verdad no suelo tirar mucha bola a eso, pero esta vez me interesó porque días atrás Facebook habia notificado una falla de seguridad de Stack Buffer Overflow que posiblemente podría generar RCE en la aplicación de Whatsapp justamente con un video MP4 malicioso!. Este es el link de la alerta de seguridad.\nBueno, me dió mucha curiosidad ver que había o si efectivamente había algo malicioso en ese video o simplemente era spam. De entrada sabía que aprendería varias cosas porque muy poca idea tenía de como podría analizar si el video era malicioso o no y bueno ya con el video que lo enviaron a un grupo público de chat, mis amigos Gonzalo y Cristhian me dijeron que ponga una entrada en mi blog sobre lo que encuentre y de no encontrar nada que hable sobre los ángulos de grabación del video XD. Bueno el resto del blog será sobre lo que encontré y aprendí. Gracias por animarme a investigar muchachos, me divertí mucho.\nEl video El video justamente era un archivo mp4 llamado evo-telefono.mp4 con este sha512:\na378c367e3c9a4be3ca639822fe79adf75aaa30ba25ca97ff8f6eb3945d36ed9eb160703ed611ecfe5fdc448c6a099e8af3a74a2c7078695db9c258a25800246 Primeramente verificar que efectivamente es un mp4 viendo los magic numbers del fichero. Generalmente los magic numbers de cualquier archivo son los primeros bytes de un archivo.\nEn el caso de un mp4 los bytes deberían ser: 00 00 00 (18 o 1C) 66 74 79 70 6D 70 34 32 y al correr:\n$ hexdump -C evo-telefono.mp4 | head -n1 00000000 00 00 00 1c 66 74 79 70 6d 70 34 32 00 00 00 01 |....ftypmp42....| Efectivamente tiene esos magic numbers que identifican a un mp4. Una forma más simple de verificar es utilizando el comando file:\n$ file evo-telefono.mp4 evo-telefono.mp4: ISO Media, MP4 v2 [ISO 14496-14] Primeras ideas Para ver si encuentro algo interesante ejecuté el comando strings contra el video y ver si encontraba alguna cadena ASCII interesante, como la vulnerabilidad explica que el error radica en la metadata del archivo, entonces imaginé que la estructura era algo interno como \u0026ldquo;clave-valor\u0026rdquo; todo en ASCII, esa suposición la descarté al no encontrar nada interesante usando strings.\n$ strings evo-telefono.mp4 La siguiente idea fue utilizar una herramienta que puede ver la metadata de diferentes formatos que se llama mediainfo y mediainfo-gui. Para esta primera etapa del análisis utilicé mediainfo porque no entendía muy bien la forma en como se presentaba con mediainfo-gui, pero esta gui más adelante fue de mucha más utilidad.\nUtilizando mediainfo contra el video evo-telefono.mp4 obtuve la siguiente salida, pondré solo ciertas partes pero dejo este gist con la salida completa del comando.\n$ mediainfo evo-telefono.mp4 General Complete name : evo-telefono.mp4 Format : MPEG-4 Format profile : Base Media / Version 2 Codec ID : mp42 (isom/mp41/mp42) File size : 6.41 MiB Duration : 1 min 2 s Overall bit rate : 857 kb/s Encoded date : UTC 2019-11-21 12:31:56 Tagged date : UTC 2019-11-21 12:31:59 ... Visualmente esa información es clave-valor pero gran parte de esas cadenas no se encontraba cuando utilicé strings lo cual me lleva a la conclusión que el formato es en su mayoría binario y que los números en esta salida no estan representados como una cadena ASCII sino en bytes similar a un paquete IP o TCP.\nNota: un paquete IP es binario en el sentido que la IP y otros flags no estan en modo texto ASCII sino encapsulados en bytes. Por ejemplo la ip 10.0.1.11 (9 bytes en ASCII) se representa en 4 bytes como 0A 00 01 0B\nEn este punto sentí que estaba pateando oxígeno y que no llegaría a ningún lado. Lo siguiente que hice es buscar si había ya algún exploit o tutorial de como explotar este CVE y efectivamente con la ayuda de Google llegue a este repositorio en Github.\nEste repo tenía un mp4 llamado poc.mp4 que en teoría explotaba esta vulnerabilidad y junto a este archivo la librería dinámica de Whatsapp libwhatsapp.so y un programa en C que invoca esta librería dinámicamente usando dlfcn.h (espero hacer un post sobre dlfcn.h en el futuro, súper interesante). Lo más importante de este repositorio que me ayudó fue tener una muestra de algo que sí causa este error.\nLo primero que hice fue ejecutar nuevamente mediainfo contra poc.mp4 y ver las diferencias entre evo-telefono.mp4 y poc.mp4, tristemente fue más frustrante ya que lo único diferente era un nuevo tag llamado com.android.version con el valor de 9 en ASCII y bueno, ya me quedé sin ideas. Al inicio pense que junto a este tag viendo el hexadecimal tal vez había un shellcode, buscando opcodes comunes y eso, pero realmente sentía que la estrategia que estaba utilizando era bastante \u0026ldquo;naive\u0026rdquo; y no estaba entendiendo el formato MP4 como tal y bueno, creo que ese era el siguiente paso. La mayoría de los archivos tiene una especificación de como están estructurados, ya sea en texto plano en un formato como json o xml o en binario como el caso de MP4. Busqué en Google los spec files, le dí una leida super rápida a lo que encontré para ver si mencionaba cosas como bytes y cosas así y no encontré uno como tal y bueno ahí pausé por un día este análisis para descansar.\nEntendiendo el formato MP4 y la vulnerabilidad Horas después que pausé, mi amigo Elvin envió este link de Hack A Day que da un resumen de la vulnerabilidad y somo esta siendo explotada, muchísimas gracias por compartirlo fue uno de los recursos más importantes para esta investigación. La verdad cuando lo leí no entendí ciertos detalles que justamente eran los más importantes, no entendía aún la estructura de un archivo mp4.\nDesde acá todos los pasos que sigo son solamente con el archivo poc.mp4 y al final aplicaré lo aprendido a evo-telefono.mp4.\nLo primero que hice es tratar de reproducir el único paso que menciona en el post de Hack A Day con la herramienta AtomicParsley contra poc.mp4. Al ejecutarlo me salió un Segmentation Fault pero lo mismo con evo-telefono.mp4, al parecer es más un error de la herramienta que ya anda descontinuada.\n$ AtomicParsley poc.mp4 -T Atom ftyp @ 0 of size: 24, ends @ 24 Atom moov @ 24 of size: 794, ends @ 818 Atom mvhd @ 32 of size: 108, ends @ 140 Atom meta @ 140 of size: 117, ends @ 257 Atom @ 152 of size: 6943, ends @ 7095\t~ ~ denotes an unknown atom ------------------------------------------------------ Total size: 7095 bytes; 4 atoms total. Media data: 0 bytes; 7095 bytes all other atoms (100.000% atom overhead). Total free atom space: 0 bytes; 0.000% waste. ------------------------------------------------------ AtomicParsley version: 0.9.6 (utf8) ------------------------------------------------------ Segmentation fault Como se ve en la salida un montón de info que no entendía xD, pero algo que si mencionaban en el post de Hack A Day es la posición en bytes y que MP4 es una estructura jerárquica y la estructura básica de de MP4 es el \u0026ldquo;Atom\u0026rdquo; (existen diferentes tipos de atoms). Más adelante hablaré con más detalle sobre los Atoms.\nCada atom tiene una cabecera que indica el size del atom. Al ser una estructura jerárquica un atom puede contener otros atoms dentro. Al ser así, el tamaño de un atom padre es el total de todos los bytes de los atom hijos y lo que se resalta y es mencionado en el post es lo siguiente:\nEl atom meta tiene un tamaño de 117 bytes pero dentro de este atom hay un atom hijo sin nombre que tiene un tamaño de 6943 bytes que es mayor a los 117 bytes del padre y bueno eso da una pista.\nAtom @ 152 of size: 6943, ends @ 7095 ~ En el post posteriormente hace referencia a 33 bytes y 1.6GB del size del atom y bueno ahí me perdí y eso era efectivamente la clave para entender el error.\nLo siguiente en hacer \u0026ndash;ya lo había procrastinado suficiente\u0026ndash; era leer las especificaciones de un archivo MP4. De los archivos que conseguí ninguno era al nivel que quería, es decir, a nivel de bytes. Por suerte, una vez más el post de Hack A Day hace referencia a dos documentos: la especificación en el sitio de Apple Developers y otra especificación un poco más rebuscada y es donde se llega a entender completamente este error.\nEl formato MP4 Como resumen super corto tras leer la especificación se puede decir que Mp4 está organizado jerárquicamente en bloques llamados Atom (lo que mencioné arriba) y cada Atom tiene una cabecera de 8 bytes, 4 bytes definen el tamaño del Atom y los otros 4 bytes (generalmente en ASCII) representan el tipo del Atom.\nAhora existen varios tipos de Atoms pero los que se muestran en el post son los siguientes:\nmoov que representa lo que es \u0026ldquo;Movie Data\u0026rdquo; que puede tener otros Atoms. Basicamente el contenido de este atom es información de la película e.g. cuando se creó, duración, etc. meta otro atom que encapsula información de Metadata hdlr un atom que es considerado el handler y viene dentro del atom meta, este atom define toda la estructura que tendrá toda la metadata dentro del atom meta La siguiente imagen muestra la representación gráfica de los atoms en forma de caja:\n¿Pero cómo se puede entender este formato a nivel binario? Esta parte me tomó un poco de tiempo pero al final utilizando mediainfo-gui y un editor hexadecimál fue algo mucho más simple.\nUn atom tiene una cabecera de 8 bytes donde los primeros 4 bytes indican el tamaño del atom y los siguientes 4 bytes el tipo de atom e.g. moov, meta, hdlr entre otros y luego vienen N bytes que son el contenido del atom, donde N es el tamaño del atom especificado en los primeros 4 bytes restando 8 bytes (la cabecera).\nUn ejemplo:\nUn atom de 794 bytes de tamaño de tipo moov se representaría como:\n00 00 03 1A 6D 6F 6F 76 XX XX XX ... 786 bytes ... XX XX Según la especificación los primeros 4 bytes son el tamaño, los siguientes 4 bytes son el tipo de atom y el resto es el cuerpo.\nLos primeros 4 bytes se pueden representar como 0x0000031A o 0x31A que en decimal es 794.\nLos siguientes 4 bytes indican el tipo de atom que es texto ASCII, entonces solo es convertir los siguientes bytes a su caracter en ASCII y tendremos:\n6D -\u0026gt; m 6F -\u0026gt; o 6F -\u0026gt; o 76 -\u0026gt; v Ahora el contenido de este atom (los restante 786 bytes) pueden ser otros atoms identificados de la misma forma y en base a la especificación del formato MP4.\nExisten casos especiales de algunos Atoms que tienen un formato especial. Un ejemplo de estos atoms especiales es que después del header no definimos directamente otro atom, es posible que algunos bytes esten reservados con algún propósito (flags, etc) y luego de estos bytes reservados recién es posible definir atoms hijos. Recuerden este párrafo ya que verán es la llave al éxito.\nSiguiendo con ejemplos en poc.mp4, hay un atom llamado mdta que es basicamente el nombre de key en la metadata (este atom tiene el key com.android.version que mencioné más arriba). Al igual que otro atom se lo representa con un header de 8 bytes y luego el contenido:\n00 00 00 1B 6D 64 74 61 63 6F 6D 2E 61 6E 64 72 6F 69 64 2E 76 65 72 73 69 6F 6E Donde:\n0x0000001B representa el tamaño que en decimal es 27 bytes 6D 64 74 61 representa en ASCII mdta el tipo del atom 63 6F 6D 2E 61 6E 64 72 6F 69 64 2E 76 65 72 73 69 6F 6E convirtiendo a ASCII representa com.android.version Para no hacer muy largo el post he creado un video donde muestro con más detalle cómo interpretar a nivel hexadecimal este formato utilizando mediainfo-gui.\nEntendiendo el bug En el anterior video se ve como entender y navegar por los diferentes atoms tanto como el visualizador de atoms mediainfo-gui como también a nivel hexadecimal. En esta parte utilizando el conocimiento adquirido hasta ahora se verá cómo el bug reportado en el CVE puede utilizarse.\nParte de CVE-2019-11931: The issue was present in parsing the elementary stream metadata of an MP4 file and could result in a DoS or RCE\nEste CVE y la forma como causar el overflow justamente dice que esta en la metadata, es decir, en el Atom meta. En el post de Hack A Day hace referencia a dos especificaciones del formato mp4. El link de Apple Developers indica que después de definir el atom de tipo meta como hijo se debería definir un atom de tipo hdlr y si vemos en el hexadecimal, sucede exactamente eso desde el offset 8C como muestra las siguientes imágenes.\nheader size meta type header size hdlr type ----------- ----------- ----------- ----------- 00 00 00 75 6d 65 74 61 00 00 00 21 68 64 6c 72 ... 0x00000075 meta 0x00000021 hdlr 0x75 o 117 meta 0x21 o 33 hdlr Lo que se ve en mediainfo-gui y en el hexadecimal tiene mucho sentido, pero si recordamos la salida de la aplicación AtomicParsley no sale en ningún momento el atom hdlr que efectivamente esta definido y en lugar de eso muestra un error de que un atom sin nombre tiene tamaño de 6943 bytes (mayor a los 117 de su atom padre meta).\n$ AtomicParsley poc.mp4 -T Atom ftyp @ 0 of size: 24, ends @ 24 Atom moov @ 24 of size: 794, ends @ 818 Atom mvhd @ 32 of size: 108, ends @ 140 Atom meta @ 140 of size: 117, ends @ 257 Atom @ 152 of size: 6943, ends @ 7095\t\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; atom sin nombre ~ denotes an unknown atom ------------------------------------------------------ Total size: 7095 bytes; 4 atoms total. Media data: 0 bytes; 7095 bytes all other atoms (100.000% atom overhead). Total free atom space: 0 bytes; 0.000% waste. ------------------------------------------------------ AtomicParsley version: 0.9.6 (utf8) ------------------------------------------------------ Segmentation fault Todos los atoms en la salida muestran su tipo ftyp, moov, mvhd, meta y dentro de meta este atom desconocido con un tamaño que es basicamente el resto del tamaño de poc.mp4 que pesa 7095 bytes, ya que si se hacen cuentas sumando el tamaño de del atom ftyp (24), mvhd (108), el header de meta (8)resulta 24 + 108 + 8 = 148 pero 7095 - 148 = 6947 que son 4 bytes extras de los 6943 que da en la salida de AtomicParsley.\nQué son estos 4 bytes?\nSe ve que en mediainfo-gui todo se muestra bien, como si nada hubiera pasado y el formato esta correcto. Sin embargo, en AtomicParsley muestra un atom sin ningún tipo y tenemos un excedente en 4 bytes haciendo sumas y restas con la salida de AtomicParsley.\nLo que sucede y logré deducir es lo siguiente, si vemos el tipo de archivo de poc.mp4 suando el comando file se ve que es ISO Media, MP4 v2 [ISO 14496-14]. Pasa que el formato mp4 esta en base al estándar ISO 14496-14 que es la continuación de otro estándar llamado ISO 14496-12 y lo más interesante viene aca: en el post de Hack A Day menciona dos links de especificaciones el de Apple Developers y uno un poco más rebuscado. En el segundo link indica claramente que es la especificación del estándar ISO/IEC 14496-12 y justamente en este es donde indica que el atom meta despúes del header de 8 bytes tiene 4 bytes reservados (3 para flags y 1 para versión) y después de estos 4 bytes recién se pueden definir otros atoms hijo.\nDe ser así y volviendo a analizar el hexadecimal se tiene lo siguiente:\n4 bytes header size meta type reservados header size type inválido ----------- ----------- ----------- ----------- ----------- 00 00 00 75 6d 65 74 61 00 00 00 21 68 64 6c 72 00 00 00 00 ... Sabiendo eso, los 4 bytes hacen que la definición del atom hijo se recorra y tenga la cabecera 68 64 6c 72 00 00 00 00 donde 0x68646c72 es el size y 00 00 00 00 es el type que en ASCII que no es nada válido y justamente eso explica el porqué del fallo de la aplicación AtomicParsley. Ahora si se convierte 0x68646c72 a decimal se obtiene el valor de 1751411826 es decir, el tamaño de ese atom desconocido sería de todo esa gran cantidad de bytes que llega a ser 1.6GB (igual que en el post de Hack A Day).\nSabiendo esto, ya pude reproducir con confianza el archivo poc.mp4 xD.\nLo que deduzco es lo siguiente: la librería libwhatsapp.so (repo) posiblemente esta obedeciendo el estándar ISO 14496-12 y no el ISO 14496-14 o simplemente fue un error de desarrollo ya que en el update actual de Whatsapp esto no sucede. Ahora relacionado a la aplicación AtomicParsley muy probable que suceda lo mismo ya que esta si tuvo problemas con ese atom desconocido.\nAnalizando el archivo evo-telefono.mp4 La verdad hasta este punto estaba feliz por lo mucho que había aprendido, pero faltaba el objetivo principal que era analizar si este archivo tenía algo malicioso específicamente por este CVE.\nDe entrada puedo decir que NO (al menos no con este CVE).\nLas razón es la siguiente: para poder causar este error se necesita tener definido el atom meta el cual no se define dentro de ningún atom en el video evo-telefono.mp4. De todos modos es la primera vez que analizo un archivo a este nivel y bueno, en algunos grupos de chat mencionaron Pegasus y eso, no estaría de más darle una revisada a eso \u0026#x1f604;.\nComentarios Finales En serio que fue una experiencia buena en cuanto a aprendizaje y ver este tipo de vectores de ataque que se aprovechan de este tipo de detalles.\nOtra cosa aprendida que para este tipo de análisis, al igual que al analizar protocolos de red es necesario leer el RFC, en el caso de formatos es necesario leer la especificación del formato de algún archivo. Hay un ezine llamado Paged Out que en una de sus secciones habla de técnicas de hacer reversing a formatos de archivos, se los recomiendo. Link.\nFinalmente agradecer nuevamente a Elvin por compartir ese post de Hack A Day que fue clave para este análisis y a Gonzalo y Cris por animarme a hacerlo, les debo una cerveza a los tres \u0026#x1f604;.\n","permalink":"https://blog.donkeysharp.xyz/es/post/analisis-video-evo/","summary":"\u003cp\u003eHola, durante los meses de Octubre y Noviembre sucedieron diferentes conflictos sociales y políticos en Bolivia, esta entrada no es tanto para discutir el tema político, es más será una entrada 100% técnica pero se encuentra relacionada con estos hechos.\u003c/p\u003e\n\u003cp\u003eLa anterior semana una gran cantidad de clientes de un ISP local recibieron un SMS con un link de \u003ccode\u003ebit.ly\u003c/code\u003e a un video MP4 en Dropbox que después fue dado de baja.\u003c/p\u003e","title":"Análisis De Video En Busca De Supuesto Malware"},{"content":"Hola, hace un tiempo publiqué sobre el setup inicial que tengo en los equipos Debian con los que trabajo, como ser: aplicaciones, configuraciones de apariencia del escritorio, etc. Los últimos meses instalé y reinstalé Debian varias veces en los equipos con los que trabajo (nuevos, actualizaciones de un nuevo disco duro, etc.) y ya se volvió una tarea repetitiva.\nBásicamente lo que realizaba es revisar mi anterior entrada de blog y repetía esas acciones. Hasta ahora me funcionó, pero al volverse repetitivo me animé en automatizar todo este proceso tanto de instalación y configuración de ciertas aplicaciones como el manejo de la apariencia del escritorio (con el setup que siempre utilizo).\nLa Idea Si bien podría haber hecho este proyecto utilizando un simple script Bash, me animé por utilizar Bash + Ansible a modo de practicar y divertirme \u0026#x1f604;.\nLo que tenía en mente al momento de iniciar este proyecto fue que ni bien termine la instalación del sistema operativo (en mi caso la distribución Debian), solo tendría que ejecutar un comando y \u0026ldquo;mágicamente\u0026rdquo; todas las apliaciones, configuraciones, etc. sobre mi setup personal se aplicarían.\nAnalizando el Proyecto Este proyecto lo publiqué en Github si desean ver el código fuente.\nAl ser este un proyecto que utiliza Ansible, existe cierta convención en cuestión a árbol de directorios, nombres de archivos, etc. Decidí utilizar la siguiente estructura:\n├── init-setup.sh ├── inventory ├── README.md ├── roles │ ├── chrome │ ├── common │ ├── docker │ ├── dotenvs │ │ ├── tasks │ │ └── templates │ ├── games │ │ └── tasks │ │ └── main.yml │ ├── mysql │ │ ├── files │ │ └── tasks │ ├── node │ ├── php7 │ ├── virtualbox │ ├── vscode │ └── xfce4 ├── setup-playbook.yml └── update-desktop-layout.sh En Ansible lo que se denominaría como el programa principal es el playbook el cual se encarga de ejecutar diferentes tareas en contra de uno o más servidores, en este caso, el programa principal de Ansible sería el archivo setup-playbook.yml. Como se puede ver este archivo tiene una sección llamada roles.\nNota: En Ansible lo que se denomina role llega a ser una pieza de código reutilizable (como un módulo). Esto nos permite tener el proyecto mejor organizado y cuenta con una estructura interna de directorios como se ve en el árbol de directorios de arriba.\nDividí el proyecto en diferentes roles, cada uno para un diferente propósito como ser: una aplicación específica o un grupo de aplicaciones incluidas sus configuraciones. Por ejemplo, en el role common instala todas las utilidades de escritorio y las utilidades de línea de comandos que utilizo día a día. Y en general hay roles para cosas específicas que utilizo, Docker, herramientas y tecnologías de desarrollo de software y otros. Si tienen más curiosidad pueden revisar el repo \u0026#x1f604;.\nSiguiendo con la explicación, dentro de Ansible existe un concepto bastante importante que es el inventory, el cual indica todos los servidores en los que se aplicarán las tareas especificada en los roles. Este proyecto tiene un archivo llamado inventory con opciones particulares, ya que todas las tareas no se ejecutarán en contra de varios servidores, sino en contra de uno solo y es la misma máquina local.\n[local] localhost ansible_connection=local Lo explico: [local] es el grupo de servidores, yo lo denominé local pero podría llamarse cualquier cosa, lo importante es que si se cambiase de nombre, este nombre también debería reflejarse en setup-playbook.yml en hosts: local. Luego la siguientes líneas indican el hostname que en este caso es localhost y ansible_connection=local que indica que será una ejecución local y de ese modo evitar el proceso de autenticación por SSH que Ansible realiza en cada ejecución hacia la misma máquina.\nFinalmente el script init-setup.sh es un wizard el cual pregunta por ciertas opciones antes de realizar todo el proceso de instalación y configuración. Este llegaría a ser el comando \u0026ldquo;mágico\u0026rdquo; que se encarga de todo:\nbash \u0026lt;(wget -q -O- https://raw.githubusercontent.com/sguillen-proyectos/fresh-install-setup/master/init-setup.sh) ¿Qué gano con esto? Bueno, primero aprendí un par de cosas que no sabía sobre Ansible, me divertí y lo más importante para mí (además que era el objetivo de este proyecto) es que ahora me ahorro todo el tiempo de configuración manual que realizaba en un sistema operativo recién instalado.\nSi bien ya tenía mi guía de qué paquetes instalar y que configuraciones realizar, eso me tomaba entre una a dos horas, ahora todo este tiempo de setup esta principalmente condicionado a la velocidad de internet.\nAlgo en este proyecto que es bastante útil para mí, es que logré uniformizar mis configuraciones de escritorio (en este caso Xfce4), ya que en muchas ocaciones el \u0026ldquo;estilizar\u0026rdquo; mi escritorio es en lo que más perdía mi tiempo.\nComentarios Finales Si bien este proyecto solo esta orientado para mi setup personal, decidí compartirlo por si la idea de automatizar el environment le sirve a alguna de las personas que lean esta entrada.\nDurante el tiempo que invertí para realizar este proyecto sucedieron varias cosas divertidas y que me hicieron renegar de las cuales aprendí algo nuevo. Escribiré sobre esa serie de eventos desafortunados en una siguiente entrada.\nHappy Hacking!\n","permalink":"https://blog.donkeysharp.xyz/es/post/automatizando-setup-post-instalacion-debian/","summary":"\u003cp\u003eHola, hace un tiempo \u003ca href=\"/es/post/configuracion-de-escritorio-debian\"\u003epubliqué sobre el setup inicial\u003c/a\u003e que tengo en los equipos Debian con los que trabajo, como ser: aplicaciones, configuraciones de apariencia del escritorio, etc. Los últimos meses instalé y reinstalé Debian varias veces en los equipos con los que trabajo (nuevos, actualizaciones de un nuevo disco duro, etc.) y ya se volvió una tarea repetitiva.\u003c/p\u003e\n\u003cp\u003eBásicamente lo que realizaba es revisar mi anterior entrada de blog y repetía esas acciones. Hasta ahora me funcionó, pero al volverse repetitivo me animé en automatizar todo este proceso tanto de instalación y configuración de ciertas aplicaciones como el manejo de la apariencia del escritorio (con el setup que siempre utilizo).\u003c/p\u003e","title":"Automatizando setup post-instalación en mis máquinas Debian"},{"content":"El otro día cenando con mi amigo Francisco, él me comentaba que tiene un proyecto personal en mente y lo que desea conseguir. Escuchando sus preguntas la primera herramienta que pasó por mi cabeza para resolver algunos de sus problemas fue utilizar OpenSSH.\nEsta entrada la hago principalmente para mi amigo Francisco pero la redactaré de forma general para que el benficio sea general.\nQué es SSH? SSH significa Secure Shell y es un protocolo para administrar servicios en una red mediante un canal cifrado (he ahí el porqué de \u0026ldquo;Secure\u0026rdquo; \u0026#x1f609;). Algunas de las tareas más comunes que se pueden realizar con este protocolo es la de poder iniciar sesión en un servidor o la ejecución remota de comandos.\nAl ser SSH solamente un protocolo necesitamos una herramienta que implemente dicho protocolo. La más utilizada es OpenSSH y es la herramienta que utilizaremos para esta guía. OpenSSH u otras implementaciones vienen por defecto en sistemas Unix-like (OSX, OpenBSD, FreeBSD, Linux, etc.) y en el caso de Windows personalmente yo instalo Git Bash ya que es una terminal Unix-like en Windows. Otros prefieren utilizar PuTTY.\nEnsuciandonos las manos Para esta guía si bien podríamos probarlo con una máquina virtual o una PC con Linux instalada en una red local o incluso en la misma máquina local, prefiero hacerlo en un entorno un poco más real para probar mi punto, es por eso que crearé un servidor público.\nCreando un servidor público Importante Si bien en esta sección utilizo DigitalOcean, pueden utilizar cualquier servidor público que probablemente tengan u otros cloud providers e.g. AWS, Vultr, Linode, etc. La idea es tener un servidor que pueda ser accedido desde internet.\nPara esto voy a utilizar el cloud provider DigitalOcean que además de vender servidor virtuales públicos y otros servicios, este tiene una política de cobrar por hora, es decir que una máquina que nos costaría 5 USD al mes si la tenemos corriendo todo el tiempo, pero si la utilizamos solo un par de horas, nos costará aproximadamente entre 0.007 a 0.014 centavos de USD. Pueden ver los precios en esta página.\nEl proceso de creación de un servidor en DigitalOcean es bastante simple, solo un par de clicks y saber elegir la distribución Linux y la cantidad de recursos a asignar. Yo elegiré Debian 9 x64 con 1GB de memoria y me costará 0.007 USD la hora.\nLes recomiendo que al momento de crear el droplet, lo asocien con una llave SSH.\nEste link muestra como crear un droplet en DigitalOcean.\nIniciando sesión por SSH Por defecto DigitalOcean permite por defecto acceder a los servidores con usuario root lo cual es considerado una mala práctica en términos de seguridad, para un servidor real les recomiendo deshabilitar el login para el usuario root.\nPara poder iniciar sesión, la forma de hacerlo es en el siguiente formato:\n$ ssh usuario@servidor Para el caso de DigitalOcean sería:\n$ ssh root@ip_droplet Una vez conectados al servidor remoto podemos hacer distintas cosas: ejecutar comandos, configurarlo, instalar/desinstalar paquetes, etc. Ya con esto podemos comenzar a jugar un poco con algunas de las cosas divertidas que podemos hacer con SSH.\nLocal Port Forwarding Explicaré el concepto mediante un ejemplo: imaginemos que se tiene un servidor público y detrás de este existe una red privada donde pueden haber servidores de base de datos, servicios disponibles solo en la red privada, etc. Entonces al estar estos en una red privada no hay una forma directa de acceder a ellos desde internet. Algunas opciones para poder acceder a servicios detrás de una red privada es utilizando una VPN o utilizar Local Port Forwading. Local Port Forwarding nos permite crear un túnel entre un servicio privado con nuestra máquina a través de un servidor público.\nVeamos la siguiente configuración:\nEn la imagen vemos que existe un servidor de base de datos MySQL en una red privada con la dirección IP 10.100.1.23 y puerto 3306 y existe un servidor público con la dirección IP 152.190.23.56.\nSi desearamos acceder a este servidor de base de datos tendríamos que estar dentro de la red privada lo cual no es cierto ya que nosotros estamos en la red local (privada) de nuestra casa, universidad, trabajo, etc. Por suerte el servidor 152.190.23.56 tiene acceso a esta red privada además de poder acceder a este desde internet.\nLo que haremos es utilizar la característica de Local Port Forwarding de OpenSSH para crear un canal seguro por medio del servidor público 152.190.23.56 entre el puerto 3306 de mi máquina local (PC o laptop) hacia el puerto 3306 del servidor privado de base de datos 10.100.1.23.\nEl formato de ejecución es el siguiente:\nssh -nNT -L puertoA:host_privado:puertoB usuario@servidor Donde:\n-n evita leer desde STDIN o leer la escritura por línea de comandos -N no ejecutar un comando remoto -T deshabilitar la opción de mostrar una terminal -L indicador de Local Port Forwarding puertoA puerto de nuestra computadora donde será expuesto el servicio remoto (MySQL) host_privado la dirección IP del host al cual no podemos llegar públicamente pero si a través del servidor público puertoB puerto TCP del servicio que el host_privado está exponiendo En nuestro ejemplo sería algo similar a:\nssh -nNT -L 3306:10.100.1.23:3306 root@ip_droplet Es importante saber que si tuvieramos instalado MySQL en nuestra computadora local habría un conflicto por el puerto 3306 entonces como este puede ser cualquier puerto podemos utilizar algo como:\nssh -nNT -L 3307:10.100.1.23:3306 root@ip_droplet Ahora imaginemos el caso que en la red local de casa, universidad o trabajo existe gente que desea acceder al servidor de base de datos. La respuesta simple es que ellos podrían aplicar el mismo procedimiento y tener acceso a MySQL desde sus máquinas locales.\nEn el supuesto caso que existiera la restricción de que ellos no tengan y no deban tener acceso al servidor público, una solución es que yo exponga el servicio de MySQL en la red local utilizando Local Port Forwarding y otras máquinas en mi red local puedan conectarse a mi computadora como si yo estuviera exponiendo un servicio MySQL pero en realidad estoy exponiendo el servicio MySQL que está corriendo en una red privada en algún lugar del mundo.\nEl diagrama muestra lo que se desea conseguir:\nPara esto solo adicionamos un pequeño cambio a la ejecución previa y debemos ejecutar en el siguiente formato:\nssh -nNT -L ip_local:puertoA:host_privado:puertoB usuario@servidor Donde\nip_local es la dirección IP de nuestra máquina en la red local privada, de no estar seguros cual es esta simplemente podemos poner el valor 0.0.0.0 para exponer por cualquier interfaz de red. ssh -nNT -L 192.168.1.100:3306:10.100.1.23:3306 root@ip_droplet De este modo las demás máquinas en nuestra red local podrán conectarse a 192.168.1.100:3306 y así acceder al servicio de MySQL.\nReproduciendo el ejemplo en nuestro servidor público Una ventaja que MySQL tiene por defecto es el de no exponer su puerto 3306 a ninguna red por seguridad, es decir, solo está disponible localmente. Entonces si instalamos MySQL en nuestro servidor público el puerto 3306 no será expuesto al público y este lo podríamos considerar como si estuviera en una \u0026ldquo;red privada\u0026rdquo; y poder aplicar lo aprendido.\nEn nuestro servidor instalamos MySQL con el siguiente comando:\n$ sudo apt install mysql-server Ahora lo que haremos es percatarnos que MySQL no expone el puert 3306 al público.\nPara ello existen diferentes alternativas:\nUtilizando netstat internamente** Verificaremos que existe un socket abierto en el puerto 3306 pero utilizando la dirección 127.0.0.1, es decir, solo localmente.\n$ netstat -tlpn Veremos una salida similar a:\nActive Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 127.0.0.1:3306 0.0.0.0:* LISTEN 2722/mysqld tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 784/sshd tcp6 0 0 :::22 :::* LISTEN 784/sshd Y efectivamente nos percatamos que MySQL esta expuesto solo internamente al ver esto en el resultado anterior: 127.0.0.1:3306\nUtilizando nmap externamente** Para poder ver desde cualquier otro lado si el puerto 3306 esta expuesto al público utilizamos la herramienta nmap:\n$ nmap -Pn ip_servidor -p 3306 Esperamos una respuesta similar a la siguiente:\nStarting Nmap 7.40 ( https://nmap.org ) at 2018-08-18 23:24 -04 Nmap scan report for 142.93.204.171 Host is up (0.12s latency). PORT STATE SERVICE 3306/tcp closed mysql 3306/tcp closed mysql indica que no podemos acceder desde afuera al puerto 3306 en nuestro servidor.\nAplicando Local Port Forwarding Ahora nos preguntamos ¿Cómo acceder desde afuera a MySQL?. Existen varias respuestas para esa pregunta pero en este post utilizaremos SSH con Local Port Forwarding para mapear un puerto local (nuestra PC o laptop) con un puerto que es accesible internamente dentro del servidor.\nssh -nNT -L 3306:localhost:3306 root@ip_servidor De este modo si verificamos en nuestra PC o laptop, el puerto 3306 debería estar abierto y apuntando a través de un túnel SSH al puerto 3306 disponible solo internamente.\nPara verificar, ejecutamos lo siguiente en nuestro dispositivo local:\n$ netstat -tlpn Esperando una salida similar a:\nProto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 127.0.0.1:3306 0.0.0.0:* LISTEN 23193/ssh tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN - Donde 127.0.0.1:3306 nos indica que efectivamente 3306 localmente está abierto y se conecta a través de un canal seguro a MySQL instalado en el servidor que solo podía ser accedido internamente.\nAlgo más para completar esta sección es permitir a máquinas en nuestra red local acceder al puerto 3306 que a su vez esta mapeado por un canal seguro con MySQL instalado en el servidor. Para ello solo adicionamos un parámetro extra a nuestro túnel SSH.\n$ ssh -nNT -L 192.168.1.100:3306:localhost:3306 root@ip_servidor Y al ejecutar netstat localmente veremos algo como:\nProto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 192.1681.1.100:3306 0.0.0.0:* LISTEN 23193/ssh tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN - Comentarios Finales En esta primera parte vimos como exponer en nuestra máquina local o red local un servicio que se encuentra disponible en una red privada en un datacenter en otro lado del mundo utilizando Local Port Forwarding.\nPersonalmente yo utilizo esta técnica bastante para acceder a base de datos o cualquier otro servicio interno en una nube privada en caso de no existir una VPN, lo cual facilita bastante el trabajo y nos evita tener que exponer puertos de servicios que no deben ser públicos.\nEn la segunda parte de esta serie de entradas, veremos como utilizar SSH para publicar servicios en nuestra máquina local a internet mediante un servidor público.\n","permalink":"https://blog.donkeysharp.xyz/es/post/diversion-con-ssh-parte-1/","summary":"\u003cp\u003eEl otro día cenando con mi amigo Francisco, él me comentaba que tiene un proyecto personal en mente y lo que desea conseguir. Escuchando sus preguntas la primera herramienta que pasó por mi cabeza para resolver algunos de sus problemas fue utilizar OpenSSH.\u003c/p\u003e\n\u003cp\u003eEsta entrada la hago principalmente para mi amigo Francisco pero la redactaré de forma general para que el benficio sea general.\u003c/p\u003e\n\u003ch2 id=\"qué-es-ssh\"\u003eQué es SSH?\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://es.wikipedia.org/wiki/Secure_Shell\"\u003eSSH\u003c/a\u003e significa Secure Shell y es un protocolo para administrar servicios en una red mediante un canal cifrado (he ahí el porqué de \u0026ldquo;Secure\u0026rdquo; \u0026#x1f609;). Algunas de las tareas más comunes que se pueden realizar con este protocolo es la de poder iniciar sesión en un servidor o la ejecución remota de comandos.\u003c/p\u003e","title":"Diversión con SSH - Local Port Forwarding"},{"content":"Holas, durante mi tiempo libre estuve trabajando en un proyecto llamado Gocho. Esta pequeña aplicación permite compartir archivos en una red local, por ejemplo la red de la casa, red del trabajo, etc. con la característica de auto-descubrimiento de nodos (auto-discovery). En este post explicaré de que se trata, por qué lo hice y algunos de los retos que surgieron al hacer la aplicación.\nPor qué lo hice? Lo hice porque deseaba una aplicación donde pueda compartir un directorio; poder aguantar varias descargas al mismo tiempo; no tener que preguntar la dirección IP o puerto de los recursos que otros comparten; tener algo que sea simple de ejecutar en cualquier sistema operativo (Windows, OSX, GNU/Linux) sin tener que instalar ningún requisito previo.\nEste tipo de proyectos siempre los hago a modo de aprender algo nuevo. En este caso ya desde el año pasado (2017) tenía ganas de hacer algo con Go. Si bien hice uno que otro experimento a modo de aprender (Golondrina; EazyPanel), esta aplicación me pareció un buen caso de uso para este lenguaje en específico (lo elaboro más adelante).\nPor qué Go? Si alguien ya ejecutó la aplicación es fácil notar que no es algo de otro mundo. Gocho podría haber sido desarrollado en otros lenguajes como: C/C++, Python, Ruby, Java, CSharp, etc. pero tenía algunas observaciones preliminares:\nPython/Ruby - es importante tener los intérpretes instalados, en Windows no vienen por defecto. Java/CSharp - es importante tener las máquinas virtuales respectivas instaladas (JVM o NetCore). No siempre se dá el caso que ya venga instalado por defecto en el sistema operativo. C/C++ - sería la opción más obvia, pero un par de problemas que encontré sería que por defecto son dinámicamente enlazados, lo cual causaría en algún caso la instalación de librerias necesarias (a menos que use el flag de estáticamente enlazados) y el segundo problema es que a pesar que se leer C/C++ no me siento en la confianza de lanzarme a hacer algo Go - estáticamente compilado por defecto (todo en un binario), el binario resultante no require que exista la instalación previa de alguna librería, intérprete, máquina virtual y otros. Con esta lista —un poco parcializada \u0026#x1f609;— Go cumple con las necesidades que tengo.\nAlgo no mencionado es la facilidad con la que puedo crear binarios para diferentes plataformas. Por ejemplo Gocho esta disponible para distintas plataformas sin mucho problema. Releases Gocho\nAlgunos Problemas que Encontré Problema 1: Compartir Archivos En la empresa donde trabajo existe una diversidad en cuestión a sistemas operativos. Algun@s compañer@s utilizan Windows, OSX y otr@s GNU/Linux. Para compartir archivo, existe un Active Directory o algo similar configurado, personalmente nunca logré acceder a las carpetas compartidas por otros (uso GNU/Linux). Al intentar acceder me salía la opción de insertar un dominio y credenciales; pése a que introducía los datos —que en teoría eran correctos— no lograba acceder a los archivos compartidos.\nEn el trabajo hay compañeros que comparten información por ejemplo videos, cursos, etc. montando un servidor httpd en su máquina local o en mi caso ejecutaba python -m SimpleHTTPServer en el directorio que deseaba compartir. Noté un problema con SimpleHTTPServer, con pocas personas tratando de descargar el mismo archivo, esta pequeña utilidad solo permite manejar una descarga al mismo tiempo.\nMi segundo intento fue utilizar algo un poco más robusto que SimpleHTTPServer pero sin la necesidad de levantar algo grande como httpd. Tuve la suerte de chocar con un ejemplo en la documentación de Go para el modulo net/http que justamente —con pocas líneas de código— me permitía compartir un directorio y podía soportar varias descargas simultáneas sin problemas.\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { // Simple static webserver: directory := \u0026#34;some/directory\u0026#34; log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, http.FileServer(http.Dir(\u0026#34;/home/myuser/some/directory\u0026#34;)))) } Solo tuve que compilar este archivo; poner el binario que llamé http en algún directorio que se encuentre en mi PATH de ejecución y voila! ya contaba con algo más robusto que pueda aguantar más descargas y no tenga que montar todo un servicio.\nya contaba con algo más robusto que pueda aguantar más descargas\nCon esto me refiero a algo que aguante varias conexiones simultáneas, algo que Go lo plantea de una manera simple y en el caso del módulo net/http ya viene por defecto.\nHasta este punto, solamente tengo un pequeño binario que me permite compartir un directorio y pueda aguantar varias descargas simultáneas —algo no tan complejo como montar un servicio httpd pero más robusto que SimpleHTTPServer de Python.\nLa verdad si compilara este binario http para varios sistemas operativos sería suficiente para compartir archivos, pero quería ir un poco más alla.\nProblema 2: Indicar Donde se Encuentran los Archivos Compartidos Otro problema que encontré es que cada vez que un usuario comparte algo en una red local, este debe —de algún modo— comunicar como acceder a los recursos que se comparten. Una forma común de realizar esto es compartir la url de descarga http://ip_red_local:algun_puerto en algun grupo de chat o similar.\nYa que esta aplicación la tengo orientada para el contexto de una red local, algo que se me pasó por la cabeza son los juegos en red como StarCraft. En StarCraft cuando alguien crea una partida de red de área local, los jugadores que se unirán a una partida no especifican como tal la dirección IP de la máquina servidor a la que se conectarán. El juego simplemente muestra las partidas creadas en la red actualmente y uno puede conectarse sin problemas de forma automática.\nInvestigando un poco sobre cómo estos juegos hacian posible mostrar las partidas ya existentes en la red sin tener que especificar una dirección IP o algo similar, me llevó al concepto de multicast.\nEn palabras simples, Multicast es un método que permite enviar información a nodos interesados en una red.\nPor ejemplo, si deseo enviar el mensaje \u0026ldquo;hola mundo\u0026rdquo; a computadoras interesadas en recibir este mensaje sin que yo tenga que saber a qué máquinas específicamente, la idea sería la siguiente:\nMi Computadora: Enviar datagrama UDP con mensaje \u0026ldquo;hola mundo\u0026rdquo; a alguna dirección IP reservada para multicast ej. 239.6.6.6:1234 Computadora Interesada 1: Escuchar por datagramas UDP en 239.6.6.6:1234 Computadora Interesada 2: Escuchar por datagramas UDP en 239.6.6.6:1234 Computadora Interesada n: Escuchar por datagramas UDP en 239.6.6.6:1234 De este modo multicast permite que cualquier máquina que desee compartir algo, solo debe enviar su información de nodo (identificador, dirección IP, puerto) por multicast y otras máquinas interesadas.\nSabiendo esto, Gocho además de compartir un directorio podrá saber lo que otros nodos Gocho estan compartiendo.\nYa viendo un poco la implementación de esto, podemos ver algunos trozos de código que utilicé en Gocho.\npkg/node/net.go\nLa función announceNode básicamente envía un paquete multicast.\nfunc announceNode(nodeInfo *NodeInfo) { address, err := net.ResolveUDPAddr(\u0026#34;udp\u0026#34;, MULTICAST_ADDRESS) // error handling conn, err := net.DialUDP(\u0026#34;udp\u0026#34;, nil, address) // error handling for { ... conn.Write([]byte(message)) time.Sleep(ANNOUNCE_INTERVAL_SEC * time.Second) } } La función listenForNodes que escucha los mensajes multicast.\nfunc listenForNodes(nodeList *list.List) { address, err := net.ResolveUDPAddr(\u0026#34;udp\u0026#34;, MULTICAST_ADDRESS) // error handling conn, err := net.ListenMulticastUDP(\u0026#34;udp\u0026#34;, nil, address) // error handling conn.SetReadBuffer(MULTICAST_BUFFER_SIZE) for { packet := make([]byte, MULTICAST_BUFFER_SIZE) size, udpAddr, err := conn.ReadFromUDP(packet) ... } } Entonces gran parte de la \u0026ldquo;mágia\u0026rdquo; de Gocho se encuentra en el trabajo con multicast. Con multicast una máquina puede anunciarse a sí misma y a la vez descubrir a otros nodos.\nProblema 3: Formato de los Mensajes Si bien ya tenemos una forma de comunicarnos entre nodos, ví conveniente poder identificar los paquetes que envía Gocho con otros. Básicamente el paquete (en esta versión inicial) debe seguir lo siguiente:\nLos primeros 4 bytes deben ser 0x60, 0x0d, 0xf0, 0x0d o 0x600df00d, que es la cabecera que identifica que lo enviado es un mensaje de otro nodo de Gocho El siguiente byte especifica el comando, actualmente solo existe un solo comando que es 0x01 que indica que un nodo se esta anunciando. La información del nodo se encuentra en el payload Finalmente el resto es el payload. Para esto decidí utilizar el formato JSON Un hexdump de un mensaje en el que se anuncia un nodo luce de la siguiente forma:\n00000000 60 0d f0 0d 01 7b 22 6e 6f 64 65 49 64 22 3a 22 |`....{\u0026#34;nodeId\u0026#34;:\u0026#34;| 00000010 6e 6f 64 6f 2d 73 65 72 67 69 6f 22 2c 22 69 70 |nodo-sergio\u0026#34;,\u0026#34;ip| 00000020 41 64 64 72 65 73 73 22 3a 22 22 2c 22 77 65 62 |Address\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;web| 00000030 50 6f 72 74 22 3a 22 35 30 30 30 22 7d |Port\u0026#34;:\u0026#34;5000\u0026#34;}| La decisión de tener este formato fue la de ahorrar la mayor cantidad de bytes posibles. De hecho, si no utilizara el formato JSON se ahorrarían unos cuantos bytes más.\nEn el futuro es posible que existan más comandos diferentes al de anunciar un nodo (0x01). Es por eso que se dejó un byte reservado para ello.\nDiseño de la aplicación Esta sección habla un poco más de la implementación ya habiendo conocido los problemas mencionados arriba. Para poder seguir esta sección hago referencia al código fuente de la aplicación.\nEstructura del Código La estructura de código del proyecto Gocho está basada en este artículo en el que se plantea una esctructura para proyectos en Go. Esta estructura es bastante utilizada en diferentes proyectos, entre ellos algunos proyectos bastante conocidos como Kubernetes o Docker.\nEl proyecto incluye un archivo Makefile el cual indica los pasos necesarios para poder construir o desarrollar el proyecto, como también crear los binarios para las plataformas soportadas.\nComponentes del Servicio En el anterior punto mencioné la estructura de código que utilizo. En este punto me enforcaré en los componentes dentro del directorio pkg, especialmente en pkg/node.\nComponente Descripción pkg/info Información básica de la aplicación como nombre o versión. pkg/cmds Toda la lógica de flags de la utilidad de línea de comandos. Por ejemplo gocho start [options] o gocho configure. pkg/config Todo el código con la lógica y estructuras necesarias para representar la lógica de Gocho. Aca se encuentra la lógica de establecer valores por defecto o cargar las diferentes configuraciones de un archivo .gocho.conf o de los options establecidos por línea de comandos. pkg/node La lógica principal de la aplicación radica en este directorio. El cómo se tiene un dashboard web embebido; el formato de los paquetes; el mecanismo de auto-discovery (multicast) y el índice de archivos que muestra el contenido del directorio compartido. Algunas Estructuras de Datos y Lógica Utilizada La aplicación debe guardar la información de otros nodos, para esto decidí hacer uso de una lista enlazada por la simplicidad al borrar o insertar elementos.\nEs importante notar que mientras más nodos se anuncien en una red, es posible que partes del código (de cualquier nodo) tendrá que ejecutar las mismas sentencias al mismo tiempo. Para evitar problemas de concurrencia: principalmente en la lista enlazada que guarda información de otros nodos, hice uso de un Mutex con lo cual pude controlar estos comportamientos que podrían llevar a resultados inesperados.\nAlgo importante es notar que existen algunos timeouts por defecto que constantemente verifican la lista enlazada de nodos. Básicamente estos timeouts nos permite liberar recursos, cuando un nodo después de cierto tiempo deja de anunciarse.\nEl Dashboard Para el desarrollo del dashboard de frontent hice uso de React solamente. Tal vez algunos se preguntan ¿por qué no utilicé otras librerias como Redux o React-Router?. Pues la respuesta es simple, como el bundle javascript resultante se encuentra embebido en el binario, si tenemos un bundle más grande y pesado, el binario resultante será más grande y pesado.\nLos componentes y el código para la UI se encuentra en el directorio ui. Para la estructura quise mantener las cosas cosas simple así que utilicé Create React App para este propósito.\nDe la misma forma para los estilos, si bien podría haber utilizado un procesador como SASS, decidí mantener los estilos y solo utilizar el estándar por defecto CSS, que como verán en el código es un archivo de 184 líneas de código.\nPara generar el bundle javascript solo es necesario ejecutar el siguiente comando.\nmake dist Internamente esto utiliza los scripts de Create React App y para poder tener el bundle embebido dentro del binario, hago uso de Go Generate.\nEste archivo tiene un comentario:\npackage main //go:generate go-bindata -o ../../assets/assets_gen.go -pkg assets ../../ui/build/... import ( \u0026#34;github.com/donkeysharp/gocho/pkg/cmds\u0026#34; \u0026#34;os\u0026#34; ) ... donde se especifica donde se encuentra el bundle resultante que será embebido en el binario resultante.\nEl Índice de Archivos Compartidos Esta fue una de las partes en las que tuve mucha diversión. Como mencioné en el Problema 1, Go presenta un ejemplo para poder compartir un directorio. El problema con esto es que no cuenta con estilos, extensibilidad, el directorio .. para poder subir un nivel el directorio.\nPara poder personalizar este código ya existente tuve que utilizar Interceptor Pattern y un middleware personalizado para poder adicionar íconos y HTML personalizados, el directorio .. para ir un nivel hacia arriba y agrupar directorios de archivos de forma ordenada.\nToda la lógica para la personalización de net/http.FileServer se encuentra en el archivo index.go.\nEspero poder crear otra entrada en la que muestro específicamente la implementación de esta parte que puede ser reutilizada con cualquier otra aplicación.\nComentarios Finales Hay varias cosas que deseo mejorar de Gocho. Al ser un proyecto open-source sientanse libres de abrir un issue o mejor contribuir con algo de código (bugfixing, nuevos features, documentación, etc.)\nEsta es la primera vez que trabajo en una aplicación, la cual envia y recibe información de varias máquinas o nodos al mismo tiempo y hayan diferentes cosas que sincronizar. Personalmente fue —y espero siga siendo— una experiencia educativa.\n","permalink":"https://blog.donkeysharp.xyz/es/post/gocho-compartir-archivos-red-local/","summary":"\u003cp\u003eHolas, durante mi tiempo libre estuve trabajando en un proyecto llamado \u003ca href=\"https://github.com/donkeysharp/gocho\"\u003eGocho\u003c/a\u003e. Esta pequeña aplicación permite compartir archivos en una red local, por ejemplo la red de la casa, red del trabajo, etc. con la característica de auto-descubrimiento de nodos (auto-discovery). En este post explicaré de que se trata, por qué lo hice y algunos de los retos que surgieron al hacer la aplicación.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/gocho-dashboard.gif\"\u003e\u003c/p\u003e\n\u003ch2 id=\"por-qué-lo-hice\"\u003ePor qué lo hice?\u003c/h2\u003e\n\u003cp\u003eLo hice porque deseaba una aplicación donde pueda compartir un directorio; poder aguantar varias descargas al mismo tiempo; no tener que preguntar la dirección IP o puerto de los recursos que otros comparten; tener algo que sea simple de ejecutar en cualquier sistema operativo (Windows, OSX, GNU/Linux) sin tener que instalar ningún requisito previo.\u003c/p\u003e","title":"Gocho, Compartir Archivos En Una Red Local"},{"content":" Update\n2018-04-28 Added more packages I use and new settings I do (reinstalled machine)\nEn esta entrada explico las aplicaciones y configuraciones que uso para mi máquina de desarrollo local.\nSistema Operativo: GNU/Linux - Debian Stretch\nDektop Manager: xfce4\nAplicaciones usadas frecuentemente Estas son las aplicaciones que uso con más frecuencia y siempre trato de tenerlas instaladas, algunas son de proposito general y otras relacionadas con programación, experimentos, trabajo y proyectos personales.\nGnome file roller para comprimir archivos file-roller Font Viewer para la instalación de fuentes gnome-font-viewer Google Chrome Terminal music player mocp Vim editor, para edición por línea de comandos y servidores vim Build essentials build-essential SublimeText3 editor de texto que uso para casi todo. Emacs editor de texto que uso para algunas cosas emacs Redshift para cambiar la temperatura del monitor redshift Kazam para grabar el escritorio kazam Kupfer similar a Spotlight para búsquedas rápidas kupfer Shutter para screenshots shutter Editor de gráficos vectoriales inkscape Gnome Hex Editor ghex Meld para comparar diferencias entre dos archivos meld Armagetron, juego basado en Tron armagetronad DOSBox emulador de juegos antiguos de DOS dosbox Drivers faltantes firmware-linux-free firmware-linux-nonfree Ristretto Image viewer Ettercap para investigación de ataques MITM, pensé en usar Bettercap pero no soy un fanático de Ruby Transmission cliente bit torrent transmission Wireshark para interceptar paquetes de red wireshark Slack messaging, personalmente uso la versión web gran parte del tiempo, pero cuando necesito compartir mi escritorio es cuando uso la versión de escritorio (descargar de su sitio) Pavu Controller para configuraciones de audio pavucontrol VLC Media Player vlc Evince como visor de pdf evince xCHM visor de archivos .chm xchm Utildad para manejar discos gparted Información del hardware hardinfo Docker Community Edition NodeJs Esta es otra lista de aplicaciones que utilizo con más frecuencia específicamente en línea de comandos:\nTerminal multiplexer tmux htop visor de procesos basado en ncurses htop Tracer de llamadas al sistema strace Cliente HTTP curl Utilidades de DNS dnsutils Instalar sudo Compresores zip sudo Obviamente hay otros paquetes para cosas bastante específicas los cuales los instalando cuando es necesario.\nEscritorio Uso xfce4 con dos panels ambos en la parte superior. El primero contiene el menú de aplicación con el logo de Debian; un separador transparente que se extiende; las ventanas abiertas; otro separador transparente que se extiende. El segundo panel tiene estos items: las áreas de trabajo con dos files y dos áreas en cada fila; el visor de uso de CPU; el área de notificaciones; plugin de PulseAudio para cambiar el volumen; finalmente el plugin de fecha mostrando la fecha arriba y la hora abajo.\nEl primer panel tiene un fondo negro mientras que el otro usa el estilo por defecto del sistema para evitar colisión en colores de fuente.\nApariencia Para mis configuraciones de apariencia utilizo lo siguiente:\nIconos Numix Light que vienen en el paquete numix-icon-theme Tema de ventanas Adwaita Fuente por defecto: Sans (10) con antialias en Slight y DPI en 101 Tweaks Extra El \u0026ldquo;Switcher de Ventanas\u0026rdquo; (alt + tab) de xfce4 que viene en Debian Stretch no me agrada por el tamaño gigante de preview de ventana. Personalmente prefiero tener solo los íconos pequeños sin nombre de la ventana, para ello son solo un par de cambios a realizar: ir a Settings \u0026gt; Window Manager Tweaks y en el tab Cycling deseleccionar Cycle through windows in a list y finalmente en el tab Compositor deseleccionar Show windows preview in place of icons when cycling.\nHotkeys Tengo algunos hotkeys que uso en xfce4 para tareas comunes. Para configurar los hotkeys de xfce4 voy a Settings \u0026gt; Keyboard \u0026gt; Application Shortcuts tab. Mis hotkeys son los siguientes:\nHotkey Comando Descripción win_key + f thunar Abrir manejador de archivos Thunar win_key + t /usr/bin/xfce4-terminal Abrir una nueva terminal win_key + n mocp --next Ir a la siguiente canción en MOC player win_key + b mocp --previous Ir a la anterior canción en MOC player win_key + o mocp --pause Pausar la canción actual en MOC player win_key + p mocp --unpause Reanudar reproducción en MOC player Otras hotkeys: Este no es un hotkey de xfce4 pero lo uso con mucha frecuencia ctrl + shift + space para lanzar kupfer.\nTerminal Tema Para la terminal uso la que viene por defecto xfce4-terminal con estas configuraciones en ~/.config/xfce4/terminal/terminalrc.\nPrompt Por defecto la terminal bash tiene un prompt simple como usuario@host:directorio-actual. En mi caso que uso bastantes repositorios git, este prompt por defecto no es suficiente ya que quiero ver en el prompt si hay cambios, conflictos, etc. Podria correr git status pero con un prompt personalizado podría ahorrarme ese paso \u0026#x1f609;. Este es el script que llamo desde mi archivo .bashrc, que basicamente muestra hora, directorio actual y la información del repositorio en caso de estar usando uno. Gracias a Mike Stewart que es el autor original de este script.\nIntente utilizar zsh y sus frameworks pero no me sentia comodo y me fue difícil acostumbrarme a este, así que la forma más simple fue tener un prompt personalizado. Afortunadamente habian muchos recursos disponibles para ello, así que no fue un dolor.\nConfiguración Configuración de Tmux Utilizo tmux desde Debian Wheezy pero cuando cambié a Debian Jessie tuve algunos problemas con el directorio al crear nuevos panels. Este es el .tmux.conf que uso.\nConfiguración de MOC Player Como esta es una herramienta de línea de comandos pienso que entra en esta sección. Hay dos archivos que tengo .moc/config y el tema que uso, ambos pueden ser encontrados en mi gist.\nComentarios Finales Si bien estas configuraciones son bastante personalizadas para mi caso, hice esta entrada con el propósito de tener algo que leer por si olvido y lo compartí en caso le sea de utilidad a algún lector.\n","permalink":"https://blog.donkeysharp.xyz/es/post/configuracion-de-escritorio-debian/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eUpdate\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e2018-04-28\u003c/em\u003e Added more packages I use and new settings I do (reinstalled machine)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eEn esta entrada explico las aplicaciones y configuraciones que uso para mi máquina de desarrollo local.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eSistema Operativo:\u003c/em\u003e GNU/Linux - Debian Stretch\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eDektop Manager:\u003c/em\u003e xfce4\u003c/p\u003e\n\u003ch2 id=\"aplicaciones-usadas-frecuentemente\"\u003eAplicaciones usadas frecuentemente\u003c/h2\u003e\n\u003cp\u003eEstas son las aplicaciones que uso con más frecuencia y siempre trato de tenerlas instaladas, algunas son de proposito general y otras relacionadas con programación, experimentos, trabajo y proyectos personales.\u003c/p\u003e","title":"Mis Configuraciones De Escritorio en Debian"},{"content":"Hola, soy Sergio Guillen Mantilla\u0026hellip; en forma de github pages \u0026#x1f604;\nEste es un proyecto el cual retomaré distinto del contenido de mi antiguo blog que en su mayoría era orientado a tecnologías Microsoft. Desde que dejé de usar estas tecnologías —que aún me impresionan un montón como C#— tuve la suerte de conocer otras tecnologías de las cuales deseo compartir algunas experiencias que tuve o una que otra cosa que voy investigando.\nPara la creación de este blog uso Hugo un generador de páginas estáticas.\nSi desean pueden seguirme en Twitter.\n:() { :|:\u0026amp; }; : ","permalink":"https://blog.donkeysharp.xyz/es/post/mi-primer-post/","summary":"\u003cp\u003eHola, soy Sergio Guillen Mantilla\u0026hellip; en forma de github pages \u0026#x1f604;\u003c/p\u003e\n\u003cp\u003eEste es un proyecto el cual retomaré distinto del contenido de mi \u003ca href=\"http://donkeysharp.blogspot.com/\"\u003eantiguo blog\u003c/a\u003e que en su mayoría era orientado a tecnologías Microsoft. Desde que dejé de usar estas tecnologías —que aún me impresionan un montón como C#— tuve la suerte de conocer otras tecnologías de las cuales deseo compartir algunas experiencias que tuve o una que otra cosa que voy investigando.\u003c/p\u003e","title":"Primer Post - echo \"Hello World\""},{"content":"¡Hola! Me llamo Sergio Guillén. Me apasiona el Desarrollo de Software, GNU/Linux, FOSS, Tecnologías Cloud y Cultura Hacker. Tengo mucha curiosidad y me encanta entender cómo funcionan las cosas, desde librerías, frameworks y herramientas que uso a diario hasta cosas más especializadas, como sistemas operativos, protocolos de red, ejecutables binarios, hardware, etc.\nEn los últimos años pasé del desarrollo de software a infraestructura en la nube y adoptar la cultura DevOps. Siempre estoy emocionado por mejorar los procesos de desarrollo y despligue para que sea más sencillo lanzar nuevas versiones de un software en períodos más cortos y mejorar la forma en que se gestiona la infraestructura en la nube.\nEn mi tiempo libre colaboro con diferentes comunidades tecnológicas, lo que me da la oportunidad de aprender más de los demás y compartir lo que sé.\n","permalink":"https://blog.donkeysharp.xyz/es/about/","summary":"Sample article showcasing basic code syntax and formatting for HTML elements.","title":"Sobre mí"}]