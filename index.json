[{"content":"Hey everyone! In this post I\u0026rsquo;m gonna walk you through the process of installing Official Linux Nvidia Drivers for the RTX 50 series graphic cards on Debian 13 (Trixie). Some issues I found and other scenarios to consider such as OS upgrades.\nSomething I love when using GNU/Linux is that there are multiple ways to solve a problem. And sometimes it will depend on the hardware you are using, architecture, desktop environment, etc, etc.\nThere are two options, the recommended way (using Nvidia repositories) and the other alternative which is installing the .run file. I will show you both.\nLet\u0026rsquo;s get our hands dirty!\nMy love relationship with Debian My best friend introduced me to Debian in 2009, it was Debian 5 (Lenny), and I have been using it since then, initially as dual-boot when Windows XP was still a thing and in 2013 I started using Debian 100% for all the things I did, including work. I tested other distributions, and personally I liked Debian because it\u0026rsquo;s boring and it simply works, it does what it has to do for me. I mostly don\u0026rsquo;t need the latest version of all the software, and after I started using Docker back in 2015, having the latest version of most software was not an issue whatsoever for me anymore. Of course for certain things that didn\u0026rsquo;t work out-of-the-box, I had to do a couple fixes, but 99% of the time, everything that I need works fine. I\u0026rsquo;ve always used the stable version, no testing nor sid.\nSomething important to mention is that I don\u0026rsquo;t use brand-new hardware nor GPU, I like to buy refurbished/used computers, usually ones launched 5 years ago such as Thinkpads, or other used desktop workstations. And I didn\u0026rsquo;t have issues with drivers, compatibility, etc. Everything worked like a charm for all the things I required.\nSince mid-2025, I started doing some research on AI and decided to do it on my local setup and not cloud (I was using AWS), so this time I decided to buy the parts for a new desktop computer, and one of the parts is a Nvidia RTX 5070 Ti, which was released almost a year ago.\nSurprise, surprise. Debian 13 didn\u0026rsquo;t work with RTX 5070 Ti out of the box! Debian does not officially support the latest drivers for this model.\nSo when you install Debian and on your first boot, you will see the black screen with the terminal prompt blinking. That means your Desktop environment did not load because you don\u0026rsquo;t have the proper drivers installed for your graphic card.\nIn that case you could ssh to your machine from another machine, or use the TTY (that black terminal), to run the following steps, it\u0026rsquo;s up to you!\nDriver installation Disable Nouveau By default, Debian will install the Nouveau kernel module, we need to ignore it.\nLet\u0026rsquo;s check if Nouveau is actually being used first:\nlspci -nnk | less # and search for VGA Controller 01:00.0 VGA compatible controller [0300]: NVIDIA Corporation GB203 [GeForce RTX 5070 Ti] [10de:2c05] (rev a1) Subsystem: ASUSTeK Computer Inc. Device [1043:89f4] Kernel modules: nouveau In this case the kernel module is being used, so we need to blacklist it. Let\u0026rsquo;s create the following file /etc/modprobe.d/blacklist-nouveau.conf with the following content:\nblacklist nouveau options nouveau modeset=0 Let\u0026rsquo;s recreate the initramfs which is required every time we want to enable/disable kernel modules and reboot the instance:\nsudo update-initramfs -u sudo reboot Again, no graphical user interface (it\u0026rsquo;s expected) so we log in via terminal and validate that Nouveau kernel driver is not used anymore.\nlsmod | grep nouveau # should not print anything Requirements First, let\u0026rsquo;s uninstall any nvidia related package, you should not have any in case you have a fresh installation though.\napt remove --purge \u0026#39;*nvidia*\u0026#39; We need to install some build utilities to build the official Nvidia drivers. For that we run:\napt update apt install linux-headers-$(uname -r) build-essential libglvnd-dev pkg-config dkms Method A: The recommended way, using Nvidia repositories This method is also mentioned in official Debian documentation.\nIt\u0026rsquo;s a very simple method, you add the APT repository, update, and install the drivers. However Nvidia official documentation (at the time of writing 2026-02-27) is not updated and some links they suggest are broken. So if you want to go with this method, I recommend you the following video instead:\nImportant Note: It is important to note that at the time that video was recorded, Nvidia had an APT repository that only supported Debian 12, however it is compatible with Debian 13 (it is mentioned on the video).\nIf you use the method in the video today, you will see that adding the Nvidia repo will add a repo for Debian 13 now, however it only has the 590 version of the Driver which is still under development. So I recommend you using the Debian 12 repo for 580.x.x versions of the driver.\nMethod B: Using the .run installer For this method you need to download the .run installer from Nvidia\u0026rsquo;s website and search for the available drivers for the RTX 5070 Ti.\nIn my case, I chose the latest version and after downloading it, you must add execution permissions to the .run file.\nchmod +x NVIDIA-Linux-x86_64-580.126.18.run Now let\u0026rsquo;s execute it the following way.\nsudo ./NVIDIA-Linux-x86_64-580.126.18.run --dkms The --dkms flag means that the Nvidia driver will be re-built automatically when we upgrade our OS with a new version of the Linux kernel (see next section).\nMake sure to choose MIT/GPL, as for the latest models, we must use the official open source version of the drivers (thanks Lapsus$).\nFollow the next steps accepting DKMS, build initramfs, update X11 configuration. And that\u0026rsquo;s it. Once the installation finishes, reboot your computer and you will be able to use your desktop environment just as usual.\nIn order to make sure everything is working, just run the nvidia-smi program:\nMethod B: Upgrading Debian 13 When trying to upgrade to the latest Debian available, it\u0026rsquo;s more likely that there will be a new version of the Linux kernel to be installed. That means that the driver you previously installed will not work anymore as it was compiled for the previous version of the kernel.\nIf you installed the .run driver with DKMS, upgrading should work like a charm and it will rebuild the driver and install it during the upgrade.\nsudo apt update sudo apt upgrade -y You will see that during upgrade, the kernel module is being built for the new kernel version.\nUpgrading my OS worked like a charm! No difficult steps.\nFinal thoughts Personally, after testing both methods, I will stay with the not recommended method, just to see how it performs and experiment.\nI actually had some issues with xfce4, it was not related to the method of installation, but with some compositor settings that started breaking with versions following 580.105.08. I will write it down with more detail on my next post on how to fix that issue.\nAs mentioned, Debian didn\u0026rsquo;t cause me any issues on a daily basis, but it makes sense as I use old hardware most of the time. As a matter of fact, I am writing this post from a Thinkpad T450 (released more than a decade ago). But this time I have hardware that is not old, it made sense that I would need to do some extra steps.\nHappy hacking!\nReferences https://docs.nvidia.com/datacenter/tesla/driver-installation-guide/debian.html#debian-installation https://wiki.debian.org/GraphicsCard https://www.nvidia.com/en-us/drivers/unix/ https://wiki.debian.org/NvidiaGraphicsDrivers#Nvidia-packaged_data-center_drivers ","permalink":"https://blog.donkeysharp.xyz/installing-nvidia-drivers-rtx-50-debian13-trixie/","summary":"\u003cp\u003eHey everyone! In this post I\u0026rsquo;m gonna walk you through the process of installing Official Linux Nvidia Drivers for the RTX 50 series graphic cards on Debian 13 (Trixie). Some issues I found and other scenarios to consider such as OS upgrades.\u003c/p\u003e\n\u003cp\u003eSomething I love when using GNU/Linux is that there are multiple ways to solve a problem. And sometimes it will depend on the hardware you are using, architecture, desktop environment, etc, etc.\u003c/p\u003e","title":"Installing Nvidia Drivers for RTX 50 series on Debian 13 Trixie"},{"content":"\nHey everyone! In this post I\u0026rsquo;m going to share my experience of USB booting the Raspberry Pi 3 Model B from an SSD. Although it is something I did almost one year ago, I wanted to share it\u0026hellip; sorry for the 1 year delay xD!\nProbably there might be something I missed on my solution, so any feedback is really appreciated!\nBefore the pandemic started, a friend of mine sold me 4 Raspberry Pi 3 Model B and one Raspberry Pi 3 Model B+. I used them for an electronics + home automation project I no longer need anymore, so I decided to repurpose them to something new, a new project I started working on for which I will make another blog entry.\nThis time, as I was not going to use the RPIs for electronics, instead something that will require more disk usage, I wanted to try something I personally hadn\u0026rsquo;t tried before. I wanted to boot the RPIs from an SSD. Instead of spending money on new reliable, fast SD cards, I decided to use some spare SSDs I had instead. This is definitely something that was done before, but in my case there were some nuances that took me some days to realize how to fix them, and I wanted to share that experience with you.\nWhat I used For this experiment I used:\n4 RPI 3 Model B 1 RPI 3 Model B+ 5 Power supplies with the proper voltage and amps 5 Solid State Drives 5 USB to Sata adapters Booting from SSD There are multiple blog posts, forum entries and now LLMs that explain how to boot a RPI from an SSD (I added references I used in the references section below). In summary the steps required for RPI 3 Model B to USB boot from SSD are the next:\nUsing an SD card with RaspberryPI OS installed, run a full upgrade: sudo apt update -y \u0026amp;\u0026amp; sudo apt full-upgrade -y The following command will give two possible results depending on whether USB boot is enabled or not: vcgencmd otp_dump | grep 17 17:1020000a \u0026lt;\u0026lt;\u0026lt;\u0026lt; it means USB boot is disabled 17:3020000a \u0026lt;\u0026lt;\u0026lt;\u0026lt; it means USB boot is enabled In case it is disabled, the way to enable it is by adding the following lines to the /boot/firmware/config.txt. Some tutorials mention the /boot/config.txt but those are outdated. program_usb_boot_mode=1 program_usb_boot_timeout=1 Finally, clone the content of the SD card that has RaspberryPI OS to the SSD, you can do that with dd: # /dev/mmcblk0 is the SD card in my case # /dev/sdb is the SSD connected via USB dd if=/dev/mmcblk0 of=/dev/sdb status=progress Then remove the SD card from the RPI, connect the SSD via USB and reboot. Congrats! You booted your RPI from USB and an SSD. Now you can resize and stuff. That\u0026rsquo;s the result I wished I had, but no, it didn\u0026rsquo;t work for 4 of my 5 RaspberryPIs. Let me tell you how I fixed-ish it.\nThe difference between RPI3 model B and RPI3 model B+ I executed the previous steps first (by coincidence on the RPI 3 Model B+). And it worked like a charm. Then for the next RPIs 3 Model B (not B+) it didn\u0026rsquo;t work, I tried all of the 4 RPIs with the same result. I made sure I added the program_usb_boot_timeout=1 setting to the /boot/firmware/config.txt file to increase the wait for RPI to detect the disk, even with that verification it didn\u0026rsquo;t work. No USB boot for the RPIs 3 Model B.\nAt that point, I got to the conclusion that at least in terms of booting, the RPI 3 Model B and RPI 3 Model B+ have differences.\nMy Solution (a fluke) After trying multiple combinations that didn\u0026rsquo;t work, it booted! I checked in the terminal and indeed, it booted from SSD. I executed some IO speed tests to make sure it was indeed using the SSD. It finally worked!\nThe new problem was I didn\u0026rsquo;t add any new configuration change xD, so I was very confused on why it worked. And I noticed that this time I forgot to remove the SD card. Both SD and USB SSD were connected.\nMy hypothesis was that for RPI 3 Model B, an SD card is still required to boot, no matter if the operating system is installed on a USB SSD.\nTo finally verify that hypothesis, I formatted the SD card and just copied the content of the /boot/ directory from the SSD, actually they must be the same.\nVoila! It booted again!\nWith that test, I was sure that the RPI 3 Model B requires an SD card to boot no matter where the OS is located.\nI used the SD cards from my old project, formatted them and copied the content of /boot/firmware from each SSD and it worked fine.\nIt is important not to copy the content of the /boot/firmware directory from one SSD to all the SD cards, as each SSD has a different UUID specified in the /boot/firmware/cmdline.txt file. Copy from each SSD or make sure the /boot/firmware/cmdline.txt has the proper values for the disk UUID.\nOne example of the cmdline.txt\nconsole=serial0,115200 console=tty1 root=PARTUUID=e000a75d-02 rootfstype=ext4 fsck.repair=yes rootwait cfg80211.ieee80211_regdom=US cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory Upgrading the operating system All the steps that I mentioned before happened when the latest RaspberryPI OS was based on Debian 12. Now the latest version of the RaspberryPI OS was released based on Debian 13, I decided that instead of formatting and doing the same process from scratch, I will follow the steps to do a RaspberryPI OS major upgrade.\nThese are the steps I follow for this matter:\nEdit /etc/apt/sources.list and /etc/apt/sources.list.d/raspi.list and change from bookworm to trixie. Run apt update to refresh the package indexes Run apt install -y apt dpkg to install the latest version of the package manager. Run apt upgrade --without-new-pkgs to install latest version without installing new dependencies. Make sure everything is fine. Finally apt full-upgrade Reboot That would be enough on any setup (it works on the RPI 3 Model B+), but in this case as we boot from the SD card first and not the SSD, we will need to follow some extra steps.\nLet\u0026rsquo;s recall some Linux theory: The Linux kernel image is located at the /boot directory, but in our case the RPI boots from SD. We still have the boot content from the previous installation. After the upgrade when I rebooted, the kernel was still 6.1 and not 6.12 which is the one that comes with Debian 13.\nIn order to have the upgrade 100% ready, I had to repeat the previous steps I did to make the RPI boot from SSD.\nMount the SD card mount /dev/mmcblk0p1 /mnt/sdboot/, Copy /boot/firmware from SSD to the SD card: cp -r /boot/firmware/* /mnt/sdboot/ Reboot After I followed those steps, I was able to have the upgrade 100% functional.\nIf you want to use containers In case you want to use containers or a container orchestrator such as K3S, make sure that the fields cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory are set in the /boot/firmware/cmdline.txt\nReferences https://forums.raspberrypi.com/viewtopic.php?t=359795 https://www.makeuseof.com/how-to-boot-raspberry-pi-ssd-permanent-storage/ https://pysselilivet.blogspot.com/2020/10/raspberry-pi-1-2-3-4-usb-ssd-boot.html ","permalink":"https://blog.donkeysharp.xyz/boot-rpi-3-model-b-from-ssd/","summary":"\u003cp\u003e\u003cimg alt=\"alt text\" loading=\"lazy\" src=\"/img/rpi-homelab.jpeg\"\u003e\u003c/p\u003e\n\u003cp\u003eHey everyone! In this post I\u0026rsquo;m going to share my experience of USB booting the Raspberry Pi 3 Model B from an SSD. Although it is something I did almost one year ago, I wanted to share it\u0026hellip; sorry for the 1 year delay xD!\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eProbably there might be something I missed on my solution, so any feedback is really appreciated!\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eBefore the pandemic started, a friend of mine sold me 4 Raspberry Pi 3 Model B and one Raspberry Pi 3 Model B+. I used them for an electronics + home automation project I no longer need anymore, so I decided to repurpose them to something new, a new project I started working on for which I will make another blog entry.\u003c/p\u003e","title":"Booting Raspberry Pi 3 Model B from SSD"},{"content":"\nIntroducing AWS Nuke! In this post I will give you a quick introduction to AWS Nuke, a tool developed in Golang that aims to delete all resources in an AWS account. This tool helped me a lot.\nUse Cases Cleaning Free Tier AWS Account This was a personal use-case, I created a free tier AWS Account some months ago and I\u0026rsquo;ve been using it for different purposes, some of the resources I created were via Terraform which made it simpler to delete the resources I created after using them, on the other hand, I created other resources manually without tracking them, some of them were costing me money!. So I prefered to delete everything in this account as I use it for learning purposes only. AWS Nuke is a great fit for this task.\nNote on new free plan accounts: AWS announced their new free tier plans, instead of giving you free usage of some services during the free tier period they give you accounts with 100 USD instead for six months which I personally think is better for new people coming to AWS, as the previous free tier didn\u0026rsquo;t include some resources and using them costed money e.g. NAT Gateway. So still, if you don\u0026rsquo;t want to burn your 100 USD quickly, AWS Nuke can help you!\nCleaning Research Accounts It is very common that some companies have accounts that are used for research, where engineers can try new services, proof of concepts, etc. Depending on how the resources were created it can be simple or not to remove and keep track of all of them. The good thing is that these research accounts are not supposed to have running production-infrastructure, hence deleting the resources to save some money is a perfect case for AWS Nuke as well.\nWarning: The two use cases for AWS Nuke I mentioned, consider temporal or ephemeral accounts only. It is important to note that using Infrastructure As Code tools or at least having a good tagging convention (so it is simple to identify which AWS resources exist) can prevent the need of this tool, which in my opinion is similar to using kill -9 in Unix systems, i.e. use it as your last option.\nUsing AWS Nuke Requirements Before moving forward, make sure your AWS Account has an alias associated with it, it is a MUST have. For this, log into your account, go to the IAM service and in the right section there is an \u0026ldquo;Account\u0026rdquo; section where you can edit the alias.\nSince this tool can be very destructive, it is important that you know what you are doing and mainly what you are about to delete. Anyway the tool runs in dry-run mode by default i.e. it will not apply any changes until you add a specific flag and add some extra confirms. Fortunately, you can be as specific as possible on what you want to delete. From specific resources of certain kind to all resources of one or multiple kinds.\nSomething I loved about how it is programmed, is that it will fail if the alias of you AWS account has the prod string in it, which in my opinion is a very important validation to prevent any execution by mistake.\nFor downloading and installing follow its documentation page. Once installed, you can continue with the next steps.\nIn addition to the tool, it is expected that you already have CLI access to AWS.\nConfiguring your YAML file AWS Nuke needs a configuration file where you can specifiy which accounts will be impacted, which resources will be included excluded, etc. This file is in YAML format.\nIn my case, what I wanted to do was to delete all resources in my AWS Account, except the default VPC, IAM User I used for admin, its access keys and MFA configuration.\nThis is the configuration file I used.\n# aws-nuke-config.yml regions: - us-east-1 # only delete in us-east-1 - global resource-types: excludes: # Some optimizations, for instance do not delete each S3 Object # or DynamoDBTable record, internally aws nuke will empty the bucket anyway - OSPackage - S3Object - DynamoDBTableItem # Keep for default VPC - EC2DefaultSecurityGroupRule # Do not remove IAM User and its dependencies - IAMUser - IAMLoginProfile - IAMUserAccessKey - IAMVirtualMFADevice - IAMUserPolicyAttachment blocklist: - \u0026#34;999999999999\u0026#34; # aws nuke always requires to have an account blocklist accounts: \u0026#34;123456789777\u0026#34;: # my account filters: # Exclude all resources that have the DefaultVPC or the IsDefault properties EC2DHCPOption: - property: DefaultVPC value: \u0026#34;true\u0026#34; EC2InternetGateway: - property: DefaultVPC value: \u0026#34;true\u0026#34; EC2InternetGatewayAttachment: - property: DefaultVPC value: \u0026#34;true\u0026#34; EC2RouteTable: - property: DefaultVPC value: \u0026#34;true\u0026#34; EC2Subnet: - property: DefaultVPC value: \u0026#34;true\u0026#34; EC2VPC: - property: IsDefault value: \u0026#34;true\u0026#34; # END: Filter all default VPC resources Executing AWS Nuke! Executing is very simple, once AWS Nuke is installed you only need to run as follows in order to have a dry-run plan of what is about to be deleted.\n$ aws-nuke nuke --config ./aws-nuke-config.yml This will give a plan and as the screenshot, the records that are to be deleted have the would be removed string.\nI encourage you to try it Try removing or modifying your YAML configuration file and see what changes in the plan.\nThe Last Step, Nuke\u0026rsquo;em All! After you are happy with the deletion plan, you can run:\n$ aws-nuke nuke --config ./aws-nuke-config.yml --no-dry-run-mode Which will ask for your alias and confirm that you really want to delete the resources for that account.\nFinal Thoughts I hope you find this post useful. AWS Nuke helped me a lot with a personal account I use for learning purposes, so I can save some dollars. But never forget that this is a destructive tool, you MUST be very careful while using it.\n","permalink":"https://blog.donkeysharp.xyz/intro-to-aws-nuke/","summary":"\u003cp\u003e\u003cimg alt=\"alt text\" loading=\"lazy\" src=\"/img/aws-nuke.png\"\u003e\u003c/p\u003e\n\u003ch2 id=\"introducing-aws-nuke\"\u003eIntroducing AWS Nuke!\u003c/h2\u003e\n\u003cp\u003eIn this post I will give you a quick introduction to \u003ca href=\"https://aws-nuke.ekristen.dev/\"\u003eAWS Nuke\u003c/a\u003e, a tool developed in Golang that aims to delete all resources in an AWS account. This tool helped me a lot.\u003c/p\u003e\n\u003ch2 id=\"use-cases\"\u003eUse Cases\u003c/h2\u003e\n\u003ch3 id=\"cleaning-free-tier-aws-account\"\u003eCleaning Free Tier AWS Account\u003c/h3\u003e\n\u003cp\u003eThis was a personal use-case, I created a free tier AWS Account some months ago and I\u0026rsquo;ve been using it for different purposes, some of the resources I created were via Terraform which made it simpler to delete the resources I created after using them, on the other hand, I created other resources manually without tracking them, some of them were costing me money!. So I prefered to delete everything in this account as I use it for learning purposes only. AWS Nuke is a great fit for this task.\u003c/p\u003e","title":"Nuke'em all! Using AWS Nuke to clean your AWS accounts"},{"content":"In this post, I will present a solution (one I personally find quite tidy) to a problem I encountered with AWS SSO and the Serverless Framework.\nServerless Framework is one of my favorite tools when I need to work with AWS Lambda and other serverless services, it is an alternative to AWS SAM, but personally, I prefer Serverless Framework due to its support for various cloud providers and plugins. Previously, I had used Serverless to access AWS API by configuring my ~/.aws/credentials file. However this time I was using AWS Single Sign-On to access AWS API from my local computer. Unfortunately, when I wanted to deploy some Lambda functions using the sls CLI, I was not able to do it, it shows a message saying the AWS profile I was using was not configured, even I logged in successfully a couple minutes ago.\n$ export AWS_PROFILE=\u0026#39;some-aws-sso-profile\u0026#39; $ aws sso login # Logs in successfully $ sls deploy --stage dev DOTENV: Loading environment variables from .env: Deploying some-random-lambda to stage dev (us-east-1) âœ– Stack some-random-lambda failed to deploy (64s) Environment: linux, node 18.16.0, framework 3.33.0 (local) 3.33.0v (global), plugin 6.2.3, SDK 4.3.2 Docs: docs.serverless.com Support: forum.serverless.com Bugs: github.com/serverless/serverless/issues Error: AWS profile \u0026#34;some-aws-sso-profile\u0026#34; doesn\u0026#39;t seem to be configured So after some googling I discovered that Serverless does not support AWS SSO, it appears that sls expects the ~/.aws/credentials file to be configured, but AWS SSO doesn\u0026rsquo;t require storing credentials locally since it generates temporary credentials each time you log in.\nI read that I can install a Serverless plugin named Better Credentials. However, I personally prefer to avoid installing and versioning a plugin that\u0026rsquo;s only useful for local development.\nHow AWS SSO store credentials After you login successfully, an access token is stored in a JSON file located at ~/.aws/sso/cache. Fortunately, this access token can be used to obtain the actual ACCESS KEY and ACCESS SECRET KEY, which can be used added to the ~/.aws/credentials file, which is the one sls CLI expects to be configured.\nThe content of this JSON file looks like:\n{ \u0026#34;startUrl\u0026#34;: \u0026#34;https://\u0026lt;some-id\u0026gt;.awsapps.com/start#/\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;accessToken\u0026#34;: \u0026#34;\u0026lt;an-access-token\u0026gt;\u0026#34;, \u0026#34;expiresAt\u0026#34;: \u0026#34;2023-10-20T05:58:17Z\u0026#34; } Additionally, my ~/.aws/config file includes the next SSO configuration:\n[profile some-aws-sso-profile] sso_start_url = https://\u0026lt;some-id\u0026gt;.awsapps.com/start#/ sso_region = us-east-1 sso_account_id = 123456789012 sso_role_name = MyRoleName region = us-east-1 In order to retrieve the actual ACCESS KEY and ACCESS SECRET KEY execute the next command:\n$ export SSO_TOKEN=\u0026#39;token-from-the-json-file\u0026#39; $ aws sso get-role-credentials --access-token $SSO_TOKEN --role-name MyRoleName --account-id 123456789012 { \u0026#34;roleCredentials\u0026#34;: { \u0026#34;accessKeyId\u0026#34;: \u0026#34;an-access-key\u0026#34;, \u0026#34;secretAccessKey\u0026#34;: \u0026#34;a-secret-access-key\u0026#34;, \u0026#34;sessionToken\u0026#34;: \u0026#34;a-session-token\u0026#34; \u0026#34;expiration\u0026#34;: 1698038986000 } } This information can be added to the ~/.aws/credentials file using the same AWS profile name, which will make the sls CLI work as expected.\nIntroducing aws-sso-creds-helper util Although the solution previously mentioned will work, it involves several manual steps. Fortunately there is a utility that automates this process. It is a JS utility known as aws-sso-creds-helper.\n$ npm install -g aws-sso-creds-helper Putting it all together The final step is integrating all these elements. To make the entire solution work, all you need to do is execute the ssocreds command immediately after using AWS SSO login.\n$ export AWS_PROFILE=\u0026#39;some-aws-sso-profile\u0026#39; $ aws sso login # Logs in successfully $ ssocreds -p $AWS_PROFILE # Adds the temporal credentials to the ~/.aws/credentials file $ sls deploy --stage dev # It deploys successfully Final thoughts Despite the introduction of an extra tool to work with AWS SSO, I find this solution to be elegant as it eliminates the need to modify or add extra dependencies to the project solely for local development purposes.\n","permalink":"https://blog.donkeysharp.xyz/serverless-aws-sso/","summary":"\u003cp\u003eIn this post, I will present a solution (one I personally find quite tidy) to a problem I encountered with AWS SSO and the Serverless Framework.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.serverless.com/framework/docs\"\u003eServerless Framework\u003c/a\u003e is one of my favorite tools when I need to work with AWS Lambda and other serverless services, it is an alternative to \u003ca href=\"https://aws.amazon.com/serverless/sam/\"\u003eAWS SAM\u003c/a\u003e, but personally, I prefer Serverless Framework due to its support for various cloud providers and plugins. Previously, I had used Serverless to access AWS API by configuring my \u003ccode\u003e~/.aws/credentials\u003c/code\u003e file. However this time I was using \u003ca href=\"https://aws.amazon.com/what-is/sso/\"\u003eAWS Single Sign-On\u003c/a\u003e to access AWS API from my local computer. Unfortunately, when I wanted to deploy some Lambda functions using the \u003ccode\u003esls\u003c/code\u003e CLI, I was not able to do it, it shows a message saying the AWS profile I was using was not configured, even I logged in successfully a couple minutes ago.\u003c/p\u003e","title":"Using Serverless Framework with AWS SSO"},{"content":"Some time ago I was working on creating a local docker-based development environment for some microservices at work so developers can have the necessary infra components on their machines and that will help them with their daily tasks. Initially, the business logic of some microservices were a black box to me. After containerizing the applications and creating the docker-compose setup, some of them started failing and after checking the logs it turns out that the applications were using AWS SDK to get ec2 instance metadata.\nFor those who are not familiar with EC2 metadata, it is a set of HTTP endpoints that are served in the 169.254.169.254 IP address. This is used to retrieve metadata such as instance ip, AWS region, availability zone, IAM credentials, etc. And internally the AWS SDK uses these enpodints for the same purpose.\nBy default, any user from their local machines won\u0026rsquo;t be able to reach 169.254.169.254 because it is part of the IPv4 Link-Local Address space. So we have two problems:\nRoute all traffic to that special IP address somewhere that is known. Simulate all the HTTP metadata endpoints. Making 169.254.169.254 available locally Fortunately, it is possible to make traffic to 169.254.169.254 to work locally or in a docker-based local environment. Linux and MacOS provide tools that make these kinds of tasks simple.\nDepending on the operating system you are using, there are different ways to route traffic 169.254.169.254 to the local interface.\nIn MacOS you can do it by running the command:\n$ sudo ifconfig lo0 alias 169.254.169.254 In Linux, there are different options:\nUsing ifconfig:\n$ sudo ifconfig lo:0 169.254.169.254 netmask 255.255.255.255 Using iptabes:\n$ sudo iptables -t nat -A OUTPUT -d 169.254.169.254 -j DNAT --to-destination 127.0.0.1 This way any network connection going to 169.254.169.254 will go to our local machine under the hood.\nSimulate all the HTTP metadata endpoints Because a lot of engineers might have the same issue which is accessing the metadata server in a local environment, AWS decide to create a mock server that serves all the HTTP endpoints. The project amazon-ec2-metadata-mock helps us with that.\nJust download the binary for your operating system from its releases page and you can start using it.\nSome options that it has are: For the AWS SDK to work when trying to request metadata, a request to http://169.254.169.254/latest/meta-data must work. Fortunately, we solved the issue of pointing 169.254.169.254 to localhost in the previous section. ec2-metadata-mock by default exposes itself in port 1338, so to trick AWS SDK we need to expose the fake endpoints in port 80.\nFor that, we only need to run it as:\n$ sudo ec2-metadata-mock -p 80 Putting it all together! Now that we know how to route traffic to 169.254.169.254 wherever we want and we have a fake EC2 metadata server, we can join everything and have a fully docker-based development environment.\nFor this, I am going to have a container for the ec2-metadata-mock tool and another which will be named debug that might represent any application that will need access to the EC2 metadata mock server.\nThe source code for this experiment can be found in this repository.\nSo the Docker compose file will look like this:\nversion: \u0026#39;3\u0026#39; services: mock_metadata: image: ec2-metadata-mock build: context: . dockerfile: Dockerfile.metadata-mock debug: image: ec2-metadata-debug build: context: . dockerfile: Dockerfile.debug environment: MOCK_HOSTNAME: mock_metadata command: - sleep - \u0026#39;3600\u0026#39; cap_add: - NET_ADMIN And it contains a container that will run the ec2-metadata-mock server in port 80 and a debug container that simulates an application. Remember the goal is to make any HTTP request from within the application container (in this case the debug container) to http://169.254.169.254/ and the connection goes to the metadata server container under the hood.\nFor applications to route traffic to metadata server, I added an entry point script that runs before the application starts. It retrieves the internal IP address used in the docker network for the metadata server container, then it creates an iptable rule that routes any traffic to 169.254.169.254 to the metadata server ip address. It is important to note that we need to add the NET_ADMIN Linux capability in order to use iptables inside a container.\n#!/bin/bash if [[ -z $MOCK_HOSTNAME ]]; then echo \u0026#34;MOCK_HOSTNAME must be set\u0026#34; exit 1 fi mock_ip_address=$(dig +short $MOCK_HOSTNAME) echo \u0026#39;INFO - Make traffic to 169.254.169.254 go through local mock server\u0026#39; iptables -t nat -A OUTPUT -d 169.254.169.254 -j DNAT --to-destination ${mock_ip_address} exec $@ So once running the whole solution, we can test that indeed we can curl 169.254.169.254 from within the debug container.\n$ docker exec -it local-ec2-metadata_debug_1 curl http://169.254.169.254/latest/meta-data/instance-id i-1234567890abcdef0 Recommendations Although this solution uses iptables and works, I will investigate and make an update to this post if it is possible to define a custom network in docker-compose using the link-local range and assign a specific ip address to the ec2-metadata container.\n","permalink":"https://blog.donkeysharp.xyz/mock-ec2-metadata/","summary":"\u003cp\u003eSome time ago I was working on creating a local docker-based development environment for some microservices at work so developers can have the necessary infra components on their machines and that will help them with their daily tasks. Initially, the business logic of some microservices were a black box to me. After containerizing the applications and creating the docker-compose setup, some of them started failing and after checking the logs it turns out that the applications were using AWS SDK to get ec2 instance metadata.\u003c/p\u003e","title":"Mocking EC2 metadata server locally"},{"content":"It is possible when you are writing code, at some point you might need to pause the execution of a process by calling the sleep(NUMBER_OF_SECONDS) function depending on the problem you are solving. In this post, I will share what I learned so far while investigating the internal kernel mechanisms that make the sleep function work the way it does.\nI appreciate your feedback. I am not an expert on this topic as Linux Kernel\u0026rsquo;s internals are new for me, it was just my curiosity that drove me to get into kernel\u0026rsquo;s source code, and wanted to share what I learned. If you find something incorrect in this post, please let me know by opening an issue on this blog\u0026rsquo;s Github repository. I will really appreciate it!.\nProcess State One of the first concepts that we need to review is the process state. A Linux process has a state associated which represents its execution state in the operating system. A process can be on one of the next states:\nRunning Sleeping (interruptible and uninterruptible) Stopped Zombie When a process is executing instructions in the CPU, it is in Running state and when the process is waiting for something to happen i.e. waiting for network or disk I/O, or it calls the sleep function, it will change to a Sleeping state.\nWe can verify it with a simple C example program:\n// states.c // gcc states.c -o states #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #define SOME_MAGIC_NUMBER 365000000l void start_processing() { long i; printf(\u0026#34;Starting Loop\\n\u0026#34;); for (i = 0; i \u0026lt; (long)(10 * SOME_MAGIC_NUMBER); i++); printf(\u0026#34;Loop Finished\\n\u0026#34;); } int main() { pid_t pid = getpid(); printf(\u0026#34;PID: %d\\n\u0026#34;, pid); start_processing(); printf(\u0026#34;Sleep process\\n\u0026#34;); sleep(5); printf(\u0026#34;Sleep finished\\n\u0026#34;); start_processing(); return 0; } The code above will run a loop for some seconds, then it will sleep for 5 seconds, and finally loop for another number of seconds. Hence we expect the process state to be Running -\u0026gt; Sleeping -\u0026gt; Running.\nWhile the program is being executed we can check the process state with the Htop tool, which will usually show a letter on the 8th column that represents the process state e.g. R (Running), S (Sleeping), T (Stopped), etc.\nAs expected, the states the process had while executing were: Running (R) -\u0026gt; Sleeping (S) -\u0026gt; Running (R).\nTL;DR When a program calls the sleep(NUMBER_OF_SECONDS) function (in C), it will call the nanosleep system call. Other programming languages might call other syscalls that can also send a process to sleep for some seconds e.g. select syscall, but inside the Linux kernel the behavior will be tsimilar no matter the syscall used.\nThe Linux Kernel implementation of the nanosleep system call will do the next:\nInitialize a High Resolution sleep timer. Change the process state to TASK_INTERRUPTIBLE (Sleeping). Starts the High Resolution sleep timer. Calls the process scheduler so it can schedule another process and pause the execution of the current process. The Linux kernel processes High Resolution timers in following way:\nThe computer hardware has a CPU Timer that will cause interruptions periodically, making the kernel handle them by calling the hrtimer_interrupt function. The hrtimer_interrupt function will process the existing timers and see if a timer expired. Once a High Resolution sleep timer expires, the kernel will call the hrtimer_wakeup function which will wake up the process associated with the timer, and that will change the state from TASK_INTERRUPTIBLE (Sleeping) to TASK_RUNNING (Running). Finally some CPU cycles later, the process scheduler will continue the execution of the process exactly where it was paused. Continue reading if you are interested in more details.\nGoing Deeper As software engineers most of the time we might be writing applications that run in user space or user mode such as servers (any kind) or server-side applications, web, mobile or desktop applications, automation scripts, etc.\nNo matter the programming language, framework, or technology, internally a program running in user mode will always interact one way or another with the operating system (for this post Linux) via System Calls or syscalls. For instance, when we read a file, our code (no matter the programming language) might indirectly communicate with the Linux Kernel via the read system call, then the kernel will ask the physical hard drive for the contents of the file we want based on the filesystem, and finally return the requested content to our program.\nThere is a tool called Strace that monitors all the system calls a process executes.\nIf we execute the previous C example using strace, we can see the following output:\n$ strace ./states ... syscalls for process loading (they won\u0026#39;t be useful right now) ... write(1, \u0026#34;PID: 26846\\n\u0026#34;, 11PID: 26846 ) = 11 write(1, \u0026#34;Starting Loop\\n\u0026#34;, 14Starting Loop ) = 14 write(1, \u0026#34;Loop Finished\\n\u0026#34;, 14Loop Finished ) = 14 write(1, \u0026#34;Sleep process\\n\u0026#34;, 14Sleep process ) = 14 nanosleep({tv_sec=5, tv_nsec=0}, 0x7ffefc933be0) = 0 write(1, \u0026#34;Sleep finished\\n\u0026#34;, 15Sleep finished ) = 15 write(1, \u0026#34;Starting Loop\\n\u0026#34;, 14Starting Loop ) = 14 write(1, \u0026#34;Loop Finished\\n\u0026#34;, 14Loop Finished ) = 14 exit_group(0) = ? +++ exited with 0 +++ The real output is longer than the one showed above, but most of the first syscalls are always executed when a process starts and loads the C Standard Library among other things, but the ones we are interested to review are the last ones.\nThe write syscall tells the Kernel that the program wants to display a string on the standard output (in this case the terminal). With that information, we can have an idea that the printf function communicates with the operating system by calling the write syscall.\nThen the nanosleep syscall is called, which will indicate the Linux kernel to move the process from a Running state to a Sleeping state.\nBefore reviewing the Linux implementation of the nanosleep syscall, first we have to review two concepts in order to have a better understanding of what\u0026rsquo;s coming.\nHigh Resolution Timers Inside the Linux Kernel, different components need to wait for some time before executing something, this is where the Timers concept comes in. A timer is a struct in which we define its expiration time (the time to wait) and what function will be called once the timer expires.\nThe Linux Kernel has two types of timers: Low Resolution timers and High Resolution timers. We are going to review High Resolution Timers.\nThe High Resolution Timers framework inside the Linux Kernel is a set of structs and functions that optimally process timers. Its implementation is based on a queue of timers that are sorted by the timer that is sooner to expire. In order to make this queue efficient, it uses a Red Black Tree data structure so insertion and deletion can be done in logarithmic time.\nThis framework is very interesting by itself, in this article we will only review how it is used and some specific parts of its implementation. For more information regarding implementation details read [0], [1] and [2].\nTime management inside the Linux Kernel is a very interesting topic and the talk given by Stephen Boyd titled \u0026ldquo;Timekeeping In The Linux Kernel\u0026rdquo; [4] gave me a better understanding of how the Linux Kernel handles time-related tasks as well as its complexity. This talk helped me a lot. Thanks!\nHardware Timer The CPU (physically) has a clock and internally it has a programmable timer. In simple words, the main purpose of this timer is to cause interruptions periodically (many times within a second) so the kernel can handle them. The frequency of these interruptions will depend on the architecture which is specified in the Linux Kernel during its compilation. The Linux Kernel abstracts this as Clock Event Devices and there is one Clock Event device per CPU. A Clock Event Device is used to schedule the next event interrupt [3].\nWhen a timer interruption happens, the Linux Kernel will handle it by calling a function. [4] mentions that for High Resolution Timers the hrtimer_interrupt function is the handler for Timer interruptions (we will review its code later).\nAnother way to check which handler will be used, is to review the /proc/timer_list read-only file that contains the list of pending timers and the Clock Event devices.\nIn my case, I validated that the handler for these devices on my machine is indeed the hrtimer_interrup function.\n# /proc/timer_list .... Tick Device: mode: 1 Per CPU device: 0 Clock Event Device: lapic-deadline max_delta_ns: 1916620707137 min_delta_ns: 1000 mult: 9624619 shift: 25 mode: 3 next_event: 14403083615478 nsecs set_next_event: lapic_next_deadline shutdown: lapic_timer_shutdown periodic: lapic_timer_set_periodic oneshot: lapic_timer_set_oneshot oneshot stopped: lapic_timer_shutdown event_handler: hrtimer_interrupt \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; the interrupt handler retries: 1316 Tick Device: mode: 1 Per CPU device: 1 Clock Event Device: lapic-deadline max_delta_ns: 1916620707137 min_delta_ns: 1000 mult: 9624619 shift: 25 mode: 3 next_event: 14403083615478 nsecs set_next_event: lapic_next_deadline shutdown: lapic_timer_shutdown periodic: lapic_timer_set_periodic oneshot: lapic_timer_set_oneshot oneshot stopped: lapic_timer_shutdown event_handler: hrtimer_interrupt \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; the interrupt handler retries: 484 .... The rest of devices per CPU of my machine Now we have an idea of High Resolution Timers and that the CPU has a hardware timer that periodically causes interruptions to the Kernel, we can continue with the nanosleep syscall.\nLinux Kernel Implementation of nanosleep The Linux Kernel is a big project, thousands of files and millions of lines of code, navigating through them can be challenging. There is an online tool called LXR that helps navigate the Linux Kernel source code in a friendly way. The URL of the site is https://elixir.bootlin.com/linux/v5.14/source.\nSo far we know that the nanosleep syscall does all the magic to move the process state from Running to Sleeping for a given number of seconds, then to Running state again. Now we will explore the Linux Kernel source code and review what are the internal mechanisms behind that \u0026ldquo;simple\u0026rdquo; behavior.\nFirst, we have to check where the nanosleep syscall is defined. After googleing a little bit, I found a document that specifies how syscalls are defined in the Linux Kernel. Hence we have to search for SYSCALL_DEFINE2(nanosleep, ....), the 2 in SYSCALL_DEFINE2 indicates the number of arguments of the syscall. I know nanosleep has two arguments after reviewing its man page (all syscalls have a man page(2)).\nAfter searching for the term nanosleep in LXR, I found that the syscall is defined in the kernel/time/hrtimer.c file.\nSYSCALL_DEFINE2(nanosleep, struct __kernel_timespec __user *, rqtp, struct __kernel_timespec __user *, rmtp) { struct timespec64 tu; if (get_timespec64(\u0026amp;tu, rqtp)) return -EFAULT; if (!timespec64_valid(\u0026amp;tu)) return -EINVAL; current-\u0026gt;restart_block.nanosleep.type = rmtp ? TT_NATIVE : TT_NONE; current-\u0026gt;restart_block.nanosleep.rmtp = rmtp; return hrtimer_nanosleep(timespec64_to_ktime(tu), HRTIMER_MODE_REL, CLOCK_MONOTONIC); } Of course, every line of code has its reason to be, but I will highlight the call to timespec64_to_ktime which converts the input arguments of the syscall to the ktime struct that is used by the High Resolution Timers framework. Finally, it calls the hrtimer_nanosleep where all fun starts.\nI will go function by function in the order they are called and explain the parts I consider relevant:\nhrtimer_nanosleep function:\nlong hrtimer_nanosleep(ktime_t rqtp, const enum hrtimer_mode mode, const clockid_t clockid) { ... hrtimer_init_sleeper_on_stack(\u0026amp;t, clockid, mode); hrtimer_set_expires_range_ns(\u0026amp;t.timer, rqtp, slack); ret = do_nanosleep(\u0026amp;t, mode); ... } There are three relevant parts here:\nThe HR Sleep timer initialization (we will review this with more detail) The expiration time for the initialized timer. Although that seems a simple task, there is a lot of logic under the hood. Because the Linux Kernel works at the hardware level, to convert human time to computer time it has to use some formulas that are based on the HZ constant that varies depending on the architecture. Also, the concept of Jiffies comes up. I will not go into more details, however [4] explains this part very well. Finally, it calls the do_nanosleep function that has the logic that sends a process to sleep. The hrtimer_init_sleeper_on_stack function (which in the end calls the __hrtimer_init_sleeper function) allocates and initializes an HR sleep timer associated with the current process that is being executed.\nThe function attribute of the HR sleep timer is the callback function, which means that this function will be called after the HR sleep timer expires. In this case, the function attribute of the sleep timer is set to hrtimer_wakeup which we will see later (don\u0026rsquo;t forget this \u0026#x1f609;).\nstatic void __hrtimer_init_sleeper(struct hrtimer_sleeper *sl, clockid_t clock_id, enum hrtimer_mode mode) { ... __hrtimer_init(\u0026amp;scicil-\u0026gt;timer, clock_id, mode); sl-\u0026gt;timer.function = hrtimer_wakeup; // \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; This function will be called after the timer expires sl-\u0026gt;task = current; // \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; Associates the timer with the current process } In the Linux Kernel, the current variable is a pointer to the current process being executed (in our case the program that calls the sleep function).\nBefore continuing with the do_nanosleep function, I will make a parenthesis about the __hrtimer_init function.\nstatic void __hrtimer_init(struct hrtimer *timer, clockid_t clock_id, enum hrtimer_mode mode) { ... timerqueue_init(\u0026amp;timer-\u0026gt;node); } I mentioned that HR Timers use a queue which under the hood is implemented using a Red Black Tree. The call to timerqueue_init functions is only allocating and initializing a Red-Black Tree Node, but not adding it to the tree yet.\nAfter that short parenthesis, let\u0026rsquo;s see what happens inside the do_nanosleep function.\nI initially thought that the do/while loop iterates until the HR sleep timer expired (something like an infinite loop), things happen differently.\nstatic int __sched do_nanosleep(struct hrtimer_sleeper *t, enum hrtimer_mode mode) { struct restart_block *restart; do { set_current_state(TASK_INTERRUPTIBLE); // \u0026lt;\u0026lt;\u0026lt;\u0026lt; This causes the process to go to a Sleeping state hrtimer_sleeper_start_expires(t, mode); if (likely(t-\u0026gt;task)) freezable_schedule(); hrtimer_cancel(\u0026amp;t-\u0026gt;timer); mode = HRTIMER_MODE_ABS; } while (t-\u0026gt;task \u0026amp;\u0026amp; !signal_pending(current)); __set_current_state(TASK_RUNNING); if (!t-\u0026gt;task) return 0; restart = \u0026amp;current-\u0026gt;restart_block; if (restart-\u0026gt;nanosleep.type != TT_NONE) { ktime_t rem = hrtimer_expires_remaining(\u0026amp;t-\u0026gt;timer); struct timespec64 rmt; if (rem \u0026lt;= 0) return 0; rmt = ktime_to_timespec64(rem); return nanosleep_copyout(restart, \u0026amp;rmt); } return -ERESTART_RESTARTBLOCK; } As mentioned before, when the sleep function is called, the current process will pass to a Sleeping state. We can see that happening on the line that calls the set_current_state function that changes the state of the current process to TASK_INTERRUPTIBLE (Sleeping).\nThe call to the hrtimer_sleeper_start_expires function will call other functions until it calls the __hrtimer_start_range_ns function which in turn will call the enqueue_hrtimer function, it is at this point where the timer (node) we initialized before is added to the Red Black Tree structure so the timer can be processed later.\nFinally, the freezable_schedule function indicates the process scheduler to schedule another process because the current process went to sleep and the execution of our process pauses here.\nHow Does The Process Wakes Up? So far we reviewed that the nanosleep implementation changes the state of the process to TASK_INTERRUPTIBLE and pauses the process execution.\nNow that the process state is in TASK_INTERRUPTIBLE state, the process scheduler will not consider the process for execution in the future, until the state of the process is set back to TASK_RUNNING.\nWe mentioned that the Hardware Timer will cause periodic interrupts so the Linux Kernel can handle them by calling the hrtimer_interrupt function on each interruption (multiple times in a second). It is in this function where High Resolution Timers are processed by calling the __hrtimer_run_queues function.\nstatic void __hrtimer_run_queues(struct hrtimer_cpu_base *cpu_base, ktime_t now, unsigned long flags, unsigned int active_mask) { struct hrtimer_clock_base *base; unsigned int active = cpu_base-\u0026gt;active_bases \u0026amp; active_mask; for_each_active_base(base, cpu_base, active) { struct timerqueue_node *node; ktime_t basenow; basenow = ktime_add(now, base-\u0026gt;offset); while ((node = timerqueue_getnext(\u0026amp;base-\u0026gt;active))) { struct hrtimer *timer; timer = container_of(node, struct hrtimer, node); if (basenow \u0026lt; hrtimer_get_softexpires_tv64(timer)) break; __run_hrtimer(cpu_base, base, timer, \u0026amp;basenow, flags); if (active_mask == HRTIMER_ACTIVE_SOFT) hrtimer_sync_wait_running(cpu_base, flags); } } } The __hrtimer_run_queues function will iterate the timers in the Red Black Tree, remember that it will iterate starting by the timers that are sooner to expire. Something to note here is that it will break the while loop if the timer didn\u0026rsquo;t expire yet (why do unnecessary iterations if the head of the queue is a timer that didn\u0026rsquo;t expire yet?). But when a timer expired, it will call the __run_hrtimer function. As we can see, its implementation will call the callback function we set during HR sleep timer initialization.\nstatic void __run_hrtimer(struct hrtimer_cpu_base *cpu_base, struct hrtimer_clock_base *base, struct hrtimer *timer, ktime_t *now, unsigned long flags) __must_hold(\u0026amp;cpu_base-\u0026gt;lock) { ... fn = timer-\u0026gt;function; // \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; This fn function is pointing to the hrtimer_wakeup function ... restart = fn(timer); ... } The function that was set as the callback function during HR sleep timer initialization was the hrtimer_wakeup function.\nstatic enum hrtimer_restart hrtimer_wakeup(struct hrtimer *timer) { struct hrtimer_sleeper *t = container_of(timer, struct hrtimer_sleeper, timer); struct task_struct *task = t-\u0026gt;task; t-\u0026gt;task = NULL; if (task) wake_up_process(task); // \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; Wake up the process!! return HRTIMER_NORESTART; } As we can see this function will call the wake_up_process function sending the process (task) associated with the HR timer as parameter. The wake_up_process function among other things will set the process state to TASK_RUNNING.\nSome CPU cycles later, the process scheduler will resume the execution of this process where it stopped (after the call to the freezable_schedule function). Then the rest of the do_nanosleep function will free memory, remove the timer from the Red Black Tree and continue with the execution. And that\u0026rsquo;s it!\nThere are other alternatives to nanosleep The nanosleep syscall is not the only syscall that can be used to sleep a process. For example Python time.sleep function uses the select syscall under the hood, however, if we review the implementation of the do_select function which in turn calls the schedule_hrtimeout_range function, you will notice that it calls the schedule_hrtimeout_range function which initializes and starts a High Resolution sleep timer, and tells the process scheduler to schedule another process (same logic as with nanosleep).\nPython sleep is one example, but other languages might be using different syscalls.\nLast Comments Although calling the sleep function in our programs might be something trivial, all the mechanisms that live behind that simple behavior are amazing. When I started digging to understand what happens when you call a sleep function, I wouldn\u0026rsquo;t have imagined how much I was going to learn.\nIn case there are parts of this post that you find incorrect, let me know by opening an issue on this blog\u0026rsquo;s Github repository. I will really appreciate it!.\nReferences [0] https://lwn.net/Articles/167897/ [1] https://lwn.net/Articles/152436/ [2] https://www.kernel.org/doc/html/latest/timers/hrtimers.html [3] https://www.kernel.org/doc/html/latest/timers/highres.html [4] https://www.youtube.com/watch?v=Puv4mW55bF8 ","permalink":"https://blog.donkeysharp.xyz/post/what-happens-when-a-process-goes-to-sleep/","summary":"\u003cp\u003eIt is possible when you are writing code, at some point you might need to pause the execution of a process by calling the \u003ccode\u003esleep(NUMBER_OF_SECONDS)\u003c/code\u003e function depending on the problem you are solving. In this post, I will share what I learned so far while investigating the internal kernel mechanisms that make the \u003ccode\u003esleep\u003c/code\u003e function work the way it does.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eI appreciate your feedback.\u003c/strong\u003e I am not an expert on this topic as Linux Kernel\u0026rsquo;s internals are new for me, it was just my curiosity that drove me to get into kernel\u0026rsquo;s source code, and wanted to share what I learned. If you find something incorrect in this post, please let me know by opening an issue on this blog\u0026rsquo;s \u003ca href=\"https://github.com/donkeysharp/donkeysharp.github.io\"\u003eGithub repository\u003c/a\u003e. I will really appreciate it!.\u003c/p\u003e","title":"What Happens When A Linux Process Goes To Sleep?"},{"content":"Hello, during the months of October and November different social and political conflicts occurred in Bolivia, this entry is not so much to discuss the political issue, it will be a 100% technical entry but it is related to those events.\nThe previous week a large number of clients from a local ISP received an SMS with a link bit.ly to an MP4 video in Dropbox that was later deleted.\nThis video went viral not only by SMS but on social networks and local media, it showed a recorded call in which a local leader talks to Evo Morales.\nThere were rumors that the video was malware or was used to track people who open it. I do not usually pay too much attention to those comments, but this time it interested me because days ago Facebook had reported a Stack Buffer Overflow security flaw that could possibly generate RCE in the WhatsApp application precisely with a malicious MP4 video! This is the link of the security alert.\nWell, I was very curious to see if there was indeed something malicious in that video or it was just spam. From the beginning I knew that I would learn several things because I had little idea of â€‹â€‹how I could analyze whether the video was malicious. The video was sent to a public chat group I\u0026rsquo;m part of, and my friends Gonzalo y Cristhian and Cristhian told me to put an entry on my blog about my findings and in case thare are not findings I could speak about recording angles of the video. The rest of this entry will be about what I found and learned. Thanks for encouraging me to investigate guys, I had a lot of fun.\nThe video The video was just an mp4 file called evo-telefono.mp4 with this sha512:\na378c367e3c9a4be3ca639822fe79adf75aaa30ba25ca97ff8f6eb3945d36ed9eb160703ed611ecfe5fdc448c6a099e8af3a74a2c7078695db9c258a25800246 Firstly I verified that it is indeed an mp4 by viewing the magic numbers of the file. Generally the magic numbers of any file are the first bytes of it.\nIn the case of an mp4 the bytes should be: 00 00 00 (18 oo 1C) 66 74 79 70 6D 70 34 32and when running:\n$ hexdump -C evo-telefono.mp4 | head -n1 00000000 00 00 00 1c 66 74 79 70 6d 70 34 32 00 00 00 01 |....ftypmp42....| It indeed has those magic numbers that identify an mp4 file. A simpler way to verify is using the command file:\n$ file evo-telefono.mp4 evo-telefono.mp4: ISO Media, MP4 v2 [ISO 14496-14] First ideas To see if I find something interesting I ran the command strings against the video and see if I found any interesting ASCII strings. As the vulnerability explains that the error lies in the metadata of the file, then I imagined that the structure was something internal as \u0026ldquo;key-value\u0026rdquo; all using ASCII, I discarded that assumption when I didn\u0026rsquo;t find anything interesting using strings.\n$ strings evo-telefono.mp4 The next idea was to use a tool that can view metadata of different formats called mediainfo and mediainfo-gui. For this first stage of the analysis I used mediainfo because I didn\u0026rsquo;t understand very well how it was presented with mediainfo-gui, but this gui later was much more useful.\nUsing mediainfo against the video evo-telefono.mp4 I got the following output, I will put only certain parts but leave this gist with full command output.\n$ mediainfo evo-telefono.mp4 General Complete name : evo-telefono.mp4 Format : MPEG-4 Format profile : Base Media / Version 2 Codec ID : mp42 (isom/mp41/mp42) File size : 6.41 MiB Duration : 1 min 2 s Overall bit rate : 857 kb/s Encoded date : UTC 2019-11-21 12:31:56 Tagged date : UTC 2019-11-21 12:31:59 ... Visually that information is key-value but most of those strings were not found when I used strings which leads me to the conclusion that the format is mostly binary and that the numbers in this output are not represented as an ASCII string, but in bytes similar to an IP or TCP packet.\nNote: An IP packet is binary in the sense that the IP and other flags are not presented as ASCII text, but encapsulated in bytes. For example the ip 10.0.1.11 (9 bytes in ASCII) is represented in 4 bytes as 0A 00 01 0B\nAt this point I felt like I was going nowhere. The next thing I did is search if there was already an exploit or tutorial on how to exploit this CVE and effectively with the help of Google, I came to this repository in Github.\nThis repo had an mp4 called poc.mp4 which in theory exploited this vulnerability and next to this file the WhatsApp dynamic library libwhatsapp.so and a C program that invokes this library dynamically using dlfcn.h (I hope I can make a post about dlfcn.h in the future, really interesting). The most important thing about this repository is that it has a sample mp4 file that causes this error, it helped me a lot.\nThe first thing I did was run mediainfo again poc.mp4 and see the differences between evo-telefono.mp4 and poc.mp4, sadly it was more frustrating since the only thing different was a new tag called com.android.version with the value of 9 in ASCII and well, I was out of ideas.\nAt the beginning I thought that next to this tag looking at the hexadecimal, maybe there was a shellcode, looking for common opcodes and that, but I really felt that the strategy I was using was quite \u0026ldquo;naive\u0026rdquo; and I was not understanding the MP4 format as such and well, I think that was the next step.\nMost files have a specification of how they are structured, either in plain text in a format e.g. json or xml or in binary e.g. MP4. I Googled for the spec files, I gave a super fast read to what I found to see if it mentioned things like bytes and things like that and I didn\u0026rsquo;t find one. I paused this analysis for a day to rest.\nUnderstanding the MP4 format and vulnerability Hours after I paused, my friend Elvin sent this link from Hack A Day which gives a summary of the vulnerability and how it is being exploited, thank you very much for sharing, it was one of the most important resources for this investigation. To be honest, when I read it I did not understand certain details that were just the most important, I was still not understanding the structure of a mp4 file.\nFrom this point all the steps I follow are only with the file poc.mp4 and in the end I will apply what I learned to evo-telefono.mp4.\nThe first thing I did was try to reproduce the only step he mentions in the Hack A Day post with the AtomicParsley tool in poc.mp4. When executing it I got a Segmentation Fault but the same with evo-telefono.mp4, apparently it is more an error of the tool that is already discontinued.\n$ AtomicParsley poc.mp4 -T Atom ftyp @ 0 of size: 24, ends @ 24 Atom moov @ 24 of size: 794, ends @ 818 Atom mvhd @ 32 of size: 108, ends @ 140 Atom meta @ 140 of size: 117, ends @ 257 Atom @ 152 of size: 6943, ends @ 7095\t~ ~ denotes an unknown atom ------------------------------------------------------ Total size: 7095 bytes; 4 atoms total. Media data: 0 bytes; 7095 bytes all other atoms (100.000% atom overhead). Total free atom space: 0 bytes; 0.000% waste. ------------------------------------------------------ AtomicParsley version: 0.9.6 (utf8) ------------------------------------------------------ Segmentation fault As you can see in the output a lot of info that I did not understand, but something that they did mention in the Hack A Day post is the position in bytes and that MP4 is a hierarchical structure and the basic structure of MP4 is the \u0026ldquo;Atom\u0026rdquo; (there are different types of atoms). Later I will talk in more detail about the Atoms.\nEach atom has a header that indicates the size of the atom. Being a hierarchical structure an atom can contain other atoms inside. As such, the size of a parent atom is the total of all the bytes of the child atoms and what is highlighted and mentioned in the post is the following:\nThe meta atom has a size of 117 bytes but inside this atom there is an unnamed child atom that has a size of 6943 bytes that is greater than the 117 bytes of the father and well that gives a clue.\nAtom @ 152 of size: 6943, ends @ 7095 ~ In the Hack A Day post it later refers to 33 bytes and 1.6GB of the size of the atom and then I got lost again and that was effectively the key to understanding the error.\nThe next thing to do â€” I\u0026rsquo;d already procrastinated it enough â€” was to read the specs from an MP4 file. From the files I got, none were at the level I wanted, that is, at the byte level. Luckily, once again the Hack A Day post refers to two documents: the specification on the Apple Developers site and another slightly more elaborate specification and this is when this error was fully understood.\nThe MP4 format As a super short summary after reading the specification it can be said that MP4 is hierarchically organized in blocks called Atoms (which I mentioned above) and each Atom has an 8-byte header, 4 bytes define the size of the Atom and the other 4 bytes (generally in ASCII) represent the type of the Atom.\nNow there are several types of Atoms but the ones shown in this post are the following:\nmoov which represents \u0026ldquo;Movie Data\u0026rdquo; and may contain other Atoms inside. Basically the content of this atom is information from the movie e.g. when it was created, duration, etc. meta another atom that encapsulates information of the Metadata. hdlr an atom that is considered the handler and comes inside the meta atom, this atom defines the whole structure that all the metadata will have inside the meta atom. The following image shows a graphic representation of the box-shaped atoms:\nBut how can this format be understood at a binary level? This part took a bit of time but in the end using mediainfo-gui and a hexadecimal editor was much simpler.\nAn atom has an 8-byte header where the first 4 bytes indicate the size of the atom and the following 4 bytes the type of atom e.g. moov, meta, hdlr among others. Then comes N bytes which are the content of the atom, where N is the size of the atom specified in the first 4 bytes subtracting 8 bytes (the header).\nAn example:\nA 794-byte atom of type moov would be represented as:\n00 00 03 1A 6D 6F 6F 76 XX XX XX ... 786 bytes ... XX XX According to the specification the first 4 bytes are the size, the next 4 bytes are the type of atom and the rest is the body.\nThe first 4 bytes can be represented as 0x0000031A or 0x31A which in decimal is 794.\nThe next 4 bytes indicate the type of atom that is ASCII text, so it is only to convert the following bytes to its character in ASCII and we will have:\n6D -\u0026gt; m 6F -\u0026gt; o 6F -\u0026gt; o 76 -\u0026gt; v Now the content of this atom (the remaining 786 bytes) can be other atoms identified in the same way and based on the specification of the MP4 format.\nThere are special cases of some Atoms that have a special format. An example of these special atoms is that after the header we do not directly define another atom, it is possible that some bytes are reserved for some purpose (flags, etc.) and after these reserved bytes it is then possible to define child atoms. Remember this paragraph as you will see is the key to success.\nContinuing with examples in poc.mp4, there is an atom called mdta which is basically the key name in the metadata (this atom has the key com.android.version that I mentioned above). Like another atom, it is represented with an 8-byte header and then the content:\n00 00 00 1B 6D 64 74 61 63 6F 6D 2E 61 6E 64 72 6F 69 64 2E 76 65 72 73 69 6F 6E Where:\n0x0000001B represents the size that in decimal is 27 bytes 6D 64 74 61 represents in ASCII mdta the type of the atom 63 6F 6D 2E 61 6E 64 72 6F 69 64 2E 76 65 72 73 69 6F 6E converting to ASCII represents com.android.version In order not to make this post too long, I have created a video where I show in more detail how to interpret this format at a hexadecimal level using mediainfo-gui (it was originally recorded in Spanish).\nUnderstanding the bug In the previous video it is shown how to understand and navigate through the different atoms using mediainfo-gui atoms viewer and navigating it in a hexadecimal level. In this section, using the knowledge acquired so far, we will see how the bug reported in the CVE can be used.\nPart of CVE-2019-11931: The issue was present in parsing the elementary stream metadata of an MP4 file and could result in a DoS or RCE\nThis CVE and the way to cause the overflow just says that it is in the metadata, that is, in the Atom meta. In the Hack A Day post, it refers to two specifications of the mp4 format. The Apple Developers link indicates that after defining the meta type atom as a child, an hdlrf type atom should be defined and if we see in the hexadecimal, that exactly happens. From the 8C offset as shown in the following images .\nheader size meta type header size hdlr type ----------- ----------- ----------- ----------- 00 00 00 75 6d 65 74 61 00 00 00 21 68 64 6c 72 ... 0x00000075 meta 0x00000021 hdlr 0x75 o 117 meta 0x21 o 33 hdlr What you see in mediainfo-gui and in the hexadecimal representation makes a lot of sense, but if we remember the output of the AtomicParsley application, the hdlr atom that is actually defined does not appear at any time and instead shows an error for an un-named atom that has 6943 bytes in size (greater than 117 of its parent meta atom).\n$ AtomicParsley poc.mp4 -T Atom ftyp @ 0 of size: 24, ends @ 24 Atom moov @ 24 of size: 794, ends @ 818 Atom mvhd @ 32 of size: 108, ends @ 140 Atom meta @ 140 of size: 117, ends @ 257 Atom @ 152 of size: 6943, ends @ 7095\t\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; atom without name ~ denotes an unknown atom ------------------------------------------------------ Total size: 7095 bytes; 4 atoms total. Media data: 0 bytes; 7095 bytes all other atoms (100.000% atom overhead). Total free atom space: 0 bytes; 0.000% waste. ------------------------------------------------------ AtomicParsley version: 0.9.6 (utf8) ------------------------------------------------------ Segmentation fault All the atoms in the output show their type ftyp, moov, mvhd, meta and within meta this unknown atom with a size that is basically the rest of the size of poc.mp4 which it weighs 7095 bytes, since if you count by adding the size of the atom ftyp (24), mvhd (108), the header of meta (8) results in 24 + 108 + 8 = 148 but 7095 - 148 = 6947 which are 4 extra bytes from the 6943 given in the AtomicParsley output.\nWhat are these 4 bytes? It is seen that in mediainfo-gui everything is shown well, as if nothing had happened and the format is correct. However, in AtomicParsley it shows an atom without any type and we have 4 extra bytes.\nWhat happens and I managed to deduce is the following: if we see the file type of poc.mp4 using the command file it is seen to be ISO Media, MP4 v2 [ISO 14496-14]. It happens that the mp4 format is based on the ISO 14496-14 standard, which is the continuation of another standard called ISO 14496-12 and the most interesting thing comes here: in the Hack A Day post, it mention two specification links: from Apple Developers and one a little more elaborate. In the second link it clearly indicates that it is the specification of the ISO / IEC 14496-12 standard and this is exactly where it indicates that the atom meta after the 8 byte header has 4 bytes reserved (3 for flags and 1 for version) and after these 4 bytes you can just define other child atoms.\nIf so, and re-analyzing the hexadecimal, we have the following:\n4 bytes header size meta type reservados header size type invÃ¡lido ----------- ----------- ----------- ----------- ----------- 00 00 00 75 6d 65 74 61 00 00 00 21 68 64 6c 72 00 00 00 00 ... Knowing that, the parser confuses those 4 bytes and makes the definition of the child atom header to be 68 64 6c 72 00 00 00 00 where 0x68646c72 is the size and 00 00 00 00 is the type that in ASCII is is not valid name for MP4 and that is exactly why the AtomicParsley application failed. Now if you convert 0x68646c72 to decimal, you get the value of 1751411826, that is, the size of that unknown atom will be 1.6GB (same as in the Hack A Day post).\nI was now able to confidently reproduce the evo-telefono.mp4 file.\nWhat I deduce is this: the libwhatsapp.so library (repo) was possibly complying with the ISO 14496-12 standard and not the ISO 14496-14 or it was simply a development error as in the current WhatsApp update it does not happen. Now related to the AtomicParsley application, it is very likely that the same will happen since this one had problems with that unknown atom.\nAnalyzing the file evo-telefono.mp4 Up to this point I was happy because of how much I had learned, but the main objective was to analyze if this file had something malicious specifically by this CVE.\nFinally I can say that NO, it does not have something malicius (at least not for this CVE).\nThe reasons are as follows: in order to cause this error, you need to have defined the meta atom, which is not defined within any atom in the evo-telefono.mp4 video. Anyway it is the first time that I analyze a file at this level and well, in some chat groups they mentioned Pegasus and that, it would be worth to give it a check.\nFinal comments Seriously it was a good experience when it came to learning and seeing these type of attack vectors that take advantage of this kind of details.\nAnother thing learned that for this type of analysis, just like when analyzing network protocols it is necessary to read the RFC, in the case of formats it is necessary to read the specification of the format of some file. There is an ezine called Paged Out which in one of its sections talks about techniques of reversing file formats, I recommend it. Link.\nFinally I thank Elvin again for sharing that Hack A Day post that was key to this analysis and Gonzalo and Cris for encouraging me to do it.\n","permalink":"https://blog.donkeysharp.xyz/post/analyzing-evo-video/","summary":"\u003cp\u003eHello, during the months of October and November different social and political conflicts occurred in Bolivia, this entry is not so much to discuss the political issue, it will be a 100% technical entry but it is related to those events.\u003c/p\u003e\n\u003cp\u003eThe previous week a large number of clients from a local ISP received an SMS with a link \u003ccode\u003ebit.ly\u003c/code\u003e to an MP4 video in Dropbox that was later deleted.\u003c/p\u003e","title":"Analyzing a Video Looking for Possible Malware"},{"content":"Hi, some time ago I posted about my initial setup in my Debian personal machines that use for work or personal projects. Some of the things I setup are: applications, desktop look and feel, etc. The last months I installed and re-installed my Debian machines so many times in different computers that I use (new computers, new hard drives, etc.) and this taks was repetitive.\nBasically what I was doing was to review my previous blog post and repeat those steps. So far it has worked for me, but as it\u0026rsquo;s becoming repetitive, I was encouraged to automate this whole process, both installation and configuration of certain applications and the configuration of the look and feel of the desktop (with the setup that I always use).\nThe idea While I could have done this project using a simple Bash script, I was encouraged to use Bash + Ansible for fun and practice \u0026#x1f604;.\nWhat I had in mind when I started this project was that as soon as the installation of the operating system was finished (in my case the Debian distribution), I would only have to execute a command and \u0026ldquo;magically\u0026rdquo; all the applications, configurations, etc. on my personal setup would be applied.\nAnalyzing the Project I posted this project on Github if you want to see the source code.\nAs this is a project that uses Ansible, there is a certain convention regarding the directory tree, file names, etc. I decided to use the following structure:\nâ”œâ”€â”€ init-setup.sh â”œâ”€â”€ inventory â”œâ”€â”€ README.md â”œâ”€â”€ roles â”‚Â â”œâ”€â”€ chrome â”‚Â â”œâ”€â”€ common â”‚Â â”œâ”€â”€ docker â”‚Â â”œâ”€â”€ dotenvs â”‚Â â”‚Â â”œâ”€â”€ tasks â”‚Â â”‚Â â””â”€â”€ templates â”‚Â â”œâ”€â”€ games â”‚Â â”‚Â â””â”€â”€ tasks â”‚Â â”‚Â â””â”€â”€ main.yml â”‚Â â”œâ”€â”€ mysql â”‚Â â”‚Â â”œâ”€â”€ files â”‚Â â”‚Â â””â”€â”€ tasks â”‚Â â”œâ”€â”€ node â”‚Â â”œâ”€â”€ php7 â”‚Â â”œâ”€â”€ virtualbox â”‚Â â”œâ”€â”€ vscode â”‚Â â””â”€â”€ xfce4 â”œâ”€â”€ setup-playbook.yml â””â”€â”€ update-desktop-layout.sh In Ansible what would be called the main program is the playbook which is responsible for executing different tasks against one or more servers, in this case, the main program for Ansible would be the file setup-playbook.yml. As you can see this file has a section called roles.\nNote: In Ansible what is called a role references to a reusable piece of code (like a module). This allows us to have the project better organized and has an internal directory structure as seen in the directory tree above.\nI divided the project into different roles, each for a different purpose such as: a specific application or a group of applications including their settings. For example, in the role common install all the desktop utilities and command line utilities that I use every day. And in general there are roles for specific things that I use, Docker, software development tools, technologies and others. If you are more curious you can check the repo \u0026#x1f604;.\nContinuing with the explanation, within Ansible there is a fairly important concept which is the inventory, which indicates all the servers where the tasks specified in the roles will be applied. This project has a file called inventory with particular options, since all the tasks will not be executed against several servers, but against one and it is the same local machine.\n[local] localhost ansible_connection=local [local] is the group of servers, I named it local but it could be called anything, the important thing is that if you change its name, this name should also be reflected in setup-playbook.yml in hosts: local. Then the following lines indicate the hostname which in this case is localhost and ansible_connection=local which indicates that it will be a local execution and thus avoid the SSH authentication process that Ansible performs on each execution towards the same machine.\nFinally the init-setup.sh script is a wizard which asks for certain options before going through the entire installation and configuration process. This would become the \u0026ldquo;magic\u0026rdquo; command that takes care of everything:\nbash \u0026lt;(wget -q -O- https://raw.githubusercontent.com/sguillen-proyectos/fresh-install-setup/master/init-setup.sh) What do I gain with this? Well, first I learned a couple of things that I didn\u0026rsquo;t know about Ansible, I had fun and the most important thing for me (besides that was the objective of this project) is that now I save all the time of manual configuration that I carried out in a newly operating system installation.\nAlthough I already had my guide of what packages to install and what configurations to perform, that took me between one and two hours, now all this time The setup is mainly conditioned to the speed of the internet.\nSomething in this project that is quite useful for me is that I managed to standardize my desktop settings (in this case Xfce4), since in many occasions \u0026ldquo;stylizing\u0026rdquo; my desktop is what I wasted the most time on.\nFinal Comments Although this project is supposed to be for my personal setup, I decided to share it in case the idea of automating the environment serves any of the people who read this blog post.\nDuring the time I invested to carry out this project, several fun things happened that made me deny that I learned something new. I\u0026rsquo;ll write about that series of unfortunate events in a next post.\nHappy Hacking!\n","permalink":"https://blog.donkeysharp.xyz/post/debian-post-installation-automated-setup/","summary":"\u003cp\u003eHi, some time ago I \u003ca href=\"https://blog.donkeysharp.xyz/posts/my-computer-setup/\"\u003eposted about my initial setup\u003c/a\u003e in my Debian personal machines that use for work or personal projects. Some of the things I setup are: applications, desktop look and feel, etc. The last months I installed and re-installed my Debian machines so many times in different computers that I use (new computers, new hard drives, etc.) and this taks was repetitive.\u003c/p\u003e\n\u003cp\u003eBasically what I was doing was to review my previous blog post and repeat those steps. So far it has worked for me, but as it\u0026rsquo;s becoming repetitive, I was encouraged to automate this whole process, both installation and configuration of certain applications and the configuration of the look and feel of the desktop (with the setup that I always use).\u003c/p\u003e","title":"Automating post-installation setup in my personal Debian machines"},{"content":"The other day when I went out eating with my friend Francisco, he commented me about a personal project and what he wanted to accomplish. Listening to his questions the first tool that came to my mind that would solve some of the challenges he has was OpenSSH.\nWhat is SSH? SSH is a short for Secure Shell, it\u0026rsquo;s a protocol to manage services inside a network using a secure channel. One of the most common tasks that can be done using this protocol is to log into a server and remotely execute commands in this server.\nSSH is the protocol and we need a tool that implements this protocol. The most used tool for this protocol is OpenSSH and the one we\u0026rsquo;ll be using for this guide. OpenSSH and other implementations come by default in Unix-like systems and in the case of Windows I personally use Git Bash because it\u0026rsquo;s Unix-like terminal in Windows. Others prefer using PuTTU.\nLet\u0026rsquo;s do it Although I could test everything in this guide using a virtual machine or a Linux PC in our local network, I preffer doing it in a more real environment to prove my point. I will create a public server.\nCreating a public server Important I will use DigitalOcean for this post, of course you can use other cloud providers e.g. AWS, Vultr, Linode, etc. The idea is to have a server that can be accessed from the Internet.\nSomething I like about DigitalOcean is that it charges you for the hours used i.e. a machine that is running for a complete month will cost 5 USD (the cheapest) but if we only have it running for an hour it will cost 0.007 cents. You can the check its pricing page.\nCreating a server in DigitalOcean is very simple, just a couple of clicks, choosing the Linux distribution and resources to assign to the machine. I will choose Debian9 x64 with 1GB of memory that will cost 0.007 USD per hour.\nI recommend you to assign an SSH Key when creating the server.\nThis link shows how to create a server.\nStarting session via SSH By default DigitalOcean allows yout to access the servers with a root user, which is considered a bad practice in terms of security. For a real server I recommend you to disable root login.\nTo log into a server run:\n$ ssh usuario@servidor For the case of DigitalOcean run:\n$ ssh root@ip_droplet Once connected we can do different things: run commands, configure the server, install/uninstall packages, etc. Now with this we can start to play and see some funny things we can do with SSH.\nLocal Port Forwarding I will explain the concept with an example: let\u0026rsquo;s suppose we have a public server and behind it there is a private network that can have different private servers such as a database server. So as these servers are in a private network there is not a simple way to access these services. Some options to access these private services is to use a VPM or Local Port Forwarding. Local Porta Forwarding allows us to create a tunnel between a private service with our local machine via a public server that can access these private services.\nLet\u0026rsquo;s check the next scenario:\nIn the picture there is a MySQL database server in a private network with address 10.100.1.23 and its port is 3306 and there is a public server with IP address 152.190.23.56.\nIn case we want to access the private database server we need to be in the private network (something that is not true) because we are in our house\u0026rsquo;s local network. Fortunately the public server 152.190.23.56 has access to the private network and it also can be accessed from the Internet.\nWhat we will do is to use this SSH\u0026rsquo;s feature called Local Port Forwarding to create a secure channel through the public server 152.190.23.56 between the port 3306 in my local computer and the port 3306 in the private database server 10.100.1.23.\nThe execution format is as follows:\nssh -nNT -L portA:private_host:portB user@server Where:\n-n avoids STDIN input i.e. avoids setting up a terminal session -N do not execute a remote command -T disable the option to display a remote terminal -L Local Port Forwarding option portA port in our local machine where the remote service will be exposed e.g. MySQL private_host the private ip address of a server that can be accessed via a public server portB remote service\u0026rsquo;s port that is running in private_host For our example it would be something like:\nssh -nNT -L 3306:10.100.1.23:3306 root@ip_droplet It is important to note that in case we have MySQL installed in our local machine it would be a port conflict. In that case we can use something like.\nssh -nNT -L 3307:10.100.1.23:3306 root@ip_droplet Let\u0026rsquo;s suppose the case that users in our local network need to access this private service as well, the simplest solution would be these users to follow the same procedure but let\u0026rsquo;s add the constraint that they don\u0026rsquo;t have permissions to access the public server. In this case the solution would be that I do the same procedure but I expose my local port inside the local network so any user can connecto to my local ip address which in turn will connect to the remote private database server.\nNext diagram shows what we want to accomplish:\nIn order to get this scenario working we only need at add an extra option to the execution:\nssh -nNT -L local_ip:portA:private_host:portB user@server Donde\nlocal_ip is our machine\u0026rsquo;s local network IP address, in case we are in different networks and not sure of the our IP address we set it to 0.0.0.0 that will expose it in all the networks that are connected to. ssh -nNT -L 192.168.1.100:3306:10.100.1.23:3306 root@ip_droplet From the previous run all machines in our local network can connect to 192.168.1.100:3306 and that way they will access the MySQL server which is in a remote private network.\nReproducing the example in our public server One of the defaults behaviors in MuSQL is that it does not expose the 3306 to any network i.e. it is only available locally. So if we install MySQL in our public server the port 3306 will not be exposed to the Internet and we could consider it to be in a \u0026ldquo;private network\u0026rdquo; and apply what we learned so far.\nOnce in the server we install MySQL by running:\n$ sudo apt install mysql-server We want to make sure that MySQL is not exposing the 3306 port to the Internet.\nFor that purpose we have two options:\nUse netstat internally We will check that there is an open socket in port 3306 but this one is using address 127.0.0.1 i.e. only available to the same machine.\n$ netstat -tlpn The output will be similar to:\nActive Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 127.0.0.1:3306 0.0.0.0:* LISTEN 2722/mysqld tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 784/sshd tcp6 0 0 :::22 :::* LISTEN 784/sshd And as expected we see that MySQL is exposing the 3306 only for the same machine and not accessible from any network (public or private).\nUsing nmap externally In order to check if port 3306 is open to the public we use nmap:\n$ nmap -Pn server_ip -p 3306 And the output will be something like:\nStarting Nmap 7.40 ( https://nmap.org ) at 2018-08-18 23:24 -04 Nmap scan report for 142.93.204.171 Host is up (0.12s latency). PORT STATE SERVICE 3306/tcp closed mysql 3306/tcp closed mysql says that we cannot access this port from the outside.\nUsing Local Port Forwarding Now the question is: How can we access MySQL from outside?. There are different answers to this question but this post will use Local Port Forwarding to map a port from our local machine with a port that can only be accessed internally from the public server.\nssh -nNT -L 3306:localhost:3306 root@server_ip This way we check in our computer that port 3306 should be open. To verify we run the next in our local machine.\n$ netstat -tlpn With an output similar to:\nProto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 127.0.0.1:3306 0.0.0.0:* LISTEN 23193/ssh tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN - Where 127.0.0.1:3306 says that port 3306 is only available for our local machine using a secure channel to the remote\u0026rsquo;s server internal MySQL server.\nIn addition we can make other machines in our local network to access our machine which in turn can access remote\u0026rsquo;s internal MySQL server. We do that by running:\n$ ssh -nNT -L 192.168.1.100:3306:localhost:3306 root@server_ip To verify that it is available in our internal network we run:\nProto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 192.1681.1.100:3306 0.0.0.0:* LISTEN 23193/ssh tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN - Final Comments In this first part we checked how to expose from our local computer a service that can be only accessed privately from a public server using Local Port Forwarding.\nPersonally I use this technique a lot in order to access database servers or any other private service inside a private cloud in case there is no VPN, this makes my work simpler and prevents us from exposing private services to the public.\nIn the next part of this posts, I will show how to expose services from our local machine or local network to the Internet by using a public server.\n","permalink":"https://blog.donkeysharp.xyz/posts/fun-with-ssh-part-1/","summary":"\u003cp\u003eThe other day when I went out eating with my friend Francisco, he commented me about a personal project and what he wanted to accomplish. Listening to his questions the first tool that came to my mind that would solve some of the challenges he has was OpenSSH.\u003c/p\u003e\n\u003ch2 id=\"what-is-ssh\"\u003eWhat is SSH?\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://es.wikipedia.org/wiki/Secure_Shell\"\u003eSSH\u003c/a\u003e is a short for Secure Shell, it\u0026rsquo;s a protocol to manage services inside a network using a secure channel. One of the most common tasks that can be done using this protocol is to log into a server and remotely execute commands in this server.\u003c/p\u003e","title":"Fun with SSH - Local Port Forwarding"},{"content":"Hey,during my free time I was working a side project called Gocho. This application allows the user to share files in a local network e.g. home network, work, etc. with the difference that it will auto-discover other nodes. In this post I will explain what this application is all about, why I wrote it and some challenges that came up duting the development.\nWhy I wrote this app? First I wanted to share a directory in a local network without the need of others having to ask for my local ip address or the port where I published the files. Also I wanted something that is simple to execute in most common operating systems (Windows, OSX and GNU/Linux) without the need to install some dependencies.\nWhen I start a project like this I always want to learn something new. In this case I wanted to learn Go since last year (2017). Although I did one thing or another with this language they were just small experiments. I wrote a small project called Golondrina; EazyPanel), and this idea seemed to be a good use case to use this programming language (I will elaborate on that below).\nWhy Go? If some of you already tested the application it\u0026rsquo;s easy to see that it\u0026rsquo;s not something from another world. Gocho could have been developed in other programming laguages such as C/C++, Python, Ruby, Java, CSharp, etc. but I had some some observations of those otions:\nPython/Ruby - It\u0026rsquo;s important to have the installer installed by default and Windows does not have it. Java/CSharp - it\u0026rsquo;s important to have the virtual machines already installed in the operating system (JVM or NetCore). It will not always be the case that those are installed by default. C/C++ - It would be the obvious choice, but found some observations: first is that by default these programming languages are dynamically linked, which could cause in some cases the need to install the libraries required (unless I use the static flag during compilation of course) and the second problem is that even I can read C/C++ I don\u0026rsquo;t feel ready enough to start writing something like this. Go - It is compiled statically by default (everything in a single binary) and the final result does not require an interpreter, virtual machine or similar to be installed previously. With that list â€”a little bit biased \u0026#x1f609;â€” Go matches with the needs I have.\nSomething else about Go is the simplicity I have to create binaries for different platforms. For instance Gocho is available for different platforms without extra complications. Releases Gocho\nSome issues I found Issue #1: Sharing files In the company where I work there a big diversity in terms of operating systems. Some use Windows, OSX and others GNU/Linux. In order to share files, there is a \u0026ldquo;Shared folder\u0026rdquo; that I couldn\u0026rsquo;t make it to work same as others (I use GNU/Linux). When I tried to accessI got some errors to instert domain and credentials; even if I input the correct values â€”that were supposed to be the correct onesâ€” I wasn\u0026rsquo;t able to access shared files.\nSome guys share information such as videos, courses, etc. by setting up a web server httpd in their local machines or in my case I just started python -m SimpleHTTPServer in the directory that I wanted to share. I found an issue with SimpleHTTPServer, with few people trying to download the same file, this server only could handle one download at a time.\nMy next try was to bring up something better that SimpleHTTPServer without the need to setup something big such as httpd. I had the luck to see a code snippet in Go\u0026rsquo;s documentation for the net/http module that starts a file sharing server that could support multiple downloads at the same time. Lucky me!\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { // Simple static webserver: directory := \u0026#34;some/directory\u0026#34; log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, http.FileServer(http.Dir(\u0026#34;/home/myuser/some/directory\u0026#34;)))) } I just had to compile this code; move the final binary to some directory in the PATH and voila! I had a simple solution that could handle more concurrent download without the need to mount a complex service.\nSo far I just have a small binary that allows me to share a directory and that it can handle multiple concurrent downloads â€”something simpler to setup compared to a service such as httpd or nginx but better than SimpleHTTPServer.\nIf we make numbers, just compiling this binary for different operating systems would have been enough. But I wanted to go further.\nIssue #2: Where are the shared files? Another thing I noticedis that everytime a user shares something in the local network, user must communicate (somehow) other users how to access the resources this user is sharing. The most common way is by sending to a chat the IP address and the port where shared files are.\nSince the beginning I\u0026rsquo;m thinking this application will only work on a local network and something that quickly came up to my mind was these old LAN games such as StarCraft. When you played StarCraft a user created a game and other players could join the game without knowing the IP address of the machine that created the game. The game automatically shows all the games available in the network and a user can connect to it automatically.\nDoing some research about how this games could detect all avaialable games in a network without the need to specify an IP address, took me to the concept of multicast.\nIn small words, Multicast is a network feature that allows a computer sending information to the network so other nodes in the network can receive these information.\nFor instance, if I want to send a \u0026ldquo;Hello world\u0026rdquo; to other computers that are interested in receiving thismessage and wihtout me having to know which computers, the idea is the next:\nMy Computer: Send an UDP datagram with a \u0026ldquo;Hello world\u0026rdquo; mesage to the reserved Multicast IP range e.g. 239.6.6.6:1234 Interested computer 1: Listen for UDP datagrams in 239.6.6.6:1234 Interested computer 2: Listen for UDP datagrams in 239.6.6.6:1234 Interested computer N: Listen for UDP datagrams in 239.6.6.6:1234 This way Multicast allows whoever computer that wants to share something, it only needs its own information (identifier, IP address, port, etc) via Multicast and interested machines will be able to get this information.\nKnowing this, Gocho in addition to just sharing a directory, it will be able to meet other Gocho nodes that are currently sharing something.\nNow regarding implementation, we can check some code snippets I used for Gocho.\npkg/node/net.go\nThe announceNode function sends a Multicast packet.\nfunc announceNode(nodeInfo *NodeInfo) { address, err := net.ResolveUDPAddr(\u0026#34;udp\u0026#34;, MULTICAST_ADDRESS) // error handling conn, err := net.DialUDP(\u0026#34;udp\u0026#34;, nil, address) // error handling for { ... conn.Write([]byte(message)) time.Sleep(ANNOUNCE_INTERVAL_SEC * time.Second) } } The listenForNodes function will listen for Multicast messages.\nfunc listenForNodes(nodeList *list.List) { address, err := net.ResolveUDPAddr(\u0026#34;udp\u0026#34;, MULTICAST_ADDRESS) // error handling conn, err := net.ListenMulticastUDP(\u0026#34;udp\u0026#34;, nil, address) // error handling conn.SetReadBuffer(MULTICAST_BUFFER_SIZE) for { packet := make([]byte, MULTICAST_BUFFER_SIZE) size, udpAddr, err := conn.ReadFromUDP(packet) ... } } Therefore most of the \u0026ldquo;magic\u0026rdquo; that Gocho has relies on Multicast. With Multicast a computer can announce itself and at the same time discover other nodes.\nIssue #3: Message format Now that we can communicate between nodes, I knew that was convenient to identify the packets that Gocho sends to other nodes. Basically the packet (this initial version) must follow next format:\nThe first 4 bytes must be 0x60, 0x0d, 0xf0, 0x0d or 0x600df00d (Good Food), which is the header tha identifies a Gocho message. The next byte specifies the command, currently there is only one command which is 0x01 that specifies that a node is announcing. The information of the node is in the payload. Finally the rest of the content is the payload. For this purpose I decided to use the JSON format. This is the hexadecimal representation of a message from a node that is announcing itself:\n00000000 60 0d f0 0d 01 7b 22 6e 6f 64 65 49 64 22 3a 22 |`....{\u0026#34;nodeId\u0026#34;:\u0026#34;| 00000010 6e 6f 64 6f 2d 73 65 72 67 69 6f 22 2c 22 69 70 |nodo-sergio\u0026#34;,\u0026#34;ip| 00000020 41 64 64 72 65 73 73 22 3a 22 22 2c 22 77 65 62 |Address\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;web| 00000030 50 6f 72 74 22 3a 22 35 30 30 30 22 7d |Port\u0026#34;:\u0026#34;5000\u0026#34;}| I decided to use this kind of format to reduce the usage of bytes as much as possible. In fact, if it wouldn\u0026rsquo;t use JSON format more bytes would be saved.\nIn the future it is possible that more commands exists other than the announce node command (0x01). That\u0026rsquo;s the reason one byte is reserved for that.\nApplication design Now that we now the issues and solutions that were given, this section describes a bit more some implementation details.\nCode structure The code structure of this project is based on this article that shows a nice code structure for Go projects. This structure is used in different projects such as Kubernetes or Docker.\nThis project includes a Makefile that has all the required steps to build and develop the project and also the multi-architecture binary build process.\nService components In the previous section I mentioned the code structure that was used. This section I will focus on the components inside the directory pkg, specially in pkg/node.\nComponent Description pkg/info Application information such as name and versoin. pkg/cmds All the CLI logic, flags, options, etc e.g. gocho start [options] or gocho configure. pkg/config All the configuration logic such as default values or loading setings from .gocho.conf file are defined in this component. pkg/node | Application\u0026rsquo;s main logic is here. Things such as the embeded dashboard; the packet\u0026rsquo;s format; the auto-discovery feature using Multicast and the index of files in the shared directory.\nA bit about some data structure and logic used Application needs to store the information of other nodes, for this purpose I decided to use a linked list due to its simplicity to delete or add elements.\nAs more nodes are getting announced in a network, it is possible that some parts of the code (from any node) will execute some parts of the code at the same time. In order to avoid concurrency problems: mainly in the linked list that stores nodes\u0026rsquo; information, I used a Mutex that will help us manage this kind of behaviors that could lead us to unexpected results.\nSomething important here is that there are timeouts set by default that constantly check the linked list. Basically these timeouts allow us to free resources when a node stops announcing itself after some time.\nThe Dashboard For the dashboar ddevelopment I only used React. Maybe some of you will make question about why I didn\u0026rsquo;t use Redux or React-Router? The answer is simple, as the final bundle with the required static files will be embedded in the binary, it will be better to have it as reduced and simple as possible.\nAll the components and code for the UI are in ui directory. The structure used for this project is the one that is created by default using Create React App.\nThe same for styles, I could have used some CSS processor such as SASS, but I decided to keep things simple. All styles are in this file which is ~184 lines of code.\nTo generate the javascript bundle just run the next command\n$ make dist The previous command uses the Creat React App scripts to generate the final bundle and embed it into the binary. I make use of Go Generate.\nThis files has a comment:\npackage main //go:generate go-bindata -o ../../assets/assets_gen.go -pkg assets ../../ui/build/... import ( \u0026#34;github.com/donkeysharp/gocho/pkg/cmds\u0026#34; \u0026#34;os\u0026#34; ) ... That specifies where the bundle is and that will be embedded in to binary.\nThe shared files web index This is one of the features where I got more fund. As I mentioned on Issye #1, Go shows an example to share a directory via web. The problem with that web index does not have any styles, cannot be extended and the .. directory does not exist.\nIn order to customize this existing code I used the Interceptor Pattern and a custom middleware that adds the icons, custom HTML code and the .. directory to go up one level.\nAll the logic used to customize net/http.FileServer is in this file index.go.\nFinal comments There are some many things that I wish to improve in Gocho. Because it is an Open Source project feel free to open an issue or contribute some bug fix or feature.\n","permalink":"https://blog.donkeysharp.xyz/posts/gocho-file-sharing/","summary":"\u003cp\u003eHey,during my free time I was working a side project called \u003ca href=\"https://github.com/donkeysharp/gocho\"\u003eGocho\u003c/a\u003e. This application allows the user to share files in a local network e.g. home network, work, etc. with the difference that it will auto-discover other nodes. In this post I will explain what this application is all about, why I wrote it and some challenges that came up duting the development.\u003c/p\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/img/gocho-dashboard.gif\"\u003e\u003c/p\u003e\n\u003ch2 id=\"why-i-wrote-this-app\"\u003eWhy I wrote this app?\u003c/h2\u003e\n\u003cp\u003eFirst I wanted to share a directory in a local network without the need of others having to ask for my local ip address or the port where I published the files. Also I wanted something that is simple to execute in most common operating systems (Windows, OSX and GNU/Linux) without the need to install some dependencies.\u003c/p\u003e","title":"Gocho, Sharing files in a local network"},{"content":" Update\n2018-04-28 Added more packages I use and new settings I do (reinstalled machine)\nIn this post I show the applications and settings I commonly use for my local development machine.\nOperating System: GNU/Linux - Debian Stretch\nDektop Manager: xfce4\nFrequently Used Applications This is the list of application I frequently use and try to have them installed after I have a fresh computer. Some applications are general purpose and others are related with programming, things I investigate, my job and personal projects.\nGnome file roller allows the user to compress filesfile-roller Font Viewer helps install fonts gnome-font-viewer Google Chrome Terminal music player mocp Vim editor, command line text editorvim Build essentials build-essential SublimeText3 text editor I use for almost everything. Emacs text editor I use for certain things emacs Redshift helps me change my monitor temperature redshift Kazam desktop recording kazam Kupfer similar to Spotlight that allows me to lauch application the easy way kupfer Shutter for screenshots shutter Vector graphics editor inkscape Gnome Hex Editor ghex Meld to compare differences between two files meld Armagetron, Tron based game armagetronad DOSBox emulator for old DOS games dosbox Missing drivers firmware-linux-free firmware-linux-nonfree Ristretto Image viewer Ettercap Transmission torrent client transmission Wireshark to see network traffic wireshark Slack messaging, I personally use the web version but when I need to share my screen I\u0026rsquo;m force to use the desktop verison. Pavu Controller for audio configuration pavucontrol VLC Media Player vlc Evince PDF viewer evince xCHM .chm files viewer xchm Utility to manage disks gparted Hardware information hardinfo Docker Community Edition NodeJs This is another list of application I use frequently in the terminal.\nTerminal multiplexer tmux htop ncurses-based process viewer htop Track system calls of a process strace HTTP client curl DNS utils dnsutils Install always sudo Compress utils zip Of course there are other packets for specific very specific things that I install when required.\nDesktop I use XFCE4 with two panels both on the top section of the screen. The first one contains the applications menu with the Debian logo; a separator with transparency enabled that extends; list of opened windos; another transparent separator that extends. The second panel has these items: workspace areas four workspace area in two rows (2x2); a CPU usage viewr; notification are; plugin for PulseAudio; and the date-time plugin.\nFirst panel has a dark background while the other uses the style that comes by default.\nLook and feel For my look and feel settings I use the next:\nNumix Light icons that are installed with numix-icon-theme Adwaita window theme Default font: Sans (10) with antialiasing enabled Slight and DPI set to 101. Extra Tweaks I don\u0026rsquo;t like the Windows Switcher (alt + tab) that comes with XFCE4 by default, it is too big with a preview of each window. I prefer to have small icons without the name of the window or things like that. With a couple of changes I can get that by going to: Settings \u0026gt; Window Manager Tweaks and select the Cycling tab and unselect Cycle through windows in a list and finally in the Compositor tab unselect Show windows preview in place of icons when cylcing.\nHotkeys I have some hotkeys configure in XFCE4 for common tasks I do. To configure hotkeys in XFCE4 go to Settings \u0026gt; Keyboard \u0026gt; Application Shortcuts tab. My common hotkeys are:\nHotkey Comando DescripciÃ³n win_key + f thunar Open Thunar file manager win_key + t /usr/bin/xfce4-terminal Open a new terminal win_key + n mocp --next Next song in MOC player win_key + b mocp --previous Previous song in MOC player win_key + o mocp --pause Paus the actual song in MOC player win_key + p mocp --unpause Continue playing song in MOC player Other hotkeys: This is not an XFCE4 hotkey but I use it frequently ctrl + shift + space that launches Kupfer.\nTerminal Theme I use xfce4-terminal with the next settings in ~/.config/xfce4/terminal/terminalrc`.\nPrompt By default Bash comes with a prompt similar to usuario@host:directorio-actual. In my case I use a lot of Git repositories, this default prompt is not good enough for me as I need to check the current branch, if there are conflicts or unstaged changes, etc. Of course I can run git status but the prompt can help me with that \u0026#x1f609;. This is the script I export in .bashrc, that basically shows repository information, current directory and the time. Thanks to Mike Stewart who is the original author of that script.\nI tried to use zsh and its frameworks but I didn\u0026rsquo;t feel comfortable and it was kind of hard getting used to it, so the simplest way for me was having a custom prompt light and simple. Fortunately there were so many resources available on the Internet so it wasn\u0026rsquo;t a pain.\nDot env settings Tmux settings I started using Tmux sing Debian Wheezy but when I upgraded to Debian Jessie I had some problems with the current working directory when creating new panels. This is the .tmux.conf I use.\nMOC Player Settings Because MOC is a CLI tool I think it fits in this section. I use two files .moc/config and another one for the theme. Both can be found here.\nFinal Comments Although this configurations are more for my personal usage, I wrote this post with the purpose to read if I forget something and shared in case it is useful for a reader.\n","permalink":"https://blog.donkeysharp.xyz/posts/my-computer-setup/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eUpdate\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e2018-04-28\u003c/em\u003e Added more packages I use and new settings I do (reinstalled machine)\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eIn this post I show the applications and settings I commonly use for my local development machine.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eOperating System:\u003c/em\u003e GNU/Linux - Debian Stretch\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eDektop Manager:\u003c/em\u003e xfce4\u003c/p\u003e\n\u003ch2 id=\"frequently-used-applications\"\u003eFrequently Used Applications\u003c/h2\u003e\n\u003cp\u003eThis is the list of application I frequently use and try to have them installed after I have a fresh computer. Some applications are general purpose and others are related with programming, things I investigate, my job and personal projects.\u003c/p\u003e","title":"My desktop setup in Debian"},{"content":"Hey there, I am Sergio Guillen publishing via Github pages \u0026#x1f604;.\nThis blog is a project I will take again and publish content different from my previous blog which was mostly about Microsoft technologies. Since I stopped using those technologies â€”that I still get impressed with them e.g. C#â€” I had the good luck to meet new technologies, so I want to share some experiences I had with them or stuff I\u0026rsquo;ve been researching.\nThis blog is powered by Hugo which is a static site generator. I like it a lot!\nYou can follow me on Twitter.\n:() { :|:\u0026amp; }; : ","permalink":"https://blog.donkeysharp.xyz/posts/my-first-post/","summary":"\u003cp\u003eHey there, I am Sergio Guillen publishing via Github pages \u0026#x1f604;.\u003c/p\u003e\n\u003cp\u003eThis blog is a project I will take again and publish content different from my \u003ca href=\"http://donkeysharp.blogspot.com/\"\u003eprevious blog\u003c/a\u003e which was mostly about Microsoft technologies. Since I stopped using those technologies â€”that I still get impressed with them e.g. C#â€” I had the good luck to meet new technologies, so I want to share some experiences I had with them or stuff I\u0026rsquo;ve been researching.\u003c/p\u003e","title":"First Post - echo \"Hello World\""},{"content":"Hi! my name is Sergio Guillen. Iâ€™m passionate about Software Development, GNU/Linux, FOSS, Cloud Technologies and Hacker Culture. I am very curious and love understanding how things work under-the-hood, starting from libraries, frameworks and tools I use on a daily basis to more specialized things such as operating systems, network protocols, binary executables, hardware, etc.\nSince 2016 I moved from software development to cloud infrastructure and embracing the DevOps culture. Iâ€™m always eager to improve development and deployment processes so it can be simple to release new versions of a software in shorter periods of time and improve the way cloud infrastructure is managed.\nIn my free time I collaborate to different technology communities which gives me the opportunity to learn more from others and to share what I know.\nI\u0026rsquo;m the dude who is always creating, breaking and fixing stuff ðŸ¤“.\n","permalink":"https://blog.donkeysharp.xyz/about/","summary":"\u003cp\u003eHi! my name is Sergio Guillen. Iâ€™m passionate about Software Development, GNU/Linux, FOSS, Cloud Technologies and Hacker Culture. I am very curious and love understanding how things work under-the-hood, starting from libraries, frameworks and tools I use on a daily basis to more specialized things such as operating systems, network protocols, binary executables, hardware, etc.\u003c/p\u003e\n\u003cp\u003eSince 2016 I moved from software development to cloud infrastructure and embracing the DevOps culture. Iâ€™m always eager to improve development and deployment processes so it can be simple to release new versions of a software in shorter periods of time and improve the way cloud infrastructure is managed.\u003c/p\u003e","title":"About me"}]